{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5b37b9",
   "metadata": {},
   "source": [
    "## RESEARCH QUESTION 3 - CYCLONE PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b18a741",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf4f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import geopandas as gpd\n",
    "import movingpandas as mpd\n",
    "from shapely.geometry import Point\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "import torch\n",
    "from darts import TimeSeries\n",
    "from darts.utils.callbacks import TFMProgressBar\n",
    "from darts.models import (NBEATSModel)\n",
    "from darts.metrics import mae, rmse, mse\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tabulate import tabulate\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbbd02a",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3285448",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PREPROCESSING\n",
    "\n",
    "cyc = pd.read_csv('IDCKMSTM0S.csv', skiprows=3)\n",
    "\n",
    "cyc.columns.to_list()\n",
    "\n",
    "cyc1 = cyc[['NAME',\n",
    "            'DISTURBANCE_ID',\n",
    "            'TM',\n",
    "            'LAT',\n",
    "            'LON',\n",
    "            'CENTRAL_PRES',\n",
    "            'ENV_PRES',\n",
    "            'MN_RADIUS_GF_SECNE',\n",
    "            'MN_RADIUS_GF_SECSE',\n",
    "            'MN_RADIUS_GF_SECSW',\n",
    "            'MN_RADIUS_GF_SECNW']]\n",
    "\n",
    "sub1 = cyc1.loc[(cyc1['DISTURBANCE_ID']== 'AU199899_09U')]\n",
    "\n",
    "# Remove white space value from all object columns\n",
    "obcols = cyc1.select_dtypes(object).columns\n",
    "cyc1[obcols] = cyc1[obcols].apply(lambda x: x.replace(\" \",\"\"))\n",
    "\n",
    "# Remove negative signs in LON column\n",
    "cyc1['LON'] = cyc1['LON'].apply(lambda x: x.replace(\"-\",\"\"))\n",
    "\n",
    "# Remove trailing white spaces from string values\n",
    "cyc1[['NAME',\n",
    "      'DISTURBANCE_ID',\n",
    "      'TM']] = cyc1[['NAME',\n",
    "                     'DISTURBANCE_ID',\n",
    "                     'TM']].apply(lambda x: x.str.strip())\n",
    "\n",
    "# Convert dtypes from object to numeric\n",
    "cyc1[['LAT',\n",
    "      'LON',\n",
    "      'CENTRAL_PRES',\n",
    "      'ENV_PRES',\n",
    "      'MN_RADIUS_GF_SECNE',\n",
    "      'MN_RADIUS_GF_SECSE',\n",
    "      'MN_RADIUS_GF_SECSW',\n",
    "      'MN_RADIUS_GF_SECNW']] = cyc1[['LAT',\n",
    "                                     'LON',\n",
    "                                     'CENTRAL_PRES',\n",
    "                                     'ENV_PRES',\n",
    "                                     'MN_RADIUS_GF_SECNE',\n",
    "                                     'MN_RADIUS_GF_SECSE',\n",
    "                                     'MN_RADIUS_GF_SECSW',\n",
    "                                     'MN_RADIUS_GF_SECNW']].apply(lambda x:pd.to_numeric(x))\n",
    "\n",
    "# Convert TM to datetime\n",
    "cyc1['TM'] = pd.to_datetime(cyc1['TM'], dayfirst=True)\n",
    "\n",
    "cyc2 = cyc1.copy()\n",
    "\n",
    "# Remove all observations that have missing values in TM\n",
    "cyc2 = cyc2[cyc2['TM'].notnull()]\n",
    "\n",
    "# Resample to 6 hourly interval\n",
    "cyc2 = cyc2.set_index('TM').groupby(['DISTURBANCE_ID', 'NAME']).resample('6H').mean()\n",
    "\n",
    "cyc3= cyc2.reset_index(level=['TM', 'NAME'])\n",
    "\n",
    "# interpolate 'inside' valid values for each group\n",
    "groupunique = cyc3.index.unique().to_list()\n",
    "for i in groupunique:\n",
    "    cyc3.loc[[i],['LAT',\n",
    "                  'LON',\n",
    "                  'CENTRAL_PRES',\n",
    "                  'ENV_PRES',\n",
    "                  'MN_RADIUS_GF_SECNE',\n",
    "                  'MN_RADIUS_GF_SECSE',\n",
    "                  'MN_RADIUS_GF_SECSW',\n",
    "                  'MN_RADIUS_GF_SECNW']] = cyc3.loc[[i],['LAT',\n",
    "                                                         'LON',\n",
    "                                                         'CENTRAL_PRES',\n",
    "                                                         'ENV_PRES',\n",
    "                                                         'MN_RADIUS_GF_SECNE',\n",
    "                                                         'MN_RADIUS_GF_SECSE',\n",
    "                                                         'MN_RADIUS_GF_SECSW',\n",
    "                                                         'MN_RADIUS_GF_SECNW']].interpolate(limit=20,\n",
    "                                                                                            limit_area='inside')\n",
    "\n",
    "cyc3 = cyc3.reset_index()\n",
    "\n",
    "# drop all rows with at least 2 NAs in R34s\n",
    "cyc3 = cyc3.drop(cyc3.loc[sum([(cyc3['MN_RADIUS_GF_SECNE'].isnull()),\n",
    "                               (cyc3['MN_RADIUS_GF_SECSE'].isnull()),\n",
    "                               (cyc3['MN_RADIUS_GF_SECSW'].isnull()),\n",
    "                               (cyc3['MN_RADIUS_GF_SECNW'].isnull())])>=2].index)\n",
    "\n",
    "# drop all rows with at least 2 NAs\n",
    "cyc3 = cyc3.drop(cyc3.loc[sum([(cyc3['MN_RADIUS_GF_SECNE'].isnull()),\n",
    "                               (cyc3['MN_RADIUS_GF_SECSE'].isnull()),\n",
    "                               (cyc3['MN_RADIUS_GF_SECSW'].isnull()),\n",
    "                               (cyc3['MN_RADIUS_GF_SECNW'].isnull()),\n",
    "                               (cyc3['ENV_PRES'].isnull())])>=2].index)\n",
    "\n",
    "# kNN-neighbour imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "cyc3_imp = imputer.fit_transform(cyc3[['ENV_PRES',\n",
    "                                       'MN_RADIUS_GF_SECNE',\n",
    "                                       'MN_RADIUS_GF_SECSE',\n",
    "                                       'MN_RADIUS_GF_SECSW',\n",
    "                                       'MN_RADIUS_GF_SECNW']])\n",
    "\n",
    "cyc3_imp = pd.DataFrame(cyc3_imp, index= cyc3.index,\n",
    "                        columns=['ENV_PRES',\n",
    "                                 'MN_RADIUS_GF_SECNE',\n",
    "                                 'MN_RADIUS_GF_SECSE',\n",
    "                                 'MN_RADIUS_GF_SECSW',\n",
    "                                 'MN_RADIUS_GF_SECNW'])\n",
    "\n",
    "cyc4 = cyc3[['LAT',\n",
    "             'LON',\n",
    "             'CENTRAL_PRES']]\n",
    "\n",
    "cyc4[['ENV_PRES',\n",
    "      'MN_RADIUS_GF_SECNE',\n",
    "      'MN_RADIUS_GF_SECSE',\n",
    "      'MN_RADIUS_GF_SECSW',\n",
    "      'MN_RADIUS_GF_SECNW']] = cyc3_imp\n",
    "\n",
    "cyc4 = cyc3.set_index('TM').groupby(['DISTURBANCE_ID']).resample('6H').interpolate()\n",
    "cyc4 = cyc4.reset_index(level='TM')\n",
    "\n",
    "cyc4[['NAME', 'DISTURBANCE_ID']] = cyc4[['NAME', 'DISTURBANCE_ID']].ffill()\n",
    "\n",
    "cyc4 = cyc4.dropna()\n",
    "cyc4 = cyc4.reset_index(drop=True)\n",
    "\n",
    "# only keep cyclone groups with at least 3 observations\n",
    "s = cyc4.groupby('DISTURBANCE_ID').size() >= 4\n",
    "cyc5 = cyc4.loc[cyc4['DISTURBANCE_ID'].isin(s[s].index)]\n",
    "\n",
    "cyc5.to_csv('cyc5.csv', index=False)\n",
    "\n",
    "################################################################################\n",
    "# Split data based on whether a cyclone ever exists East or West of a\n",
    "#  particular line of Longitude\n",
    "\n",
    "# Only keep cyclones that travel West of a particular Longitude (threshold_LON)\n",
    "threshold_LON = 131 # Approx Longitude of Darwin\n",
    "# Obtain UID of cyclones to keep in West\n",
    "UIDW = cyc5.groupby('DISTURBANCE_ID').filter(lambda x: (x['LON'] < threshold_LON).any())\n",
    "UIDW = UIDW['DISTURBANCE_ID'].unique()\n",
    "# Filter dataframe\n",
    "cyc5W = cyc5[cyc5['DISTURBANCE_ID'].isin(UIDW)]\n",
    "cyc5W.to_csv('cyc5W.csv', index=False)\n",
    "\n",
    "# Only keep cyclones that travel East of a particular Longitude (threshold_LON)\n",
    "# Obtain UID of cyclones to keep in East\n",
    "UIDE = cyc5.groupby('DISTURBANCE_ID').filter(lambda x: (x['LON'] > threshold_LON).any())\n",
    "UIDE = UIDE['DISTURBANCE_ID'].unique()\n",
    "# Filter dataframe\n",
    "cyc5E = cyc5[cyc5['DISTURBANCE_ID'].isin(UIDE)]\n",
    "cyc5E.to_csv('cyc5E.csv', index=False)\n",
    "################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a6884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Generate lists of dataframes: Total, East region only and West region only\n",
    "cyc6 = cyc5.groupby('DISTURBANCE_ID')           # For RQ1 and RQ2\n",
    "cyc6w = cyc5W.groupby('DISTURBANCE_ID')         # For RQ3\n",
    "cyc6e = cyc5E.groupby('DISTURBANCE_ID')         # For RQ3\n",
    "################################################################################\n",
    "\n",
    "# from cyc6.groups.keys()\n",
    "all_set = {'AU199697_14U', 'AU199899_05U', 'AU199900_13U', 'AU200001_06U', 'AU200203_01U', 'AU200203_02U', 'AU200203_03U', 'AU200203_04U', 'AU200203_06U', 'AU200203_07U', 'AU200304_01U', 'AU200304_02U', 'AU200304_03U', 'AU200304_04U', 'AU200304_05U', 'AU200304_07U', 'AU200304_08U', 'AU200304_09U', 'AU200304_11U', 'AU200405_01U', 'AU200405_02U', 'AU200405_03U', 'AU200405_04U', 'AU200405_05U', 'AU200405_07U', 'AU200405_08U', 'AU200405_09U', 'AU200405_10U', 'AU200506_01U', 'AU200506_05U', 'AU200506_06U', 'AU200506_08U', 'AU200506_10U', 'AU200506_14U', 'AU200506_15U', 'AU200506_16U', 'AU200506_17U', 'AU200506_20U', 'AU200506_22U', 'AU200607_05U', 'AU200607_11U', 'AU200607_12U', 'AU200607_13U', 'AU200607_16U', 'AU200708_01U', 'AU200708_03U', 'AU200708_06U', 'AU200708_08U', 'AU200708_11U', 'AU200708_15U', 'AU200708_20U', 'AU200708_22U', 'AU200708_23U', 'AU200809_02U', 'AU200809_03U', 'AU200809_06U', 'AU200809_08U', 'AU200809_10U', 'AU200809_17U', 'AU200809_18U', 'AU200809_23U', 'AU200910_01U', 'AU200910_06U', 'AU200910_07U', 'AU200910_09U', 'AU200910_11U', 'AU200910_12U', 'AU200910_13U', 'AU201011_02U', 'AU201011_09U', 'AU201011_10U', 'AU201011_11U', 'AU201011_12U', 'AU201011_14U', 'AU201011_16U', 'AU201011_17U', 'AU201011_29U', 'AU201112_01U', 'AU201112_04U', 'AU201112_07U', 'AU201112_11U', 'AU201112_12U', 'AU201112_15U', 'AU201112_16U', 'AU201213_03U', 'AU201213_04U', 'AU201213_05U', 'AU201213_10U', 'AU201213_13U', 'AU201213_14U', 'AU201213_17U', 'AU201213_18U', 'AU201314_01U', 'AU201314_03U', 'AU201314_04U', 'AU201314_07U', 'AU201314_09U', 'AU201314_10U', 'AU201314_14U', 'AU201314_15U', 'AU201314_16U', 'AU201415_04U', 'AU201415_13U', 'AU201415_14U', 'AU201415_16U', 'AU201415_17U', 'AU201415_19U', 'AU201415_21U', 'AU201415_24U', 'AU201516_08U', 'AU201516_09U', 'AU201516_10U', 'AU201617_07U', 'AU201617_19U', 'AU201617_20U', 'AU201617_23U', 'AU201617_24U', 'AU201617_26U', 'AU201617_29U', 'AU201617_30U', 'AU201718_02U', 'AU201718_03U', 'AU201718_06U', 'AU201718_17U', 'AU201718_20U', 'AU201718_22U', 'AU201718_24U', 'AU201819_04U', 'AU201819_05U', 'AU201819_07U', 'AU201819_12U', 'AU201819_14U', 'AU201819_17U', 'AU201819_19U', 'AU201819_20U', 'AU201819_21U', 'AU201819_25U', 'AU201819_26U', 'AU201920_03U', 'AU201920_05U', 'AU201920_06U', 'AU201920_07U', 'AU201920_08U', 'AU201920_10U', 'AU201920_12U', 'AU202021_07U', 'AU202021_09U', 'AU202021_10U', 'AU202021_11U', 'AU202021_15U', 'AU202021_17U', 'AU202021_22U', 'AU202021_23U', 'AU202122_02U', 'AU202122_05U', 'AU202122_07U', 'AU202122_08U', 'AU202122_10U', 'AU202122_18U', 'AU202122_22U', 'AU202122_23U', 'AU202122_27U', 'AU202122_28U', 'AU202122_36U', 'AU202223_01U', 'AU202223_05U', 'AU202223_13U', 'AU202223_14U', 'AU202223_21U', 'AU202223_23U', 'AU202324_02U', 'AU202324_04U', 'AU202324_05U', 'AU202324_08U', 'AU202324_09U', 'AU202324_11U', 'AU202324_13U'}\n",
    "\n",
    "# from cyc6w.groups.keys()\n",
    "w_set = {'AU199697_14U', 'AU199899_05U', 'AU199900_13U', 'AU200203_02U', 'AU200203_03U', 'AU200203_04U', 'AU200203_06U', 'AU200203_07U', 'AU200304_01U', 'AU200304_03U', 'AU200304_05U', 'AU200304_07U', 'AU200304_08U', 'AU200304_11U', 'AU200405_01U', 'AU200405_02U', 'AU200405_03U', 'AU200405_04U', 'AU200405_07U', 'AU200405_08U', 'AU200405_09U', 'AU200506_01U', 'AU200506_05U', 'AU200506_06U', 'AU200506_14U', 'AU200506_16U', 'AU200506_20U', 'AU200607_11U', 'AU200607_12U', 'AU200607_13U', 'AU200708_01U', 'AU200708_06U', 'AU200708_08U', 'AU200708_11U', 'AU200708_15U', 'AU200708_20U', 'AU200708_22U', 'AU200708_23U', 'AU200809_02U', 'AU200809_03U', 'AU200809_08U', 'AU200809_10U', 'AU200809_18U', 'AU200910_01U', 'AU200910_06U', 'AU200910_12U', 'AU200910_13U', 'AU201011_02U', 'AU201011_09U', 'AU201011_12U', 'AU201011_16U', 'AU201011_17U', 'AU201011_29U', 'AU201112_01U', 'AU201112_07U', 'AU201112_11U', 'AU201112_15U', 'AU201112_16U', 'AU201213_04U', 'AU201213_05U', 'AU201213_10U', 'AU201213_17U', 'AU201314_01U', 'AU201314_03U', 'AU201314_04U', 'AU201314_09U', 'AU201314_14U', 'AU201314_16U', 'AU201415_04U', 'AU201415_16U', 'AU201415_19U', 'AU201415_21U', 'AU201516_08U', 'AU201516_09U', 'AU201617_07U', 'AU201617_20U', 'AU201617_23U', 'AU201617_26U', 'AU201617_29U', 'AU201617_30U', 'AU201718_02U', 'AU201718_03U', 'AU201718_06U', 'AU201718_17U', 'AU201718_20U', 'AU201819_05U', 'AU201819_12U', 'AU201819_17U', 'AU201819_19U', 'AU201819_21U', 'AU201819_25U', 'AU201920_03U', 'AU201920_05U', 'AU201920_08U', 'AU202021_07U', 'AU202021_10U', 'AU202021_15U', 'AU202021_22U', 'AU202021_23U', 'AU202122_02U', 'AU202122_05U', 'AU202122_22U', 'AU202122_23U', 'AU202122_27U', 'AU202122_28U', 'AU202122_36U', 'AU202223_01U', 'AU202223_05U', 'AU202223_13U', 'AU202223_21U', 'AU202223_23U', 'AU202324_04U', 'AU202324_08U', 'AU202324_11U'}\n",
    "\n",
    "# from cyc6e.groups.keys()\n",
    "e_set = {'AU200001_06U', 'AU200203_01U', 'AU200203_06U', 'AU200304_02U', 'AU200304_04U', 'AU200304_09U', 'AU200405_05U', 'AU200405_07U', 'AU200405_10U', 'AU200506_08U', 'AU200506_10U', 'AU200506_15U', 'AU200506_17U', 'AU200506_22U', 'AU200607_05U', 'AU200607_16U', 'AU200708_03U', 'AU200708_08U', 'AU200809_06U', 'AU200809_17U', 'AU200809_23U', 'AU200910_07U', 'AU200910_09U', 'AU200910_11U', 'AU201011_10U', 'AU201011_11U', 'AU201011_14U', 'AU201011_17U', 'AU201112_04U', 'AU201112_12U', 'AU201213_03U', 'AU201213_13U', 'AU201213_14U', 'AU201213_18U', 'AU201314_01U', 'AU201314_07U', 'AU201314_10U', 'AU201314_15U', 'AU201415_13U', 'AU201415_14U', 'AU201415_17U', 'AU201415_24U', 'AU201516_10U', 'AU201617_19U', 'AU201617_24U', 'AU201718_20U', 'AU201718_22U', 'AU201718_24U', 'AU201819_04U', 'AU201819_07U', 'AU201819_14U', 'AU201819_20U', 'AU201819_26U', 'AU201920_06U', 'AU201920_07U', 'AU201920_10U', 'AU201920_12U', 'AU202021_09U', 'AU202021_11U', 'AU202021_17U', 'AU202122_07U', 'AU202122_08U', 'AU202122_10U', 'AU202122_18U', 'AU202223_14U', 'AU202324_02U', 'AU202324_05U', 'AU202324_09U', 'AU202324_13U'}\n",
    "\n",
    "shared_zone = w_set.intersection(e_set)\n",
    "\n",
    "west_only = list(w_set.difference(e_set))\n",
    "\n",
    "east_only = list(e_set.difference(w_set))\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Write each group as a record of dictionary named d\n",
    "d = dict()\n",
    "for k in list(cyc6.groups.keys()):\n",
    "    dfname = str(k)\n",
    "    d[dfname] = cyc6.get_group(k)\n",
    "    \n",
    "d_w = dict()\n",
    "for k in west_only:\n",
    "    dfname = str(k)\n",
    "    d_w[dfname] = cyc6w.get_group(k)\n",
    "    \n",
    "d_e = dict()\n",
    "for k in east_only:\n",
    "    dfname = str(k)\n",
    "    d_e[dfname] = cyc6e.get_group(k)\n",
    "\n",
    "################################################################################    \n",
    "\n",
    "# get data frames from dictionary d\n",
    "\n",
    "df_list = []\n",
    "for i in list(cyc6.groups.keys()):\n",
    "    df = 'd[\"' + str(i) + '\"]'\n",
    "    df_list.append(df)\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "for i in west_only:\n",
    "    print(i,end=',')\n",
    "    \n",
    "  \n",
    "for i in east_only:\n",
    "    print(i,end=',')\n",
    "\"\"\"\n",
    "\n",
    "################################################################################\n",
    "# Assign dataframes to corresponding names\n",
    "### For RQ1 and RQ2 ###\n",
    "# All\n",
    "(AU199697_14U,AU199899_05U,AU199900_13U,AU200001_06U,AU200203_01U,AU200203_02U,\n",
    " AU200203_03U,AU200203_04U,AU200203_06U,AU200203_07U,AU200304_01U,AU200304_02U,\n",
    " AU200304_03U,AU200304_04U,AU200304_05U,AU200304_07U,AU200304_08U,AU200304_09U,\n",
    " AU200304_11U,AU200405_01U,AU200405_02U,AU200405_03U,AU200405_04U,AU200405_05U,\n",
    " AU200405_07U,AU200405_08U,AU200405_09U,AU200405_10U,AU200506_01U,AU200506_05U,\n",
    " AU200506_06U,AU200506_08U,AU200506_10U,AU200506_14U,AU200506_15U,AU200506_16U,\n",
    " AU200506_17U,AU200506_20U,AU200506_22U,AU200607_05U,AU200607_11U,AU200607_12U,\n",
    " AU200607_13U,AU200607_16U,AU200708_01U,AU200708_03U,AU200708_06U,AU200708_08U,\n",
    " AU200708_11U,AU200708_15U,AU200708_20U,AU200708_22U,AU200708_23U,AU200809_02U,\n",
    " AU200809_03U,AU200809_06U,AU200809_08U,AU200809_10U,AU200809_17U,AU200809_18U,\n",
    " AU200809_23U,AU200910_01U,AU200910_06U,AU200910_07U,AU200910_09U,AU200910_11U,\n",
    " AU200910_12U,AU200910_13U,AU201011_02U,AU201011_09U,AU201011_10U,AU201011_11U,\n",
    " AU201011_12U,AU201011_14U,AU201011_16U,AU201011_17U,AU201011_29U,AU201112_01U,\n",
    " AU201112_04U,AU201112_07U,AU201112_11U,AU201112_12U,AU201112_15U,AU201112_16U,\n",
    " AU201213_03U,AU201213_04U,AU201213_05U,AU201213_10U,AU201213_13U,AU201213_14U,\n",
    " AU201213_17U,AU201213_18U,AU201314_01U,AU201314_03U,AU201314_04U,AU201314_07U,\n",
    " AU201314_09U,AU201314_10U,AU201314_14U,AU201314_15U,AU201314_16U,AU201415_04U,\n",
    " AU201415_13U,AU201415_14U,AU201415_16U,AU201415_17U,AU201415_19U,AU201415_21U,\n",
    " AU201415_24U,AU201516_08U,AU201516_09U,AU201516_10U,AU201617_07U,AU201617_19U,\n",
    " AU201617_20U,AU201617_23U,AU201617_24U,AU201617_26U,AU201617_29U,AU201617_30U,\n",
    " AU201718_02U,AU201718_03U,AU201718_06U,AU201718_17U,AU201718_20U,AU201718_22U,\n",
    " AU201718_24U,AU201819_04U,AU201819_05U,AU201819_07U,AU201819_12U,AU201819_14U,\n",
    " AU201819_17U,AU201819_19U,AU201819_20U,AU201819_21U,AU201819_25U,AU201819_26U,\n",
    " AU201920_03U,AU201920_05U,AU201920_06U,AU201920_07U,AU201920_08U,AU201920_10U,\n",
    " AU201920_12U,AU202021_07U,AU202021_09U,AU202021_10U,AU202021_11U,AU202021_15U,\n",
    " AU202021_17U,AU202021_22U,AU202021_23U,AU202122_02U,AU202122_05U,AU202122_07U,\n",
    " AU202122_08U,AU202122_10U,AU202122_18U,AU202122_22U,AU202122_23U,AU202122_27U,\n",
    " AU202122_28U,AU202122_36U,AU202223_01U,AU202223_05U,AU202223_13U,AU202223_14U,\n",
    " AU202223_21U,AU202223_23U,AU202324_02U,AU202324_04U,AU202324_05U,AU202324_08U,\n",
    " AU202324_09U,AU202324_11U,\n",
    " AU202324_13U) = (d[\"AU199697_14U\"],d[\"AU199899_05U\"],d[\"AU199900_13U\"],d[\"AU200001_06U\"],\n",
    "                  d[\"AU200203_01U\"],d[\"AU200203_02U\"],d[\"AU200203_03U\"],d[\"AU200203_04U\"],\n",
    "                  d[\"AU200203_06U\"],d[\"AU200203_07U\"],d[\"AU200304_01U\"],d[\"AU200304_02U\"],\n",
    "                  d[\"AU200304_03U\"],d[\"AU200304_04U\"],d[\"AU200304_05U\"],d[\"AU200304_07U\"],\n",
    "                  d[\"AU200304_08U\"],d[\"AU200304_09U\"],d[\"AU200304_11U\"],d[\"AU200405_01U\"],\n",
    "                  d[\"AU200405_02U\"],d[\"AU200405_03U\"],d[\"AU200405_04U\"],d[\"AU200405_05U\"],\n",
    "                  d[\"AU200405_07U\"],d[\"AU200405_08U\"],d[\"AU200405_09U\"],d[\"AU200405_10U\"],\n",
    "                  d[\"AU200506_01U\"],d[\"AU200506_05U\"],d[\"AU200506_06U\"],d[\"AU200506_08U\"],\n",
    "                  d[\"AU200506_10U\"],d[\"AU200506_14U\"],d[\"AU200506_15U\"],d[\"AU200506_16U\"],\n",
    "                  d[\"AU200506_17U\"],d[\"AU200506_20U\"],d[\"AU200506_22U\"],d[\"AU200607_05U\"],\n",
    "                  d[\"AU200607_11U\"],d[\"AU200607_12U\"],d[\"AU200607_13U\"],d[\"AU200607_16U\"],\n",
    "                  d[\"AU200708_01U\"],d[\"AU200708_03U\"],d[\"AU200708_06U\"],d[\"AU200708_08U\"],\n",
    "                  d[\"AU200708_11U\"],d[\"AU200708_15U\"],d[\"AU200708_20U\"],d[\"AU200708_22U\"],\n",
    "                  d[\"AU200708_23U\"],d[\"AU200809_02U\"],d[\"AU200809_03U\"],d[\"AU200809_06U\"],\n",
    "                  d[\"AU200809_08U\"],d[\"AU200809_10U\"],d[\"AU200809_17U\"],d[\"AU200809_18U\"],\n",
    "                  d[\"AU200809_23U\"],d[\"AU200910_01U\"],d[\"AU200910_06U\"],d[\"AU200910_07U\"],\n",
    "                  d[\"AU200910_09U\"],d[\"AU200910_11U\"],d[\"AU200910_12U\"],d[\"AU200910_13U\"],\n",
    "                  d[\"AU201011_02U\"],d[\"AU201011_09U\"],d[\"AU201011_10U\"],d[\"AU201011_11U\"],\n",
    "                  d[\"AU201011_12U\"],d[\"AU201011_14U\"],d[\"AU201011_16U\"],d[\"AU201011_17U\"],\n",
    "                  d[\"AU201011_29U\"],d[\"AU201112_01U\"],d[\"AU201112_04U\"],d[\"AU201112_07U\"],\n",
    "                  d[\"AU201112_11U\"],d[\"AU201112_12U\"],d[\"AU201112_15U\"],d[\"AU201112_16U\"],\n",
    "                  d[\"AU201213_03U\"],d[\"AU201213_04U\"],d[\"AU201213_05U\"],d[\"AU201213_10U\"],\n",
    "                  d[\"AU201213_13U\"],d[\"AU201213_14U\"],d[\"AU201213_17U\"],d[\"AU201213_18U\"],\n",
    "                  d[\"AU201314_01U\"],d[\"AU201314_03U\"],d[\"AU201314_04U\"],d[\"AU201314_07U\"],\n",
    "                  d[\"AU201314_09U\"],d[\"AU201314_10U\"],d[\"AU201314_14U\"],d[\"AU201314_15U\"],\n",
    "                  d[\"AU201314_16U\"],d[\"AU201415_04U\"],d[\"AU201415_13U\"],d[\"AU201415_14U\"],\n",
    "                  d[\"AU201415_16U\"],d[\"AU201415_17U\"],d[\"AU201415_19U\"],d[\"AU201415_21U\"],\n",
    "                  d[\"AU201415_24U\"],d[\"AU201516_08U\"],d[\"AU201516_09U\"],d[\"AU201516_10U\"],\n",
    "                  d[\"AU201617_07U\"],d[\"AU201617_19U\"],d[\"AU201617_20U\"],d[\"AU201617_23U\"],\n",
    "                  d[\"AU201617_24U\"],d[\"AU201617_26U\"],d[\"AU201617_29U\"],d[\"AU201617_30U\"],\n",
    "                  d[\"AU201718_02U\"],d[\"AU201718_03U\"],d[\"AU201718_06U\"],d[\"AU201718_17U\"],\n",
    "                  d[\"AU201718_20U\"],d[\"AU201718_22U\"],d[\"AU201718_24U\"],d[\"AU201819_04U\"],\n",
    "                  d[\"AU201819_05U\"],d[\"AU201819_07U\"],d[\"AU201819_12U\"],d[\"AU201819_14U\"],\n",
    "                  d[\"AU201819_17U\"],d[\"AU201819_19U\"],d[\"AU201819_20U\"],d[\"AU201819_21U\"],\n",
    "                  d[\"AU201819_25U\"],d[\"AU201819_26U\"],d[\"AU201920_03U\"],d[\"AU201920_05U\"],\n",
    "                  d[\"AU201920_06U\"],d[\"AU201920_07U\"],d[\"AU201920_08U\"],d[\"AU201920_10U\"],\n",
    "                  d[\"AU201920_12U\"],d[\"AU202021_07U\"],d[\"AU202021_09U\"],d[\"AU202021_10U\"],\n",
    "                  d[\"AU202021_11U\"],d[\"AU202021_15U\"],d[\"AU202021_17U\"],d[\"AU202021_22U\"],\n",
    "                  d[\"AU202021_23U\"],d[\"AU202122_02U\"],d[\"AU202122_05U\"],d[\"AU202122_07U\"],\n",
    "                  d[\"AU202122_08U\"],d[\"AU202122_10U\"],d[\"AU202122_18U\"],d[\"AU202122_22U\"],\n",
    "                  d[\"AU202122_23U\"],d[\"AU202122_27U\"],d[\"AU202122_28U\"],d[\"AU202122_36U\"],\n",
    "                  d[\"AU202223_01U\"],d[\"AU202223_05U\"],d[\"AU202223_13U\"],d[\"AU202223_14U\"],\n",
    "                  d[\"AU202223_21U\"],d[\"AU202223_23U\"],d[\"AU202324_02U\"],d[\"AU202324_04U\"],\n",
    "                  d[\"AU202324_05U\"],d[\"AU202324_08U\"],d[\"AU202324_09U\"],d[\"AU202324_11U\"],\n",
    "                  d[\"AU202324_13U\"])\n",
    "\n",
    "df_all = [AU199697_14U,AU199899_05U,AU199900_13U,AU200001_06U,AU200203_01U,AU200203_02U,\n",
    "          AU200203_03U,AU200203_04U,AU200203_06U,AU200203_07U,AU200304_01U,AU200304_02U,\n",
    "          AU200304_03U,AU200304_04U,AU200304_05U,AU200304_07U,AU200304_08U,AU200304_09U,\n",
    "          AU200304_11U,AU200405_01U,AU200405_02U,AU200405_03U,AU200405_04U,AU200405_05U,\n",
    "          AU200405_07U,AU200405_08U,AU200405_09U,AU200405_10U,AU200506_01U,AU200506_05U,\n",
    "          AU200506_06U,AU200506_08U,AU200506_10U,AU200506_14U,AU200506_15U,AU200506_16U,\n",
    "          AU200506_17U,AU200506_20U,AU200506_22U,AU200607_05U,AU200607_11U,AU200607_12U,\n",
    "          AU200607_13U,AU200607_16U,AU200708_01U,AU200708_03U,AU200708_06U,AU200708_08U,\n",
    "          AU200708_11U,AU200708_15U,AU200708_20U,AU200708_22U,AU200708_23U,AU200809_02U,\n",
    "          AU200809_03U,AU200809_06U,AU200809_08U,AU200809_10U,AU200809_17U,AU200809_18U,\n",
    "          AU200809_23U,AU200910_01U,AU200910_06U,AU200910_07U,AU200910_09U,AU200910_11U,\n",
    "          AU200910_12U,AU200910_13U,AU201011_02U,AU201011_09U,AU201011_10U,AU201011_11U,\n",
    "          AU201011_12U,AU201011_14U,AU201011_16U,AU201011_17U,AU201011_29U,AU201112_01U,\n",
    "          AU201112_04U,AU201112_07U,AU201112_11U,AU201112_12U,AU201112_15U,AU201112_16U,\n",
    "          AU201213_03U,AU201213_04U,AU201213_05U,AU201213_10U,AU201213_13U,AU201213_14U,\n",
    "          AU201213_17U,AU201213_18U,AU201314_01U,AU201314_03U,AU201314_04U,AU201314_07U,\n",
    "          AU201314_09U,AU201314_10U,AU201314_14U,AU201314_15U,AU201314_16U,AU201415_04U,\n",
    "          AU201415_13U,AU201415_14U,AU201415_16U,AU201415_17U,AU201415_19U,AU201415_21U,\n",
    "          AU201415_24U,AU201516_08U,AU201516_09U,AU201516_10U,AU201617_07U,AU201617_19U,\n",
    "          AU201617_20U,AU201617_23U,AU201617_24U,AU201617_26U,AU201617_29U,AU201617_30U,\n",
    "          AU201718_02U,AU201718_03U,AU201718_06U,AU201718_17U,AU201718_20U,AU201718_22U,\n",
    "          AU201718_24U,AU201819_04U,AU201819_05U,AU201819_07U,AU201819_12U,AU201819_14U,\n",
    "          AU201819_17U,AU201819_19U,AU201819_20U,AU201819_21U,AU201819_25U,AU201819_26U,\n",
    "          AU201920_03U,AU201920_05U,AU201920_06U,AU201920_07U,AU201920_08U,AU201920_10U,\n",
    "          AU201920_12U,AU202021_07U,AU202021_09U,AU202021_10U,AU202021_11U,AU202021_15U,\n",
    "          AU202021_17U,AU202021_22U,AU202021_23U,AU202122_02U,AU202122_05U,AU202122_07U,\n",
    "          AU202122_08U,AU202122_10U,AU202122_18U,AU202122_22U,AU202122_23U,AU202122_27U,\n",
    "          AU202122_28U,AU202122_36U,AU202223_01U,AU202223_05U,AU202223_13U,AU202223_14U,\n",
    "          AU202223_21U,AU202223_23U,AU202324_02U,AU202324_04U,AU202324_05U,AU202324_08U,\n",
    "          AU202324_09U,AU202324_11U,AU202324_13U]\n",
    "\n",
    "### For RQ3 ###\n",
    "# West region only\n",
    "\n",
    "df_west = [AU201920_05U,AU202122_05U,AU201617_29U,AU200910_06U,AU200708_22U,AU202021_07U,AU201819_12U,AU201314_03U,\n",
    "           AU200203_04U,AU200809_18U,AU201314_09U,AU200405_03U,AU201617_20U,AU201617_30U,AU202122_28U,AU200708_20U,\n",
    "           AU201213_04U,AU200506_06U,AU201617_23U,AU202223_13U,AU201718_06U,AU199697_14U,AU201011_12U,AU200809_02U,\n",
    "           AU201516_08U,AU202122_36U,AU201112_15U,AU200607_13U,AU201415_19U,AU200405_08U,AU201920_08U,AU200607_11U,\n",
    "           AU200708_15U,AU202021_15U,AU200607_12U,AU200708_06U,AU202223_05U,AU200203_03U,AU200304_03U,AU200910_12U,\n",
    "           AU200304_05U,AU200405_02U,AU202324_08U,AU200203_02U,AU200405_09U,AU201314_16U,AU200708_11U,AU200304_08U,\n",
    "           AU201718_02U,AU201920_03U,AU200809_10U,AU200708_23U,AU200506_01U,AU200506_20U,AU200708_01U,AU201617_26U,\n",
    "           AU201819_17U,AU202122_27U,AU201011_29U,AU202122_23U,AU202122_22U,AU201415_21U,AU201819_05U,AU199899_05U,\n",
    "           AU199900_13U,AU200506_14U,AU202324_11U,AU200809_03U,AU201617_07U,AU201011_09U,AU201819_21U,AU201718_17U,\n",
    "           AU200304_07U,AU200506_16U,AU202021_22U,AU201112_07U,AU202021_10U,AU200304_11U,AU200506_05U,AU200809_08U,\n",
    "           AU202223_01U,AU201112_16U,AU201213_10U,AU201415_16U,AU201819_19U,AU202324_04U,AU200405_04U,AU200203_07U,\n",
    "           AU201819_25U,AU201011_02U,AU200304_01U,AU201516_09U,AU202021_23U,AU201213_17U,AU201314_14U,AU202223_23U,\n",
    "           AU201213_05U,AU201415_04U,AU200405_01U,AU201011_16U,AU201112_01U,AU202122_02U,AU201718_03U,AU201112_11U,\n",
    "           AU202223_21U,AU200910_01U,AU200910_13U,AU201314_04U]\n",
    "\n",
    "### For RQ3 ###\n",
    "# East region only\n",
    "\n",
    "df_east = [AU201213_03U,AU202324_02U,AU202122_08U,AU201415_24U,AU200607_05U,AU201920_07U,AU200607_16U,AU200809_23U,\n",
    "           AU201011_11U,AU201516_10U,AU201011_14U,AU201213_13U,AU200910_07U,AU201718_24U,AU201314_10U,AU200506_08U,\n",
    "           AU201213_14U,AU200910_09U,AU201617_24U,AU202122_10U,AU200304_04U,AU200203_01U,AU201011_10U,AU201314_07U,\n",
    "           AU201415_14U,AU200304_02U,AU201819_14U,AU200809_06U,AU201819_04U,AU202324_13U,AU200506_22U,AU201718_22U,\n",
    "           AU201415_17U,AU201920_12U,AU201112_04U,AU202324_09U,AU200708_03U,AU202324_05U,AU202021_17U,AU202122_18U,\n",
    "           AU200001_06U,AU200304_09U,AU200506_15U,AU200809_17U,AU201920_10U,AU201314_15U,AU201819_20U,AU200910_11U,\n",
    "           AU200405_10U,AU201415_13U,AU200405_05U,AU202021_11U,AU201819_07U,AU202223_14U,AU200506_17U,AU201617_19U,\n",
    "           AU202122_07U,AU201920_06U,AU201213_18U,AU202021_09U,AU201112_12U,AU201819_26U,AU200506_10U]\n",
    "\n",
    "# Shared zone\n",
    "\n",
    "df_shared = [AU201011_17U,AU200405_07U,AU200203_06U,AU201314_01U,AU200708_08U,AU201718_20U]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c81ca",
   "metadata": {},
   "source": [
    "### DATA MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0603173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "def generate_torch_kwargs():\n",
    "    # run torch models on CPU, and disable progress bars for all model stages except training.\n",
    "    # for reproducibility\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    return {\n",
    "        \"pl_trainer_kwargs\": {\n",
    "            \"accelerator\": \"cpu\",\n",
    "            \"callbacks\": [TFMProgressBar(enable_train_bar_only=True)],\n",
    "        }\n",
    "    }\n",
    "\n",
    "def modelfitting(tscols=['LAT',\n",
    "                         'LON',\n",
    "                         'CENTRAL_PRES',\n",
    "                         'ENV_PRES',\n",
    "                         'MN_RADIUS_GF_SECNE',\n",
    "                         'MN_RADIUS_GF_SECSE',\n",
    "                         'MN_RADIUS_GF_SECSW',\n",
    "                         'MN_RADIUS_GF_SECNW'],\n",
    "                 df_list_of_cyclones=df_all,\n",
    "                 covariates=None):\n",
    "    \"\"\"\n",
    "    N-BEATS model fitting.\n",
    "    Take a name list of target features tscols.\n",
    "    Default tscols = ['LAT',\n",
    "          'LON',\n",
    "          'CENTRAL_PRES',\n",
    "          'ENV_PRES',\n",
    "          'MN_RADIUS_GF_SECNE',\n",
    "          'MN_RADIUS_GF_SECSE',\n",
    "          'MN_RADIUS_GF_SECSW',\n",
    "          'MN_RADIUS_GF_SECNW']\n",
    "    Take a df list of validating cyclones df_list_of_cyclones, default is df_all.\n",
    "    Return a dataframe for model evaluation report.\n",
    "    Return a dataframe for geoplot if 'LON' and 'LAT' are both in target features.\n",
    "    \"\"\"\n",
    "\n",
    "    # for reproducibility\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Storage for evaluation report\n",
    "    bt_act_df_list = []\n",
    "    bt_fc_df_list = []\n",
    "    sc_bt_act_df_list = []\n",
    "    sc_bt_fc_df_list = []\n",
    "    bt_error_list = []\n",
    "\n",
    "    # Flag used to monitor whether previous tracking data has been stored\n",
    "    flag_geo_plot_data = False\n",
    "\n",
    "    # Temporary variable to limit number of cyclones used for validation\n",
    "    max_cyclones = 25\n",
    "    cyclones_so_far = 1\n",
    "\n",
    "    for cyclone in df_list_of_cyclones:\n",
    "        cyclone_name = str(cyclone.iat[0, 1])\n",
    "        print(\"Length = \" + str(len(cyclone)))\n",
    "        print(\"Cyclone number = \" + str(cyclones_so_far))\n",
    "        if cyclones_so_far > max_cyclones:\n",
    "            break\n",
    "        print(\"Cyclone ID: \" + str(cyclone.iat[0, 1]))\n",
    "        # Check length of this cyclone data\n",
    "        if len(cyclone) > 13:\n",
    "            cycdf = d.get(cyclone_name)\n",
    "\n",
    "            # Validation Target series\n",
    "            validation_ts = TimeSeries.from_dataframe(cycdf, \"TM\", tscols)\n",
    "            # Scaling\n",
    "            validation_scaler = Scaler()\n",
    "            # Scale Validation data before splitting\n",
    "            validation_data_scaled = validation_scaler.fit_transform(validation_ts)\n",
    "            # Keep the last 8 obs for validation of prediction\n",
    "            (train_validation_data_scaled,\n",
    "             actual_validation_data_scaled) = (validation_data_scaled[:-8],\n",
    "                                               validation_data_scaled[-8:])\n",
    "\n",
    "            # Create temporary copy of dictionary\n",
    "            dd = {key: value[:] for key, value in d.items()}\n",
    "            # Remove validation cyclone from temp dictionary\n",
    "            dd.pop(cyclone.iat[0, 1])\n",
    "\n",
    "            # Only add cyclones with sufficient number of observations for model to work\n",
    "            cyclones = []\n",
    "            train_cyc_id = []\n",
    "            for ea in dd:\n",
    "                this_cyclone = ea\n",
    "                if len(dd.get(this_cyclone)) > 11:\n",
    "                    result = dd.get(this_cyclone)\n",
    "                    cyclones.append(result)\n",
    "                    train_cyc_id.append(this_cyclone)\n",
    "\n",
    "            # Handle target series\n",
    "\n",
    "            # Convert all target series in list to Time Series objects\n",
    "            ts_cyclones = []\n",
    "            for ea in cyclones:\n",
    "                result = TimeSeries.from_dataframe(ea, \"TM\", tscols)\n",
    "                ts_cyclones.append(result)\n",
    "\n",
    "            # Scale all series in list\n",
    "            sc_ts_cyclone_list = []\n",
    "            for ea in ts_cyclones:\n",
    "                new_scaler = Scaler()\n",
    "                result = new_scaler.fit_transform(ea)\n",
    "                sc_ts_cyclone_list.append(result)\n",
    "\n",
    "            # Handle covariates\n",
    "            if covariates is not None:\n",
    "\n",
    "                # Validation Covariate series\n",
    "                val_cov_ts = TimeSeries.from_dataframe(cycdf, \"TM\", covariates)\n",
    "                val_cov_scaler = Scaler()\n",
    "                # Scale covariate series\n",
    "                val_cov_scaled = val_cov_scaler.fit_transform(val_cov_ts)\n",
    "\n",
    "                # Training covariate series\n",
    "\n",
    "                # Convert all covariate series in list to Time Series objects\n",
    "                past_cov = []\n",
    "                for ea in cyclones:\n",
    "                    result = TimeSeries.from_dataframe(ea, \"TM\", covariates)\n",
    "                    past_cov.append(result)\n",
    "\n",
    "                # Scale all series in list\n",
    "                sc_past_cov = []\n",
    "                for ea in past_cov:\n",
    "                    new_scaler = Scaler()\n",
    "                    result = new_scaler.fit_transform(ea)\n",
    "                    sc_past_cov.append(result)\n",
    "            else:\n",
    "                sc_past_cov = None\n",
    "                val_cov_scaled = None\n",
    "\n",
    "            # Train model\n",
    "            model_NBEATS = NBEATSModel(\n",
    "                input_chunk_length=6,\n",
    "                output_chunk_length=4,\n",
    "                n_epochs=30,\n",
    "                random_state=0,\n",
    "                **generate_torch_kwargs())\n",
    "\n",
    "            model_NBEATS.fit(sc_ts_cyclone_list,\n",
    "                             past_covariates=sc_past_cov)\n",
    "\n",
    "            # BACKTEST\n",
    "\n",
    "            # HISTORICAL FORECASTS for SCALED DATA UNIVARIATE COMPONENT\n",
    "            sc_bt_fc = model_NBEATS.historical_forecasts(validation_data_scaled,\n",
    "                                                         forecast_horizon=8,\n",
    "                                                         retrain=False,\n",
    "                                                         last_points_only=False,\n",
    "                                                         past_covariates=val_cov_scaled)\n",
    "\n",
    "            # Create list of forecasts\n",
    "            sc_bt_fc_list = [np.zeros(len(tscols))]\n",
    "            for i in range(0, len(sc_bt_fc)):\n",
    "                a = sc_bt_fc[i].values()\n",
    "                sc_bt_fc_list = np.concatenate([sc_bt_fc_list, a])\n",
    "            sc_bt_fc_list = sc_bt_fc_list[1:]\n",
    "\n",
    "            # Create list of run no and cyc id\n",
    "            run_no = [cyclones_so_far] * 8 * len(sc_bt_fc)\n",
    "            cyc_id = [cyclone_name] * 8 * len(sc_bt_fc)\n",
    "\n",
    "            # Create list of corresponding actual values\n",
    "            sc_bt_act_list = [np.zeros(len(tscols))]\n",
    "            for i in range(0, len(sc_bt_fc)):\n",
    "                start = 6 + i\n",
    "                end = 6 + i + 8\n",
    "                a = validation_data_scaled[start:end].values()\n",
    "                sc_bt_act_list = np.concatenate([sc_bt_act_list, a])\n",
    "            sc_bt_act_list = sc_bt_act_list[1:]\n",
    "\n",
    "            # Create list of index of forecasts\n",
    "            sc_obs = np.array(range(1, 9))\n",
    "            for i in range(len(sc_bt_fc) - 1):\n",
    "                b = np.array(range(1, 9))\n",
    "                sc_obs = np.concatenate([sc_obs, b])\n",
    "\n",
    "            # Create df for actual for output\n",
    "            sc_bt_act_df = pd.DataFrame(index=range(len(sc_bt_act_list)),\n",
    "                                        data=sc_bt_act_list,\n",
    "                                        columns=tscols)\n",
    "            sc_bt_act_df['Obs'] = sc_obs\n",
    "            sc_bt_act_df['run_no'] = run_no\n",
    "            sc_bt_act_df['cyc_id'] = cyc_id\n",
    "\n",
    "            # Create df for forecast for output\n",
    "            sc_bt_fc_df = pd.DataFrame(index=range(len(sc_bt_fc_list)),\n",
    "                                       data=sc_bt_fc_list,\n",
    "                                       columns=tscols)\n",
    "            sc_bt_fc_df['Obs'] = sc_obs\n",
    "            sc_bt_fc_df['run_no'] = run_no\n",
    "            sc_bt_fc_df['cyc_id'] = cyc_id\n",
    "\n",
    "            # HISTORICAL FORECASTS for ORIGINAL DATA UNIVARIATE COMPONENT\n",
    "\n",
    "            # Inverse scaling scaled historical forecasts\n",
    "            bt_fc = []\n",
    "            for ts in sc_bt_fc:\n",
    "                inv_sc_bt = validation_scaler.inverse_transform(ts)\n",
    "                bt_fc.append(inv_sc_bt)\n",
    "\n",
    "                # Create list of forecasts\n",
    "            bt_fc_list = [np.zeros(len(tscols))]\n",
    "            for i in range(0, len(bt_fc)):\n",
    "                a = bt_fc[i].values()\n",
    "                bt_fc_list = np.concatenate([bt_fc_list, a])\n",
    "            bt_fc_list = bt_fc_list[1:]\n",
    "\n",
    "            # Create list of corresponding actual values\n",
    "            bt_act_list = [np.zeros(len(tscols))]\n",
    "            for i in range(0, len(bt_fc)):\n",
    "                start = 6 + i\n",
    "                end = 6 + i + 8\n",
    "                a = validation_ts[start:end].values()\n",
    "                bt_act_list = np.concatenate([bt_act_list, a])\n",
    "            bt_act_list = bt_act_list[1:]\n",
    "\n",
    "            # Create list of index of forecasts\n",
    "            obs = np.array(range(1, 9))\n",
    "            for i in range(len(bt_fc) - 1):\n",
    "                b = np.array(range(1, 9))\n",
    "                obs = np.concatenate([obs, b])\n",
    "\n",
    "            # Create df for actual for output\n",
    "            bt_act_df = pd.DataFrame(index=range(len(bt_act_list)),\n",
    "                                     data=bt_act_list,\n",
    "                                     columns=tscols)\n",
    "            bt_act_df['Obs'] = obs\n",
    "            bt_act_df['run_no'] = run_no\n",
    "            bt_act_df['cyc_id'] = cyc_id\n",
    "\n",
    "            # Create df for forecast for output\n",
    "            bt_fc_df = pd.DataFrame(index=range(len(bt_fc_list)),\n",
    "                                    data=bt_fc_list,\n",
    "                                    columns=tscols)\n",
    "            bt_fc_df['Obs'] = obs\n",
    "            bt_fc_df['run_no'] = run_no\n",
    "            bt_fc_df['cyc_id'] = cyc_id\n",
    "\n",
    "            # BACK TEST FOR MODEL GENERAL PERFORMANCE\n",
    "            bt_error = model_NBEATS.backtest(validation_data_scaled,\n",
    "                                             metric=[mae, rmse, mse],\n",
    "                                             forecast_horizon=8,\n",
    "                                             retrain=False,\n",
    "                                             past_covariates=val_cov_scaled)\n",
    "            bt_dict = {'run_no': cyclones_so_far,\n",
    "                       'cyc_id': cyclone_name,\n",
    "                       'MAE': bt_error[0],\n",
    "                       'RMSE': bt_error[1],\n",
    "                       'MSE': bt_error[2]}\n",
    "            bt_error_df = pd.DataFrame(bt_dict, index=[cyclones_so_far - 1])\n",
    "\n",
    "            # Append to storage list\n",
    "            bt_act_df_list.append(bt_act_df)\n",
    "            bt_fc_df_list.append(bt_fc_df)\n",
    "            sc_bt_act_df_list.append(sc_bt_act_df)\n",
    "            sc_bt_fc_df_list.append(sc_bt_fc_df)\n",
    "            bt_error_list.append(bt_error_df)\n",
    "\n",
    "            # Run prediction train_validation_data_scaled for plotting\n",
    "            sc_forecast_NBEATS = model_NBEATS.predict(n=8,\n",
    "                                                      series=train_validation_data_scaled,\n",
    "                                                      past_covariates=val_cov_scaled)\n",
    "\n",
    "            # Reverse scaling (at n=8 for plotting)\n",
    "            forecast_NBEATS = validation_scaler.inverse_transform(sc_forecast_NBEATS)\n",
    "            actual_validation = validation_scaler.inverse_transform(actual_validation_data_scaled)\n",
    "\n",
    "            if 'LON' in tscols and 'LAT' in tscols:\n",
    "                for idx, component in enumerate(tscols):\n",
    "                    if component == 'LAT':\n",
    "                        variable1_pred = forecast_NBEATS.univariate_component(idx)\n",
    "                        variable1_act = actual_validation.univariate_component(idx)\n",
    "                        variable1_past = validation_ts[:-8].univariate_component(idx)\n",
    "\n",
    "                    if component == 'LON':\n",
    "                        variable2_pred = forecast_NBEATS.univariate_component(idx)\n",
    "                        variable2_act = actual_validation.univariate_component(idx)\n",
    "                        variable2_past = validation_ts[:-8].univariate_component(idx)\n",
    "\n",
    "                variable1_pred_values = variable1_pred.values()\n",
    "                variable2_pred_values = variable2_pred.values()\n",
    "                variable1_act_values = variable1_act.values()\n",
    "                variable2_act_values = variable2_act.values()\n",
    "                variable1_past_values = variable1_past.values()\n",
    "                variable2_past_values = variable2_past.values()\n",
    "\n",
    "                # Concatenate values of the past, actual & prediction\n",
    "                #  data to make plots continuous\n",
    "                variable1_existing_values = np.concatenate([variable1_past_values,\n",
    "                                                            variable1_act_values])\n",
    "                variable2_existing_values = np.concatenate([variable2_past_values,\n",
    "                                                            variable2_act_values])\n",
    "                variable1_pred_values = np.concatenate([variable1_past_values,\n",
    "                                                        variable1_pred_values])\n",
    "                variable2_pred_values = np.concatenate([variable2_past_values,\n",
    "                                                        variable2_pred_values])\n",
    "\n",
    "                # Create datetime variables\n",
    "                TM_act = variable1_act.time_index\n",
    "                TM_past = variable1_past.time_index\n",
    "                TM_exist = np.concatenate([TM_past, TM_act])\n",
    "\n",
    "                # Create dataframe for plotting with geopandas\n",
    "                geo_plot_all = pd.DataFrame(index=range(len(variable1_existing_values)))\n",
    "                geo_plot_all['LAT'] = variable1_existing_values\n",
    "                geo_plot_all['LON'] = variable2_existing_values\n",
    "                geo_plot_all['TM'] = TM_exist\n",
    "                geo_plot_all['DISTURBANCE_ID'] = str(cyclone_name + \"-Real\")\n",
    "                geo_plot_pred = pd.DataFrame(index=range(len(variable1_pred_values)))\n",
    "                geo_plot_pred['LAT'] = variable1_pred_values\n",
    "                geo_plot_pred['LON'] = variable2_pred_values\n",
    "                geo_plot_pred['TM'] = TM_exist\n",
    "                geo_plot_pred['DISTURBANCE_ID'] = str(cyclone_name + \"-Predicted\")\n",
    "                if not flag_geo_plot_data:\n",
    "                    geo_plot_data = pd.concat([geo_plot_pred,\n",
    "                                               geo_plot_all],\n",
    "                                              ignore_index=True)\n",
    "                    flag_geo_plot_data = True\n",
    "                else:\n",
    "                    geo_plot_data = pd.concat([geo_plot_pred,\n",
    "                                               geo_plot_all,\n",
    "                                               geo_plot_data],\n",
    "                                              ignore_index=True)\n",
    "\n",
    "            cyclones_so_far += 1\n",
    "    bt_out = [bt_act_df_list,\n",
    "              bt_fc_df_list,\n",
    "              sc_bt_act_df_list,\n",
    "              sc_bt_fc_df_list,\n",
    "              bt_error_list]\n",
    "    for m, n in enumerate(bt_out):\n",
    "        bt_out[m] = pd.concat(n)\n",
    "    if 'LON' in tscols and 'LAT' in tscols:\n",
    "        return [bt_out,\n",
    "                tscols,\n",
    "                len(cyclones),\n",
    "                cyclones_so_far - 1,\n",
    "                covariates,\n",
    "                train_cyc_id,\n",
    "                geo_plot_data]\n",
    "    else:\n",
    "        return [bt_out,\n",
    "                tscols,\n",
    "                len(cyclones),\n",
    "                cyclones_so_far - 1,\n",
    "                covariates,\n",
    "                train_cyc_id]\n",
    "\n",
    "# Self-defined function for Geoplotting\n",
    "def geoplot(geo_plot_data, plotname='trajectory_plot.html'):\n",
    "    \"\"\"\n",
    "    Mapping cyclone path.\n",
    "    Take in a data frame containing data for plotting.\n",
    "    Allow input of plotname, default plotname = 'trajectory_plot.html'.\n",
    "    \"\"\"\n",
    "    # Set CRS to WGS84 coordinate system\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "    # Convert Longitude and Latitude to Points\n",
    "    geometry = [Point(xy) for xy in zip(geo_plot_data['LON'], geo_plot_data['LAT'])]\n",
    "    geo_data = gpd.GeoDataFrame(geo_plot_data, geometry=geometry)\n",
    "\n",
    "    # Create trajectories\n",
    "    geo_data['Time'] = pd.to_datetime(geo_data['TM'],\n",
    "                                      format='%d/%m/%Y %H:%M',\n",
    "                                      errors='coerce')\n",
    "    geo_data = geo_data.set_index('Time')\n",
    "    # Specify minimum length of trajectories\n",
    "    minimum_length = 0\n",
    "\n",
    "    # Create Trajectory Collection\n",
    "    traj_collection = mpd.TrajectoryCollection(geo_data,\n",
    "                                               'DISTURBANCE_ID',\n",
    "                                               min_length=minimum_length)\n",
    "\n",
    "    # Create DataFrame to store trajectory coordinates\n",
    "    trajectory_df = pd.DataFrame(columns=['DISTURBANCE_ID', 'LON', 'LAT'])\n",
    "\n",
    "    # For each trajectory in the TrajectoryCollection\n",
    "    #  Extract coordinates and add to the trajectory DataFrame\n",
    "    for traj in traj_collection:\n",
    "        coords = traj.to_linestring().coords[:]\n",
    "        traj_id = traj.id\n",
    "        for LON, LAT in coords:\n",
    "            trajectory_df = pd.concat([trajectory_df,\n",
    "                                       pd.DataFrame([{'DISTURBANCE_ID': traj_id,\n",
    "                                                      'LON': LON,\n",
    "                                                      'LAT': LAT}])],\n",
    "                                      ignore_index=True)\n",
    "    # Create plot\n",
    "    fig = px.line_mapbox(trajectory_df,\n",
    "                         lat='LAT', lon='LON',\n",
    "                         color='DISTURBANCE_ID',\n",
    "                         hover_name='DISTURBANCE_ID',\n",
    "                         zoom=4)\n",
    "    # Set layout options for map\n",
    "    fig.update_layout(mapbox_style=\"carto-positron\",\n",
    "                      mapbox_zoom=4,\n",
    "                      mapbox_center={\"lat\": trajectory_df['LAT'].mean(),\n",
    "                                     \"lon\": trajectory_df['LON'].mean()})\n",
    "    # Write html file containing interactive plot\n",
    "    fig.write_html(plotname)\n",
    "    print(f\"A geoplot named '{plotname}' was created.\")\n",
    "\n",
    "def unicom_rep(scaled=False):\n",
    "    \"\"\"\n",
    "    Univariate component report.\n",
    "    Default scaled = False to report on original series.\n",
    "    Change scaled = True to report on scaled series.\n",
    "    \"\"\"\n",
    "    tscols = target_col\n",
    "\n",
    "    print('\\n')\n",
    "    print('#' * 50)\n",
    "    print('UNIVARIATE COMPONENT REPORT\\n')\n",
    "    print('#' * 50)\n",
    "    if scaled == True:\n",
    "        bt_act_df = sc_bt_act\n",
    "        bt_fc_df = sc_bt_fc\n",
    "        print('Table of Scaled Error for Each Component.')\n",
    "        print('\\nError Scores: scaled between 0 to 1')\n",
    "    else:\n",
    "        bt_act_df = bt_act\n",
    "        bt_fc_df = bt_fc\n",
    "        print('Table of Error Scores for Each Component.')\n",
    "    print('#' * 50)\n",
    "    print('\\n')\n",
    "    for cycid in bt_fc_df['cyc_id'].unique():\n",
    "        for comp in tscols:\n",
    "            print('\\nCyclone ID: ', cycid)\n",
    "            print('Cyclone Name:', compare_dict[cycid])\n",
    "            print('Component: ', comp)\n",
    "\n",
    "            bt_rmse = []\n",
    "            bt_mse = []\n",
    "            bt_mae = []\n",
    "            bt_obs = []\n",
    "            bt_len = []\n",
    "            for g in bt_fc_df['Obs'].unique():\n",
    "                mask_act = bt_act_df.loc[(bt_act_df['Obs'] == g) &\n",
    "                                         (bt_act_df['cyc_id'] == cycid), comp]\n",
    "                mask_fc = bt_fc_df.loc[(bt_fc_df['Obs'] == g) &\n",
    "                                       (bt_fc_df['cyc_id'] == cycid), comp]\n",
    "\n",
    "                rmse = mean_squared_error(mask_act, mask_fc, squared=False)\n",
    "                mse = mean_squared_error(mask_act, mask_fc, squared=True)\n",
    "                mae = mean_absolute_error(mask_act, mask_fc)\n",
    "\n",
    "                bt_rmse.append(rmse)\n",
    "                bt_mse.append(mse)\n",
    "                bt_mae.append(mae)\n",
    "                bt_obs.append(g)\n",
    "                bt_len.append(len(mask_fc))\n",
    "\n",
    "            bt_rep = pd.DataFrame(list(zip(bt_len, bt_mae, bt_rmse, bt_mse)),\n",
    "                                  columns=['BT_sets', 'MAE', 'RMSE', 'MSE'])\n",
    "\n",
    "            bt_rep = bt_rep.transpose().round(2)\n",
    "            headers = ['6h\\n(n=1)', '12h\\n(n=2)', '18h\\n(n=3)', '24h\\n(n=4)',\n",
    "                       '30h\\n(n=5)', '36h\\n(n=6)', '42h\\n(n=7)', '48h\\n(n=8)']\n",
    "\n",
    "            if scaled == False:\n",
    "                if comp in ['LAT', 'LON']:\n",
    "                    nm = bt_rep.copy()\n",
    "                    nm.loc[['MAE', 'RMSE', 'MSE']] = nm.loc[['MAE', 'RMSE', 'MSE']] * 60\n",
    "                    print('\\nError Units: nautical miles (nm)')\n",
    "                    print(tabulate(nm, headers=headers, tablefmt='psql'))\n",
    "                    print('\\nError Units: decimal degrees')\n",
    "\n",
    "                elif comp in ['MN_RADIUS_GF_SECNE',\n",
    "                              'MN_RADIUS_GF_SECSE',\n",
    "                              'MN_RADIUS_GF_SECSW',\n",
    "                              'MN_RADIUS_GF_SECNW']:\n",
    "                    print('\\nError Units: kilometres')\n",
    "\n",
    "                elif comp in ['ENV_PRES', 'CENTRAL_PRES']:\n",
    "                    print('\\nError Units: hectopascals')\n",
    "\n",
    "            elif scaled == True:\n",
    "                print('\\nError Units: scaled between 0 and 1')\n",
    "\n",
    "            print(tabulate(bt_rep, headers=headers, tablefmt='psql'))\n",
    "            print('\\n')\n",
    "            print('#' * 50)\n",
    "            print('\\n')\n",
    "\n",
    "def eval_rep(scaled=True):\n",
    "    \"\"\"\n",
    "    Generate MODEL EVALUATION REPORT FOR ALL COMPONENTS\n",
    "    \"\"\"\n",
    "    tscols = target_col\n",
    "    traincycount = traincycount_fit\n",
    "    valcycount = valcycount_fit\n",
    "    bt_error = bt_error_fit\n",
    "    covariates = covariates_fit\n",
    "\n",
    "    print('#' * 50)\n",
    "    print('MODEL EVALUATION REPORT FOR ALL COMPONENTS')\n",
    "    print('#' * 50)\n",
    "    print('\\nNumber of fitted target features: ' + str(len(tscols)))\n",
    "    print('List of fitted target features: ')\n",
    "    print([i for i in tscols])\n",
    "    print('List of fitted covariate features:')\n",
    "    if covariates is not None:\n",
    "        print([i for i in covariates])\n",
    "    else:\n",
    "        print('No covariate features.')\n",
    "    print('Number of training cyclones used for each run: ' + str(traincycount))\n",
    "    print('Number of validating cyclones used for each run: 1')\n",
    "    print('Number of validating runs: ' + str(valcycount))\n",
    "    print('Prediction at step n ahead: n= ', [i for i in range(1, 9)])\n",
    "\n",
    "    print('\\n' + '#' * 50)\n",
    "\n",
    "    if scaled == True:\n",
    "        bt_act_df = sc_bt_act\n",
    "        bt_fc_df = sc_bt_fc\n",
    "        print('Table of Scaled Error for Model Performance.')\n",
    "    else:\n",
    "        bt_act_df = bt_act\n",
    "        bt_fc_df = bt_fc\n",
    "        print('Table of Error Scores for Model Performance.')\n",
    "    print('#' * 50)\n",
    "    print('\\n')\n",
    "\n",
    "    for cycid in bt_fc_df['cyc_id'].unique():\n",
    "        print('\\nCyclone ID: ', cycid)\n",
    "        print('Cyclone Name:', compare_dict[cycid])\n",
    "        bt_rmse = []\n",
    "        bt_mse = []\n",
    "        bt_mae = []\n",
    "        bt_obs = []\n",
    "        bt_len = []\n",
    "        for g in bt_fc_df['Obs'].unique():\n",
    "            mask_act = bt_act_df.loc[(bt_act_df['Obs'] == g) &\n",
    "                                     (bt_act_df['cyc_id'] == cycid), tscols]\n",
    "            mask_fc = bt_fc_df.loc[(bt_fc_df['Obs'] == g) &\n",
    "                                   (bt_fc_df['cyc_id'] == cycid), tscols]\n",
    "\n",
    "            rmse = mean_squared_error(mask_act, mask_fc, squared=False)\n",
    "            mse = mean_squared_error(mask_act, mask_fc, squared=True)\n",
    "            mae = mean_absolute_error(mask_act, mask_fc)\n",
    "\n",
    "            bt_rmse.append(rmse)\n",
    "            bt_mse.append(mse)\n",
    "            bt_mae.append(mae)\n",
    "            bt_obs.append(g)\n",
    "            bt_len.append(len(mask_fc))\n",
    "\n",
    "        bt_rep = pd.DataFrame(list(zip(bt_len, bt_mae, bt_rmse, bt_mse)),\n",
    "                              columns=['BT_sets', 'MAE', 'RMSE', 'MSE'])\n",
    "\n",
    "        # Add Backtest error output\n",
    "        mask_bt_error = bt_error.loc[bt_error['cyc_id'] == cycid]\n",
    "        a = {'BT_sets': bt_len[0],\n",
    "             'MAE': mask_bt_error['MAE'],\n",
    "             'RMSE': mask_bt_error['RMSE'],\n",
    "             'MSE': mask_bt_error['MSE']}\n",
    "        bt_rep = pd.concat([bt_rep, pd.DataFrame(a)], ignore_index=True)\n",
    "\n",
    "        bt_rep = bt_rep.round(2).transpose()\n",
    "\n",
    "        bt_rep.columns = ['6h', '12h', '18h', '24h', '30h', '36h', '42h', '48h', 'Overall']\n",
    "        bt_rep = bt_rep[['Overall'] + [x for x in bt_rep.columns if x != 'Overall']]\n",
    "\n",
    "        headers = ['Overall', '6h\\n(n=1)', '12h\\n(n=2)', '18h\\n(n=3)', '24h\\n(n=4)',\n",
    "                   '30h\\n(n=5)', '36h\\n(n=6)', '42h\\n(n=7)', '48h\\n(n=8)']\n",
    "\n",
    "        if scaled == True:\n",
    "            print('Error Units: scaled between 0 to 1')\n",
    "\n",
    "        print(tabulate(bt_rep, headers=headers, tablefmt='psql'))\n",
    "        print('\\n')\n",
    "        print('#' * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b21c7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_rep(repname='sumrep.csv'):\n",
    "    \"\"\"\n",
    "    SUMMARY REPORT FOR MODEL EVALUATION\n",
    "    Output csv file mean errors of all validating cyclones\n",
    "    \"\"\"\n",
    "    tscols = target_col\n",
    "    traincycount = traincycount_fit\n",
    "    valcycount = valcycount_fit\n",
    "    bt_error = bt_error_fit\n",
    "    covariates = covariates_fit\n",
    "    bt_act_df = sc_bt_act\n",
    "    bt_fc_df = sc_bt_fc\n",
    "    \n",
    "    print('#'*50)\n",
    "    print('SUMMARY REPORT FOR MODEL EVALUATION')\n",
    "    print('#'*50)\n",
    "    print('\\nNumber of fitted target features: '+ str(len(tscols)))\n",
    "    print('List of fitted target features: ')\n",
    "    print([i for i in tscols])\n",
    "    print('List of fitted covariate features:')\n",
    "    if covariates is not None:\n",
    "        print([i for i in covariates])\n",
    "    else:\n",
    "        print('No covariate features.')\n",
    "    print('\\nValidating Cyclone ID(s): ')\n",
    "    print([cycid for cycid in bt_fc_df['cyc_id'].unique()])\n",
    "    print('Validating Cyclone Name(s):')\n",
    "    print([compare_dict[cycid] for cycid in bt_fc_df['cyc_id'].unique()])\n",
    "    print('\\nNumber of training cyclones used for each run: ' + str(traincycount))\n",
    "    print('Validating method: leave one out')\n",
    "    print('Number of validating runs: ' + str(valcycount))\n",
    "    print('Prediction at step n ahead: n= ', [i for i in range(1,9)])\n",
    "    print('\\n'+'#'*50)\n",
    "    print(f'Summarry Table of Scaled Error after {valcycount} Validating Runs.')\n",
    "    print('#'*50)    \n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "    bt_rmse = []\n",
    "    bt_mse = []\n",
    "    bt_mae = []\n",
    "    bt_obs = []\n",
    "    bt_len = []\n",
    "    \n",
    "    # Calculate error scores\n",
    "    for g in bt_fc_df['Obs'].unique():\n",
    "\n",
    "        mask_act = bt_act_df.loc[(bt_act_df['Obs']==g),tscols]\n",
    "        mask_fc = bt_fc_df.loc[(bt_fc_df['Obs']==g),tscols]\n",
    "\n",
    "        rmse = mean_squared_error(mask_act, mask_fc, squared=False)\n",
    "        mse = mean_squared_error(mask_act, mask_fc, squared=True)\n",
    "        mae = mean_absolute_error(mask_act, mask_fc)\n",
    "\n",
    "        bt_rmse.append(rmse)\n",
    "        bt_mse.append(mse)\n",
    "        bt_mae.append(mae)\n",
    "        bt_obs.append(g)\n",
    "        bt_len.append(len(mask_fc))\n",
    "\n",
    "\n",
    "    bt_rep = pd.DataFrame(list(zip(bt_len,bt_mae,bt_rmse,bt_mse)),columns=['BT_sets','MAE', 'RMSE','MSE'])\n",
    "    \n",
    "#     # Save to csv before transpose\n",
    "#     bt_rep.to_csv(f'{repname}.csv',index=False)\n",
    "    \n",
    "    bt_rep=bt_rep.round(2).transpose()\n",
    "\n",
    "    bt_rep.columns = ['6h','12h','18h','24h','30h','36h','42h','48h']\n",
    "\n",
    "    headers = ['6h\\n(n=1)', '12h\\n(n=2)', '18h\\n(n=3)', '24h\\n(n=4)', \n",
    "             '30h\\n(n=5)', '36h\\n(n=6)', '42h\\n(n=7)', '48h\\n(n=8)']\n",
    "\n",
    "    print('Error Units: scaled between 0 to 1')   \n",
    "    print(tabulate(bt_rep, headers = headers, tablefmt = 'psql'))\n",
    "    print('\\n')\n",
    "    print('#'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "638932f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_set_errors (modelname='fit'):\n",
    "    bt_rmse = []\n",
    "    bt_mse = []\n",
    "    bt_mae = []\n",
    "    bt_set_no = []\n",
    "\n",
    "\n",
    "    tscols = target_col\n",
    "    bt_act_df = sc_bt_act\n",
    "    bt_fc_df = sc_bt_fc\n",
    "\n",
    "    \n",
    "    bt_count = int(len(bt_fc_df)/(bt_fc_df['Obs'].nunique()))\n",
    "    \n",
    "    bt_id = []\n",
    "    for i in range(1,bt_count+1):\n",
    "        a = [i]*bt_fc_df['Obs'].nunique()\n",
    "        bt_id = bt_id + a\n",
    "\n",
    "    bt_fc_df['bt_id'] = bt_id\n",
    "    bt_act_df['bt_id'] = bt_id\n",
    "\n",
    "    # Calculate error scores for each back test set\n",
    "    for g in bt_fc_df['bt_id'].unique():\n",
    "        mask_act = bt_act_df.loc[(bt_act_df['bt_id']==g),tscols]\n",
    "        mask_fc = bt_fc_df.loc[(bt_fc_df['bt_id']==g),tscols]\n",
    "        rmse = mean_squared_error(mask_act, mask_fc, squared=False)\n",
    "        mse = mean_squared_error(mask_act, mask_fc, squared=True)\n",
    "        mae = mean_absolute_error(mask_act, mask_fc)\n",
    "        bt_rmse.append(rmse)\n",
    "        bt_mse.append(mse)\n",
    "        bt_mae.append(mae)\n",
    "        bt_set_no.append(g)\n",
    "\n",
    "    bt_set_error = pd.DataFrame(list(zip(bt_set_no,bt_mae,bt_rmse,bt_mse)),columns=['BT_ID','MAE', 'RMSE','MSE'])\n",
    "    bt_set_error.to_csv(f'bt_set_error_{modelname}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea52867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_csv (modelname='fit'):\n",
    "    \"\"\"\n",
    "    save backtest outputs to 5 x csv files.\n",
    "    \"\"\"\n",
    "    bt_act.to_csv(f'bt_act_{modelname}.csv',index=False)\n",
    "    bt_fc.to_csv(f'bt_fc_{modelname}.csv',index=False)\n",
    "    sc_bt_act.to_csv(f'sc_bt_act_{modelname}.csv',index=False)\n",
    "    sc_bt_fc.to_csv(f'sc_bt_fc_{modelname}.csv',index=False)\n",
    "    bt_error_fit.to_csv(f'bt_error_fit_{modelname}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac5c8ea",
   "metadata": {},
   "source": [
    "### Model Fitting and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1a44d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name list of target features\n",
    "cycpath = ['LAT',\n",
    "          'LON']\n",
    "\n",
    "# name list of covariate features\n",
    "pres = ['CENTRAL_PRES',\n",
    "          'ENV_PRES']\n",
    "\n",
    "r34s = ['MN_RADIUS_GF_SECNE',\n",
    "        'MN_RADIUS_GF_SECSE',\n",
    "        'MN_RADIUS_GF_SECSW',\n",
    "        'MN_RADIUS_GF_SECNW']\n",
    "\n",
    "# list of comparison cyclones from BOM for validating\n",
    "# compare = [AU202021_11U, AU202021_15U, AU202021_17U, AU202021_22U]\n",
    "\n",
    "compare = df_all\n",
    "compare_id = []\n",
    "compare_name = []\n",
    "for cyc in compare:\n",
    "    cycid = cyc['DISTURBANCE_ID'].unique().tolist()\n",
    "    compare_id += cycid\n",
    "    \n",
    "for cyc in compare:\n",
    "    name = cyc['NAME'].unique().tolist()\n",
    "    compare_name += name\n",
    "\n",
    "compare_dict = dict(zip(compare_id,compare_name))\n",
    "# full excel files compare = [AU202021_02U, AU202021_07U, AU202021_09U, AU202021_11U, AU202021_15U, AU202021_17U, AU202021_22U]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c447ea5",
   "metadata": {},
   "source": [
    "#### 1. Modeling Cyclone Path for West Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729e5e14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length = 12\n",
      "Cyclone number = 1\n",
      "Cyclone ID: AU201920_05U\n",
      "Length = 5\n",
      "Cyclone number = 1\n",
      "Cyclone ID: AU202122_05U\n",
      "Length = 9\n",
      "Cyclone number = 1\n",
      "Cyclone ID: AU201617_29U\n",
      "Length = 9\n",
      "Cyclone number = 1\n",
      "Cyclone ID: AU200910_06U\n",
      "Length = 8\n",
      "Cyclone number = 1\n",
      "Cyclone ID: AU200708_22U\n",
      "Length = 5\n",
      "Cyclone number = 1\n",
      "Cyclone ID: AU202021_07U\n",
      "Length = 24\n",
      "Cyclone number = 1\n",
      "Cyclone ID: AU201819_12U\n",
      "Epoch 29: 100%|| 131/131 [00:16<00:00,  7.94it/s, train_loss=0.00113]\n",
      "Length = 8\n",
      "Cyclone number = 2\n",
      "Cyclone ID: AU201314_03U\n",
      "Length = 22\n",
      "Cyclone number = 2\n",
      "Cyclone ID: AU200203_04U\n",
      "Epoch 29: 100%|| 131/131 [00:16<00:00,  7.80it/s, train_loss=0.00131]\n",
      "Length = 22\n",
      "Cyclone number = 3\n",
      "Cyclone ID: AU200809_18U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.54it/s, train_loss=0.00125]\n",
      "Length = 5\n",
      "Cyclone number = 4\n",
      "Cyclone ID: AU201314_09U\n",
      "Length = 9\n",
      "Cyclone number = 4\n",
      "Cyclone ID: AU200405_03U\n",
      "Length = 7\n",
      "Cyclone number = 4\n",
      "Cyclone ID: AU201617_20U\n",
      "Length = 4\n",
      "Cyclone number = 4\n",
      "Cyclone ID: AU201617_30U\n",
      "Length = 12\n",
      "Cyclone number = 4\n",
      "Cyclone ID: AU202122_28U\n",
      "Length = 16\n",
      "Cyclone number = 4\n",
      "Cyclone ID: AU200708_20U\n",
      "Epoch 29: 100%|| 131/131 [00:16<00:00,  7.87it/s, train_loss=0.000985]\n",
      "Length = 6\n",
      "Cyclone number = 5\n",
      "Cyclone ID: AU201213_04U\n",
      "Length = 14\n",
      "Cyclone number = 5\n",
      "Cyclone ID: AU200506_06U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.62it/s, train_loss=0.00144]\n",
      "Length = 14\n",
      "Cyclone number = 6\n",
      "Cyclone ID: AU201617_23U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.48it/s, train_loss=0.000912]\n",
      "Length = 33\n",
      "Cyclone number = 7\n",
      "Cyclone ID: AU202223_13U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.60it/s, train_loss=0.00298]\n",
      "Length = 4\n",
      "Cyclone number = 8\n",
      "Cyclone ID: AU201718_06U\n",
      "Length = 6\n",
      "Cyclone number = 8\n",
      "Cyclone ID: AU199697_14U\n",
      "Length = 17\n",
      "Cyclone number = 8\n",
      "Cyclone ID: AU201011_12U\n",
      "Epoch 29: 100%|| 131/131 [00:16<00:00,  7.82it/s, train_loss=0.00108]\n",
      "Length = 8\n",
      "Cyclone number = 9\n",
      "Cyclone ID: AU200809_02U\n",
      "Length = 7\n",
      "Cyclone number = 9\n",
      "Cyclone ID: AU201516_08U\n",
      "Length = 15\n",
      "Cyclone number = 9\n",
      "Cyclone ID: AU202122_36U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.65it/s, train_loss=0.00103]\n",
      "Length = 6\n",
      "Cyclone number = 10\n",
      "Cyclone ID: AU201112_15U\n",
      "Length = 11\n",
      "Cyclone number = 10\n",
      "Cyclone ID: AU200607_13U\n",
      "Length = 8\n",
      "Cyclone number = 10\n",
      "Cyclone ID: AU201415_19U\n",
      "Length = 17\n",
      "Cyclone number = 10\n",
      "Cyclone ID: AU200405_08U\n",
      "Epoch 29: 100%|| 131/131 [00:18<00:00,  7.07it/s, train_loss=0.00223]\n",
      "Length = 20\n",
      "Cyclone number = 11\n",
      "Cyclone ID: AU201920_08U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.56it/s, train_loss=0.000885]\n",
      "Length = 25\n",
      "Cyclone number = 12\n",
      "Cyclone ID: AU200607_11U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.65it/s, train_loss=0.000618]\n",
      "Length = 20\n",
      "Cyclone number = 13\n",
      "Cyclone ID: AU200708_15U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.56it/s, train_loss=0.00119]\n",
      "Length = 33\n",
      "Cyclone number = 14\n",
      "Cyclone ID: AU202021_15U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.61it/s, train_loss=0.00195]\n",
      "Length = 22\n",
      "Cyclone number = 15\n",
      "Cyclone ID: AU200607_12U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.58it/s, train_loss=0.00159]\n",
      "Length = 19\n",
      "Cyclone number = 16\n",
      "Cyclone ID: AU200708_06U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.69it/s, train_loss=0.00261]\n",
      "Length = 14\n",
      "Cyclone number = 17\n",
      "Cyclone ID: AU202223_05U\n",
      "Epoch 29: 100%|| 131/131 [00:18<00:00,  7.24it/s, train_loss=0.00111]\n",
      "Length = 6\n",
      "Cyclone number = 18\n",
      "Cyclone ID: AU200203_03U\n",
      "Length = 8\n",
      "Cyclone number = 18\n",
      "Cyclone ID: AU200304_03U\n",
      "Length = 13\n",
      "Cyclone number = 18\n",
      "Cyclone ID: AU200910_12U\n",
      "Length = 19\n",
      "Cyclone number = 18\n",
      "Cyclone ID: AU200304_05U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.66it/s, train_loss=0.00369]\n",
      "Length = 4\n",
      "Cyclone number = 19\n",
      "Cyclone ID: AU200405_02U\n",
      "Length = 17\n",
      "Cyclone number = 19\n",
      "Cyclone ID: AU202324_08U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.65it/s, train_loss=0.00182]\n",
      "Length = 30\n",
      "Cyclone number = 20\n",
      "Cyclone ID: AU200203_02U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.69it/s, train_loss=0.00202]\n",
      "Length = 9\n",
      "Cyclone number = 21\n",
      "Cyclone ID: AU200405_09U\n",
      "Length = 14\n",
      "Cyclone number = 21\n",
      "Cyclone ID: AU201314_16U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.63it/s, train_loss=0.00204]\n",
      "Length = 31\n",
      "Cyclone number = 22\n",
      "Cyclone ID: AU200708_11U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.37it/s, train_loss=0.00134]\n",
      "Length = 49\n",
      "Cyclone number = 23\n",
      "Cyclone ID: AU200304_08U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.63it/s, train_loss=0.000905]\n",
      "Length = 6\n",
      "Cyclone number = 24\n",
      "Cyclone ID: AU201718_02U\n",
      "Length = 16\n",
      "Cyclone number = 24\n",
      "Cyclone ID: AU201920_03U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.43it/s, train_loss=0.00159]\n",
      "Length = 7\n",
      "Cyclone number = 25\n",
      "Cyclone ID: AU200809_10U\n",
      "Length = 12\n",
      "Cyclone number = 25\n",
      "Cyclone ID: AU200708_23U\n",
      "Length = 20\n",
      "Cyclone number = 25\n",
      "Cyclone ID: AU200506_01U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.38it/s, train_loss=0.00149]\n",
      "Length = 9\n",
      "Cyclone number = 26\n"
     ]
    }
   ],
   "source": [
    "# Fitting cycpath and validate model using cyclone from west region\n",
    "path_west = modelfitting(tscols=cycpath,df_list_of_cyclones=df_west)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc133bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: only need to change model name in [] in for loop.\n",
    "# Don't change names of variables on the left hand side.\n",
    "\n",
    "for fittedmodel in [path_west]:\n",
    "    ### DON'T CHANGE BELOW CODE\n",
    "    \n",
    "    # Assign backtest output of fitted model for reports\n",
    "    bt_act = fittedmodel[0][0]\n",
    "    bt_fc = fittedmodel[0][1]\n",
    "    sc_bt_act = fittedmodel[0][2]\n",
    "    sc_bt_fc = fittedmodel[0][3]\n",
    "    bt_error_fit = fittedmodel[0][4]\n",
    "\n",
    "    # Assign other outputs from fitted model for reports\n",
    "    target_col = fittedmodel[1]\n",
    "    traincycount_fit = fittedmodel[2]\n",
    "    valcycount_fit = fittedmodel[3]\n",
    "    covariates_fit = fittedmodel[4]\n",
    "    train_cycs = fittedmodel[5]\n",
    "    \n",
    "    ### DON'T CHANGE ABOVE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f93ed7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "SUMMARY REPORT FOR MODEL EVALUATION\n",
      "##################################################\n",
      "\n",
      "Number of fitted target features: 2\n",
      "List of fitted target features: \n",
      "['LAT', 'LON']\n",
      "List of fitted covariate features:\n",
      "No covariate features.\n",
      "\n",
      "Validating Cyclone ID(s): \n",
      "['AU201819_12U', 'AU200203_04U', 'AU200809_18U', 'AU200708_20U', 'AU200506_06U', 'AU201617_23U', 'AU202223_13U', 'AU201011_12U', 'AU202122_36U', 'AU200405_08U', 'AU201920_08U', 'AU200607_11U', 'AU200708_15U', 'AU202021_15U', 'AU200607_12U', 'AU200708_06U', 'AU202223_05U', 'AU200304_05U', 'AU202324_08U', 'AU200203_02U', 'AU201314_16U', 'AU200708_11U', 'AU200304_08U', 'AU201920_03U', 'AU200506_01U']\n",
      "Validating Cyclone Name(s):\n",
      "['Riley', 'HARRIET', 'Ilsa', 'Pancho', 'DARYL', 'Caleb', 'Freddy', 'Bianca', 'Karim', 'WILLY', 'Ferdinand', 'GEORGE', 'Ophelia', 'Marian', 'JACOB', 'Melanie', 'Darian', 'MONTY', 'Neville', 'FIONA', 'Jack', 'Nicholas', 'FAY', 'Claudia', 'BERTIE']\n",
      "\n",
      "Number of training cyclones used for each run: 93\n",
      "Validating method: leave one out\n",
      "Number of validating runs: 25\n",
      "Prediction at step n ahead: n=  [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "\n",
      "##################################################\n",
      "Summarry Table of Scaled Error after 25 Validating Runs.\n",
      "##################################################\n",
      "\n",
      "\n",
      "Error Units: scaled between 0 to 1\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|         |      6h |     12h |     18h |     24h |     30h |     36h |     42h |     48h |\n",
      "|         |   (n=1) |   (n=2) |   (n=3) |   (n=4) |   (n=5) |   (n=6) |   (n=7) |   (n=8) |\n",
      "|---------+---------+---------+---------+---------+---------+---------+---------+---------|\n",
      "| BT_sets |  218    |  218    |  218    |  218    |  218    |  218    |  218    |  218    |\n",
      "| MAE     |    0.03 |    0.05 |    0.08 |    0.1  |    0.13 |    0.14 |    0.16 |    0.19 |\n",
      "| RMSE    |    0.04 |    0.07 |    0.11 |    0.14 |    0.17 |    0.19 |    0.22 |    0.25 |\n",
      "| MSE     |    0    |    0.01 |    0.01 |    0.02 |    0.03 |    0.04 |    0.05 |    0.06 |\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "\n",
      "\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "# save back test outputs to csv files\n",
    "bt_csv(modelname='path_west')\n",
    "\n",
    "# calculate back test set error and save to csv files\n",
    "bt_set_errors(modelname='path_west')\n",
    "\n",
    "# summary report\n",
    "summary_rep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81b21ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A geoplot named 'path_west.html' was created.\n"
     ]
    }
   ],
   "source": [
    "# Geoplot\n",
    "if len(path_west)==7:\n",
    "    geoplot(geo_plot_data=path_west[-1],plotname='path_west.html')\n",
    "else:\n",
    "    print('No data for geoplot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172964ff",
   "metadata": {},
   "source": [
    "#### 2. Modeling Cyclone Path for East Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b1f1e77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length = 4\n",
      "Cyclone number = 1\n",
      "Cyclone ID: AU201213_03U\n",
      "Length = 34\n",
      "Cyclone number = 1\n",
      "Cyclone ID: AU202324_02U\n",
      "Epoch 29: 100%|| 131/131 [00:16<00:00,  7.95it/s, train_loss=0.00133]\n",
      "Length = 11\n",
      "Cyclone number = 2\n",
      "Cyclone ID: AU202122_08U\n",
      "Length = 4\n",
      "Cyclone number = 2\n",
      "Cyclone ID: AU201415_24U\n",
      "Length = 5\n",
      "Cyclone number = 2\n",
      "Cyclone ID: AU200607_05U\n",
      "Length = 4\n",
      "Cyclone number = 2\n",
      "Cyclone ID: AU201920_07U\n",
      "Length = 7\n",
      "Cyclone number = 2\n",
      "Cyclone ID: AU200607_16U\n",
      "Length = 7\n",
      "Cyclone number = 2\n",
      "Cyclone ID: AU200809_23U\n",
      "Length = 30\n",
      "Cyclone number = 2\n",
      "Cyclone ID: AU201011_11U\n",
      "Epoch 29: 100%|| 131/131 [00:16<00:00,  8.00it/s, train_loss=0.000915]\n",
      "Length = 9\n",
      "Cyclone number = 3\n",
      "Cyclone ID: AU201516_10U\n",
      "Length = 12\n",
      "Cyclone number = 3\n",
      "Cyclone ID: AU201011_14U\n",
      "Length = 9\n",
      "Cyclone number = 3\n",
      "Cyclone ID: AU201213_13U\n",
      "Length = 29\n",
      "Cyclone number = 3\n",
      "Cyclone ID: AU200910_07U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.52it/s, train_loss=0.00116]\n",
      "Length = 11\n",
      "Cyclone number = 4\n",
      "Cyclone ID: AU201718_24U\n",
      "Length = 21\n",
      "Cyclone number = 4\n",
      "Cyclone ID: AU201314_10U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.50it/s, train_loss=0.00124]\n",
      "Length = 9\n",
      "Cyclone number = 5\n",
      "Cyclone ID: AU200506_08U\n",
      "Length = 14\n",
      "Cyclone number = 5\n",
      "Cyclone ID: AU201213_14U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.51it/s, train_loss=0.00161]\n",
      "Length = 25\n",
      "Cyclone number = 6\n",
      "Cyclone ID: AU200910_09U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.37it/s, train_loss=0.00135]\n",
      "Length = 23\n",
      "Cyclone number = 7\n",
      "Cyclone ID: AU201617_24U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.59it/s, train_loss=0.00151]\n",
      "Length = 12\n",
      "Cyclone number = 8\n",
      "Cyclone ID: AU202122_10U\n",
      "Length = 9\n",
      "Cyclone number = 8\n",
      "Cyclone ID: AU200304_04U\n",
      "Length = 4\n",
      "Cyclone number = 8\n",
      "Cyclone ID: AU200203_01U\n",
      "Length = 10\n",
      "Cyclone number = 8\n",
      "Cyclone ID: AU201011_10U\n",
      "Length = 4\n",
      "Cyclone number = 8\n",
      "Cyclone ID: AU201314_07U\n",
      "Length = 8\n",
      "Cyclone number = 8\n",
      "Cyclone ID: AU201415_14U\n",
      "Length = 10\n",
      "Cyclone number = 8\n",
      "Cyclone ID: AU200304_02U\n",
      "Length = 19\n",
      "Cyclone number = 8\n",
      "Cyclone ID: AU201819_14U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.51it/s, train_loss=0.00108]\n",
      "Length = 4\n",
      "Cyclone number = 9\n",
      "Cyclone ID: AU200809_06U\n",
      "Length = 52\n",
      "Cyclone number = 9\n",
      "Cyclone ID: AU201819_04U\n",
      "Epoch 29: 100%|| 131/131 [00:18<00:00,  7.27it/s, train_loss=0.00168]\n",
      "Length = 5\n",
      "Cyclone number = 10\n",
      "Cyclone ID: AU202324_13U\n",
      "Length = 32\n",
      "Cyclone number = 10\n",
      "Cyclone ID: AU200506_22U\n",
      "Epoch 29: 100%|| 131/131 [00:18<00:00,  7.13it/s, train_loss=0.00154]\n",
      "Length = 10\n",
      "Cyclone number = 11\n",
      "Cyclone ID: AU201718_22U\n",
      "Length = 54\n",
      "Cyclone number = 11\n",
      "Cyclone ID: AU201415_17U\n",
      "Epoch 29: 100%|| 125/125 [00:17<00:00,  7.27it/s, train_loss=0.00141]\n",
      "Length = 32\n",
      "Cyclone number = 12\n",
      "Cyclone ID: AU201920_12U\n",
      "Epoch 29: 100%|| 131/131 [00:18<00:00,  7.25it/s, train_loss=0.000963]\n",
      "Length = 5\n",
      "Cyclone number = 13\n",
      "Cyclone ID: AU201112_04U\n",
      "Length = 11\n",
      "Cyclone number = 13\n",
      "Cyclone ID: AU202324_09U\n",
      "Length = 12\n",
      "Cyclone number = 13\n",
      "Cyclone ID: AU200708_03U\n",
      "Length = 6\n",
      "Cyclone number = 13\n",
      "Cyclone ID: AU202324_05U\n",
      "Length = 18\n",
      "Cyclone number = 13\n",
      "Cyclone ID: AU202021_17U\n",
      "Epoch 29: 100%|| 131/131 [00:18<00:00,  7.13it/s, train_loss=0.00107]\n",
      "Length = 15\n",
      "Cyclone number = 14\n",
      "Cyclone ID: AU202122_18U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.40it/s, train_loss=0.00111]\n",
      "Length = 12\n",
      "Cyclone number = 15\n",
      "Cyclone ID: AU200001_06U\n",
      "Length = 9\n",
      "Cyclone number = 15\n",
      "Cyclone ID: AU200304_09U\n",
      "Length = 10\n",
      "Cyclone number = 15\n",
      "Cyclone ID: AU200506_15U\n",
      "Length = 26\n",
      "Cyclone number = 15\n",
      "Cyclone ID: AU200809_17U\n",
      "Epoch 29: 100%|| 131/131 [00:18<00:00,  7.26it/s, train_loss=0.00158]\n",
      "Length = 5\n",
      "Cyclone number = 16\n",
      "Cyclone ID: AU201920_10U\n",
      "Length = 40\n",
      "Cyclone number = 16\n",
      "Cyclone ID: AU201314_15U\n",
      "Epoch 29: 100%|| 131/131 [00:19<00:00,  6.80it/s, train_loss=0.000724]\n",
      "Length = 27\n",
      "Cyclone number = 17\n",
      "Cyclone ID: AU201819_20U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.38it/s, train_loss=0.00162]\n",
      "Length = 9\n",
      "Cyclone number = 18\n",
      "Cyclone ID: AU200910_11U\n",
      "Length = 19\n",
      "Cyclone number = 18\n",
      "Cyclone ID: AU200405_10U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.40it/s, train_loss=0.00225]\n",
      "Length = 15\n",
      "Cyclone number = 19\n",
      "Cyclone ID: AU201415_13U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.36it/s, train_loss=0.00157]\n",
      "Length = 7\n",
      "Cyclone number = 20\n",
      "Cyclone ID: AU200405_05U\n",
      "Length = 14\n",
      "Cyclone number = 20\n",
      "Cyclone ID: AU202021_11U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.34it/s, train_loss=0.00102]\n",
      "Length = 19\n",
      "Cyclone number = 21\n",
      "Cyclone ID: AU201819_07U\n",
      "Epoch 29: 100%|| 131/131 [00:18<00:00,  7.27it/s, train_loss=0.00106]\n",
      "Length = 14\n",
      "Cyclone number = 22\n",
      "Cyclone ID: AU202223_14U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.35it/s, train_loss=0.00116]\n",
      "Length = 20\n",
      "Cyclone number = 23\n",
      "Cyclone ID: AU200506_17U\n",
      "Epoch 29: 100%|| 131/131 [00:18<00:00,  7.26it/s, train_loss=0.00252]\n",
      "Length = 4\n",
      "Cyclone number = 24\n",
      "Cyclone ID: AU201617_19U\n",
      "Length = 13\n",
      "Cyclone number = 24\n",
      "Cyclone ID: AU202122_07U\n",
      "Length = 22\n",
      "Cyclone number = 24\n",
      "Cyclone ID: AU201920_06U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.37it/s, train_loss=0.000952]\n",
      "Length = 6\n",
      "Cyclone number = 25\n",
      "Cyclone ID: AU201213_18U\n",
      "Length = 6\n",
      "Cyclone number = 25\n",
      "Cyclone ID: AU202021_09U\n",
      "Length = 5\n",
      "Cyclone number = 25\n",
      "Cyclone ID: AU201112_12U\n",
      "Length = 10\n",
      "Cyclone number = 25\n",
      "Cyclone ID: AU201819_26U\n",
      "Length = 4\n",
      "Cyclone number = 25\n",
      "Cyclone ID: AU200506_10U\n"
     ]
    }
   ],
   "source": [
    "# Fitting cycpath and validate model using cyclone from east region\n",
    "path_east = modelfitting(tscols=cycpath, df_list_of_cyclones=df_east)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79bc5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: only need to change model name in [] in 'for loop' for reports\n",
    "# Don't change names of variables on the left hand side.\n",
    "\n",
    "for fittedmodel in [path_east]:\n",
    "    ### DON'T CHANGE BELOW CODE\n",
    "    \n",
    "    # Assign backtest output of fitted model for reports\n",
    "    bt_act = fittedmodel[0][0]\n",
    "    bt_fc = fittedmodel[0][1]\n",
    "    sc_bt_act = fittedmodel[0][2]\n",
    "    sc_bt_fc = fittedmodel[0][3]\n",
    "    bt_error_fit = fittedmodel[0][4]\n",
    "\n",
    "    # Assign other outputs from fitted model for reports\n",
    "    target_col = fittedmodel[1]\n",
    "    traincycount_fit = fittedmodel[2]\n",
    "    valcycount_fit = fittedmodel[3]\n",
    "    covariates_fit = fittedmodel[4]\n",
    "    \n",
    "    ### DON'T CHANGE ABOVE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e85f623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "SUMMARY REPORT FOR MODEL EVALUATION\n",
      "##################################################\n",
      "\n",
      "Number of fitted target features: 2\n",
      "List of fitted target features: \n",
      "['LAT', 'LON']\n",
      "List of fitted covariate features:\n",
      "No covariate features.\n",
      "\n",
      "Validating Cyclone ID(s): \n",
      "['AU202324_02U', 'AU201011_11U', 'AU200910_07U', 'AU201314_10U', 'AU201213_14U', 'AU200910_09U', 'AU201617_24U', 'AU201819_14U', 'AU201819_04U', 'AU200506_22U', 'AU201415_17U', 'AU201920_12U', 'AU202021_17U', 'AU202122_18U', 'AU200809_17U', 'AU201314_15U', 'AU201819_20U', 'AU200405_10U', 'AU201415_13U', 'AU202021_11U', 'AU201819_07U', 'AU202223_14U', 'AU200506_17U', 'AU201920_06U']\n",
      "Validating Cyclone Name(s):\n",
      "['Jasper', 'Anthony', 'Olga', 'Edna', 'Tim', 'Ului', 'Debbie', 'Oma', 'Owen', 'MONICA', 'Nathan', 'Harold', 'Niran', 'Dovi', 'Hamish', 'Ita', 'Trevor', 'KERRY', 'Lam', 'Lucas', 'Penny', 'Gabrielle', 'WATI', 'Uesi']\n",
      "\n",
      "Number of training cyclones used for each run: 93\n",
      "Validating method: leave one out\n",
      "Number of validating runs: 24\n",
      "Prediction at step n ahead: n=  [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "\n",
      "##################################################\n",
      "Summarry Table of Scaled Error after 24 Validating Runs.\n",
      "##################################################\n",
      "\n",
      "\n",
      "Error Units: scaled between 0 to 1\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|         |      6h |     12h |     18h |     24h |     30h |     36h |     42h |     48h |\n",
      "|         |   (n=1) |   (n=2) |   (n=3) |   (n=4) |   (n=5) |   (n=6) |   (n=7) |   (n=8) |\n",
      "|---------+---------+---------+---------+---------+---------+---------+---------+---------|\n",
      "| BT_sets |  302    |  302    |  302    |  302    |  302    |  302    |  302    |  302    |\n",
      "| MAE     |    0.04 |    0.07 |    0.1  |    0.14 |    0.17 |    0.19 |    0.22 |    0.24 |\n",
      "| RMSE    |    0.05 |    0.09 |    0.14 |    0.18 |    0.22 |    0.25 |    0.28 |    0.32 |\n",
      "| MSE     |    0    |    0.01 |    0.02 |    0.03 |    0.05 |    0.06 |    0.08 |    0.1  |\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "\n",
      "\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "# save back test outputs to csv files\n",
    "bt_csv(modelname='path_east')\n",
    "\n",
    "# calculate back test set error and save to csv files\n",
    "bt_set_errors(modelname='path_east')\n",
    "\n",
    "# summary report\n",
    "summary_rep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dbb7c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A geoplot named 'path_east.html' was created.\n"
     ]
    }
   ],
   "source": [
    "# Geoplot\n",
    "if len(path_east)==7:\n",
    "    geoplot(geo_plot_data=path_east[-1],plotname='path_east.html')\n",
    "else:\n",
    "    print('No data for geoplot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c88e43",
   "metadata": {},
   "source": [
    "#### 3. Modeling Cyclone Path for Shared Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84cfd0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length = 42\n",
      "Cyclone number = 1\n",
      "Cyclone ID: AU201011_17U\n",
      "Epoch 29: 100%|| 131/131 [00:16<00:00,  7.71it/s, train_loss=0.000958]\n",
      "Length = 43\n",
      "Cyclone number = 2\n",
      "Cyclone ID: AU200405_07U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.52it/s, train_loss=0.00113]\n",
      "Length = 15\n",
      "Cyclone number = 3\n",
      "Cyclone ID: AU200203_06U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.60it/s, train_loss=0.00112]\n",
      "Length = 23\n",
      "Cyclone number = 4\n",
      "Cyclone ID: AU201314_01U\n",
      "Epoch 29: 100%|| 131/131 [00:18<00:00,  7.25it/s, train_loss=0.000938]\n",
      "Length = 4\n",
      "Cyclone number = 5\n",
      "Cyclone ID: AU200708_08U\n",
      "Length = 34\n",
      "Cyclone number = 5\n",
      "Cyclone ID: AU201718_20U\n",
      "Epoch 29: 100%|| 131/131 [00:17<00:00,  7.40it/s, train_loss=0.000931]\n"
     ]
    }
   ],
   "source": [
    "# Fitting cycpath and validate model using cyclone from east region\n",
    "path_shared = modelfitting(tscols=cycpath, df_list_of_cyclones=df_shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e480517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: only need to change model name in [] in 'for loop' for reports\n",
    "# Don't change names of variables on the left hand side.\n",
    "\n",
    "for fittedmodel in [path_shared]:\n",
    "    ### DON'T CHANGE BELOW CODE\n",
    "    \n",
    "    # Assign backtest output of fitted model for reports\n",
    "    bt_act = fittedmodel[0][0]\n",
    "    bt_fc = fittedmodel[0][1]\n",
    "    sc_bt_act = fittedmodel[0][2]\n",
    "    sc_bt_fc = fittedmodel[0][3]\n",
    "    bt_error_fit = fittedmodel[0][4]\n",
    "\n",
    "    # Assign other outputs from fitted model for reports\n",
    "    target_col = fittedmodel[1]\n",
    "    traincycount_fit = fittedmodel[2]\n",
    "    valcycount_fit = fittedmodel[3]\n",
    "    covariates_fit = fittedmodel[4]\n",
    "    \n",
    "    ### DON'T CHANGE ABOVE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16c3be8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "SUMMARY REPORT FOR MODEL EVALUATION\n",
      "##################################################\n",
      "\n",
      "Number of fitted target features: 2\n",
      "List of fitted target features: \n",
      "['LAT', 'LON']\n",
      "List of fitted covariate features:\n",
      "No covariate features.\n",
      "\n",
      "Validating Cyclone ID(s): \n",
      "['AU201011_17U', 'AU200405_07U', 'AU200203_06U', 'AU201314_01U', 'AU201718_20U']\n",
      "Validating Cyclone Name(s):\n",
      "['Carlos', 'INGRID', 'CRAIG', 'Alessia', 'Marcus']\n",
      "\n",
      "Number of training cyclones used for each run: 93\n",
      "Validating method: leave one out\n",
      "Number of validating runs: 5\n",
      "Prediction at step n ahead: n=  [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "\n",
      "##################################################\n",
      "Summarry Table of Scaled Error after 5 Validating Runs.\n",
      "##################################################\n",
      "\n",
      "\n",
      "Error Units: scaled between 0 to 1\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|         |      6h |     12h |     18h |     24h |     30h |     36h |     42h |     48h |\n",
      "|         |   (n=1) |   (n=2) |   (n=3) |   (n=4) |   (n=5) |   (n=6) |   (n=7) |   (n=8) |\n",
      "|---------+---------+---------+---------+---------+---------+---------+---------+---------|\n",
      "| BT_sets |   92    |   92    |   92    |   92    |   92    |   92    |   92    |   92    |\n",
      "| MAE     |    0.03 |    0.04 |    0.06 |    0.09 |    0.11 |    0.13 |    0.15 |    0.18 |\n",
      "| RMSE    |    0.04 |    0.05 |    0.08 |    0.12 |    0.15 |    0.19 |    0.21 |    0.24 |\n",
      "| MSE     |    0    |    0    |    0.01 |    0.02 |    0.03 |    0.04 |    0.05 |    0.06 |\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "\n",
      "\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "# save back test outputs to csv files\n",
    "bt_csv(modelname='path_shared')\n",
    "\n",
    "# calculate back test set error and save to csv files\n",
    "bt_set_errors(modelname='path_shared')\n",
    "\n",
    "# summary report\n",
    "summary_rep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4c8ea92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A geoplot named 'path_shared.html' was created.\n"
     ]
    }
   ],
   "source": [
    "# Geoplot\n",
    "if len(path_shared)==7:\n",
    "    geoplot(geo_plot_data=path_shared[-1],plotname='path_shared.html')\n",
    "else:\n",
    "    print('No data for geoplot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7d45d",
   "metadata": {},
   "source": [
    "### Compare Model Performance On East and West Region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc11dc4",
   "metadata": {},
   "source": [
    "#### Based on Overall Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3bfe3f",
   "metadata": {},
   "source": [
    "1. Each validating run evaluates the performance of 1 validating cyclone.\n",
    "2. A validating run can have multiple back tests through out the length of series.\n",
    "3. Each back test produces a set of error scores.\n",
    "4. Take the mean of all back tests within a run to obtain the overall performance score against the validating cyclone.\n",
    "5. Unit of sample size is the number of cyclones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b187866b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##################################################\n",
      "Table of Mean of Error Scores Accross 8 Forecast Points\n",
      "##################################################\n",
      "\n",
      "Sample size:  24  cyclones\n",
      "+----+-----------+--------+--------+--------+\n",
      "|    | Model     |    MAE |   RMSE |    MSE |\n",
      "|----+-----------+--------+--------+--------|\n",
      "|  0 | path_east | 0.1343 | 0.1608 | 0.0396 |\n",
      "|  1 | path_west | 0.1249 | 0.1448 | 0.0321 |\n",
      "+----+-----------+--------+--------+--------+\n",
      "\n",
      "\n",
      "##################################################\n",
      "FLIGNER-KILLEEN TEST FOR EQUALITY OF VARIANCE \n",
      "OF ERROR SCORES BETWEEN EAST AND WEST REGIONS\n",
      "##################################################\n",
      "\n",
      "Sample size:  24  cyclones\n",
      "+------+-----------+--------------------------+---------------------------+\n",
      "|      |   p_value | Significant alpha=0.05   | Equal Variance Assummed   |\n",
      "|------+-----------+--------------------------+---------------------------|\n",
      "| MAE  |     0.736 | no                       | yes                       |\n",
      "| RMSE |     0.765 | no                       | yes                       |\n",
      "| MSE  |     0.452 | no                       | yes                       |\n",
      "+------+-----------+--------------------------+---------------------------+\n",
      "\n",
      "\n",
      "##################################################\n",
      "T-TEST FOR TWO INDEPENDENT SAMPLES \n",
      "OF ERROR SCORES AT 95% CONFIDENCE LEVEL\n",
      "##################################################\n",
      "\n",
      "Sample size:  24  cyclones\n",
      "+------+-----------+--------------------------+\n",
      "|      |   p_value | Significant alpha=0.05   |\n",
      "|------+-----------+--------------------------|\n",
      "| MAE  |     0.567 | no                       |\n",
      "| RMSE |     0.416 | no                       |\n",
      "| MSE  |     0.401 | no                       |\n",
      "+------+-----------+--------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Read in bt_error_fit files generated from previous sessions\n",
    "\n",
    "f_path_east = pd.read_csv('bt_error_fit_path_east.csv')\n",
    "f_path_west = pd.read_csv('bt_error_fit_path_west.csv')\n",
    "\n",
    "\n",
    "# Get MAE, RMSE, MSE columns only\n",
    "east, west = f_path_east.iloc[:,-3:], f_path_west.iloc[:,-3:]\n",
    "\n",
    "if len(west) > len(east):\n",
    "    west = west.sample(len(east), random_state=555)\n",
    "elif len(west) < len(east):\n",
    "    east = east.sample(len(east), random_state=555)\n",
    "    \n",
    "errordf = [east, west]\n",
    "modelname = ['path_east','path_west']\n",
    "\n",
    "# Generate table of mean errors across all models\n",
    "mean_list=[]\n",
    "for df in errordf:\n",
    "    m = df.mean().values\n",
    "    mean_list.append(m)\n",
    "mean_df = pd.DataFrame(mean_list, columns= ['MAE','RMSE','MSE'])\n",
    "mean_df['Model'] = modelname\n",
    "mean_df = mean_df[['Model','MAE','RMSE','MSE']]\n",
    "print('\\n')\n",
    "print('#'*50)\n",
    "print('Table of Mean of Error Scores Accross 8 Forecast Points')\n",
    "print('#'*50)\n",
    "print('\\nSample size: ', len(west), ' cyclones')\n",
    "print(tabulate(mean_df.round(4), headers = 'keys', tablefmt = 'psql'))\n",
    "\n",
    "# Fligner-Killeen Test for Equality of Variance\n",
    "\n",
    "p_val = []\n",
    "for metrics in ['MAE','RMSE','MSE']:\n",
    "    p = stats.fligner(east[metrics], west[metrics]).pvalue.round(3)\n",
    "    p = p.tolist()\n",
    "    p_val.append(p)\n",
    "      \n",
    "fk_test = pd.DataFrame(p_val,index= ['MAE','RMSE','MSE'], columns=['p_value'])\n",
    "fk_test['Significant alpha=0.05'] = np.where(fk_test['p_value']<0.05,'yes', 'no')\n",
    "fk_test['Equal Variance Assummed'] = np.where(fk_test['p_value']<0.05,'no', 'yes')\n",
    "print('\\n')\n",
    "print('#'*50)\n",
    "print('FLIGNER-KILLEEN TEST FOR EQUALITY OF VARIANCE \\nOF ERROR SCORES BETWEEN EAST AND WEST REGIONS')\n",
    "print('#'*50)\n",
    "print('\\nSample size: ', len(west), ' cyclones')\n",
    "print(tabulate(fk_test, headers = 'keys', tablefmt = 'psql'))\n",
    "\n",
    "\n",
    "# Generate table of T-Test for Two Independent Samples\n",
    "p_val = []\n",
    "for metrics in ['MAE','RMSE','MSE']:\n",
    "    p = stats.ttest_ind(east[metrics], west[metrics]).pvalue.round(3)\n",
    "    p = p.tolist()\n",
    "    p_val.append(p)\n",
    "\n",
    "t_test = pd.DataFrame(p_val,index= ['MAE','RMSE','MSE'], columns=['p_value'])\n",
    "t_test['Significant alpha=0.05'] = np.where(t_test['p_value']<0.05,'yes', 'no')\n",
    "print('\\n')\n",
    "print('#'*50)\n",
    "print(f\"T-TEST FOR TWO INDEPENDENT SAMPLES \\nOF ERROR SCORES AT 95% CONFIDENCE LEVEL\")\n",
    "print('#'*50)\n",
    "print('\\nSample size: ', len(west), ' cyclones')\n",
    "print(tabulate(t_test, headers = 'keys', tablefmt = 'psql'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0439d",
   "metadata": {},
   "source": [
    "#### Based on Back test set performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86727657",
   "metadata": {},
   "source": [
    "1. Each validating run evaluates the performance of 1 validating cyclone.\n",
    "2. A validating run can have multiple back tests through out the length of series.\n",
    "3. Each back test produces a set of error scores.\n",
    "4. Sample size is the number of back test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11db5b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##################################################\n",
      "Table of Mean of Error Scores Accross 8 Forecast Points\n",
      "##################################################\n",
      "\n",
      "Sample size:  218  back test sets\n",
      "+----+-----------+--------+--------+--------+\n",
      "|    | Model     |    MAE |   RMSE |    MSE |\n",
      "|----+-----------+--------+--------+--------|\n",
      "|  0 | path_east | 0.1463 | 0.1717 | 0.0448 |\n",
      "|  1 | path_west | 0.1114 | 0.1301 | 0.0262 |\n",
      "+----+-----------+--------+--------+--------+\n",
      "\n",
      "\n",
      "##################################################\n",
      "FLIGNER-KILLEEN TEST FOR EQUALITY OF VARIANCE \n",
      "OF ERROR SCORES BETWEEN EAST AND WEST REGIONS\n",
      "##################################################\n",
      "\n",
      "Sample size:  218  back test sets\n",
      "+------+-----------+--------------------------+---------------------------+\n",
      "|      |   p_value | Significant alpha=0.05   | Equal Variance Assummed   |\n",
      "|------+-----------+--------------------------+---------------------------|\n",
      "| MAE  |     0.206 | no                       | yes                       |\n",
      "| RMSE |     0.158 | no                       | yes                       |\n",
      "| MSE  |     0     | yes                      | no                        |\n",
      "+------+-----------+--------------------------+---------------------------+\n",
      "\n",
      "\n",
      "##################################################\n",
      "T-TEST FOR TWO INDEPENDENT SAMPLES \n",
      "OF ERROR SCORES AT 95% CONFIDENCE LEVEL\n",
      "##################################################\n",
      "\n",
      "Sample size:  218  back test sets\n",
      "+------+-----------+--------------------------+\n",
      "|      |   p_value | Significant alpha=0.05   |\n",
      "|------+-----------+--------------------------|\n",
      "| MAE  |         0 | yes                      |\n",
      "| RMSE |         0 | yes                      |\n",
      "| MSE  |         0 | yes                      |\n",
      "+------+-----------+--------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Read in bt_error_fit files generated from previous sessions\n",
    "\n",
    "f_path_east = pd.read_csv('bt_set_error_path_east.csv')\n",
    "f_path_west = pd.read_csv('bt_set_error_path_west.csv')\n",
    "\n",
    "\n",
    "# Get MAE, RMSE, MSE columns only\n",
    "east, west = f_path_east.iloc[:,-3:], f_path_west.iloc[:,-3:]\n",
    "\n",
    "if len(west) > len(east):\n",
    "    west = west.sample(len(east), random_state=555)\n",
    "elif len(west) < len(east):\n",
    "    east = east.sample(len(east), random_state=555)\n",
    "    \n",
    "errordf = [east, west]\n",
    "modelname = ['path_east','path_west']\n",
    "\n",
    "# Generate table of mean errors across all models\n",
    "mean_list=[]\n",
    "for df in errordf:\n",
    "    m = df.mean().values\n",
    "    mean_list.append(m)\n",
    "mean_df = pd.DataFrame(mean_list, columns= ['MAE','RMSE','MSE'])\n",
    "mean_df['Model'] = modelname\n",
    "mean_df = mean_df[['Model','MAE','RMSE','MSE']]\n",
    "print('\\n')\n",
    "print('#'*50)\n",
    "print('Table of Mean of Error Scores Accross 8 Forecast Points')\n",
    "print('#'*50)\n",
    "print('\\nSample size: ', len(west), ' back test sets')\n",
    "print(tabulate(mean_df.round(4), headers = 'keys', tablefmt = 'psql'))\n",
    "\n",
    "\n",
    "# Fligner-Killeen Test for Equality of Variance\n",
    "\n",
    "p_val = []\n",
    "for metrics in ['MAE','RMSE','MSE']:\n",
    "    p = stats.fligner(east[metrics], west[metrics]).pvalue.round(3)\n",
    "    p = p.tolist()\n",
    "    p_val.append(p)\n",
    "      \n",
    "fk_test = pd.DataFrame(p_val,index= ['MAE','RMSE','MSE'], columns=['p_value'])\n",
    "fk_test['Significant alpha=0.05'] = np.where(fk_test['p_value']<0.05,'yes', 'no')\n",
    "fk_test['Equal Variance Assummed'] = np.where(fk_test['p_value']<0.05,'no', 'yes')\n",
    "print('\\n')\n",
    "print('#'*50)\n",
    "print('FLIGNER-KILLEEN TEST FOR EQUALITY OF VARIANCE \\nOF ERROR SCORES BETWEEN EAST AND WEST REGIONS')\n",
    "print('#'*50)\n",
    "print('\\nSample size: ', len(west), ' back test sets')\n",
    "print(tabulate(fk_test, headers = 'keys', tablefmt = 'psql'))\n",
    "\n",
    "# Generate table of T-Test for Two Independent Samples\n",
    "p_val = []\n",
    "for metrics in ['MAE','RMSE','MSE']:\n",
    "    if metrics == 'MSE':\n",
    "        p = stats.ttest_ind(east[metrics], west[metrics],equal_var=False).pvalue.round(5)\n",
    "    else:\n",
    "        p = stats.ttest_ind(east[metrics], west[metrics]).pvalue.round(5)\n",
    "    p = p.tolist()\n",
    "    p_val.append(p)\n",
    "\n",
    "t_test = pd.DataFrame(p_val,index= ['MAE','RMSE','MSE'], columns=['p_value'])\n",
    "t_test['Significant alpha=0.05'] = np.where(t_test['p_value']<0.05,'yes', 'no')\n",
    "print('\\n')\n",
    "print('#'*50)\n",
    "print(f\"T-TEST FOR TWO INDEPENDENT SAMPLES \\nOF ERROR SCORES AT 95% CONFIDENCE LEVEL\")\n",
    "print('#'*50)\n",
    "print('\\nSample size: ', len(west), ' back test sets')\n",
    "print(tabulate(t_test, headers = 'keys', tablefmt = 'psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3688002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of west cyclones:  60\n",
      "Count of east cyclones:  29\n",
      "Count of shared_zone cyclones:  5\n"
     ]
    }
   ],
   "source": [
    "# 94 cyclones used for modeling in this report during model fitting\n",
    "# obtained from train_cycs + 1\n",
    "\n",
    "modeling_cycs = ['AU200001_06U', 'AU200203_02U', 'AU200203_04U', 'AU200203_06U',\n",
    " 'AU200203_07U', 'AU200304_01U', 'AU200304_05U', 'AU200304_08U', 'AU200304_11U',\n",
    " 'AU200405_07U', 'AU200405_08U', 'AU200405_10U', 'AU200506_01U', 'AU200506_05U',\n",
    " 'AU200506_06U', 'AU200506_14U', 'AU200506_16U', 'AU200506_17U', 'AU200506_22U',\n",
    " 'AU200607_11U', 'AU200607_12U', 'AU200708_03U', 'AU200708_06U', 'AU200708_11U',\n",
    " 'AU200708_15U', 'AU200708_20U', 'AU200708_23U', 'AU200809_03U', 'AU200809_17U',\n",
    " 'AU200809_18U', 'AU200910_01U', 'AU200910_07U', 'AU200910_09U', 'AU200910_12U',\n",
    " 'AU201011_11U', 'AU201011_12U', 'AU201011_14U', 'AU201011_16U', 'AU201011_17U',\n",
    " 'AU201011_29U', 'AU201112_01U', 'AU201112_11U', 'AU201112_16U', 'AU201213_05U',\n",
    " 'AU201213_10U', 'AU201213_14U', 'AU201314_01U', 'AU201314_04U', 'AU201314_10U',\n",
    " 'AU201314_14U', 'AU201314_15U', 'AU201314_16U', 'AU201415_04U', 'AU201415_13U',\n",
    " 'AU201415_17U', 'AU201617_23U', 'AU201617_24U', 'AU201617_26U', 'AU201718_03U',\n",
    " 'AU201718_20U', 'AU201819_04U', 'AU201819_07U', 'AU201819_12U', 'AU201819_14U',\n",
    " 'AU201819_17U', 'AU201819_19U', 'AU201819_20U', 'AU201819_21U', 'AU201920_03U',\n",
    " 'AU201920_05U', 'AU201920_06U', 'AU201920_08U', 'AU201920_12U', 'AU202021_11U',\n",
    " 'AU202021_15U', 'AU202021_17U', 'AU202021_22U', 'AU202021_23U', 'AU202122_07U',\n",
    " 'AU202122_10U', 'AU202122_18U', 'AU202122_23U', 'AU202122_27U', 'AU202122_28U',\n",
    " 'AU202122_36U', 'AU202223_05U', 'AU202223_13U', 'AU202223_14U', 'AU202223_21U',\n",
    " 'AU202223_23U', 'AU202324_02U', 'AU202324_04U', 'AU202324_08U','AU202324_11U']\n",
    "\n",
    "\n",
    "# Count of cyclones in each region in modeling_cycs\n",
    "model_west = sum([c in west_only for c in modeling_cycs])\n",
    "model_east = sum([c in east_only for c in modeling_cycs])\n",
    "model_shared_zone = sum([c in shared_zone for c in modeling_cycs])\n",
    "print('Count of west cyclones: ', model_west)\n",
    "print('Count of east cyclones: ', model_east)\n",
    "print('Count of shared_zone cyclones: ', model_shared_zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765b4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
