{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d1ed25",
   "metadata": {},
   "source": [
    "# RESEARCH QUESTION 2 - Cyclone Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b157e3",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85363fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import geopandas as gpd\n",
    "import movingpandas as mpd\n",
    "from shapely.geometry import Point\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "import torch\n",
    "from darts import TimeSeries\n",
    "from darts.utils.callbacks import TFMProgressBar\n",
    "from darts.models import (NBEATSModel)\n",
    "from darts.metrics import mae, rmse, mse\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73175db5",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c076c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PREPROCESSING\n",
    "\n",
    "cyc = pd.read_csv('IDCKMSTM0S.csv', skiprows=3)\n",
    "\n",
    "cyc.columns.to_list()\n",
    "\n",
    "cyc1 = cyc[['NAME',\n",
    "            'DISTURBANCE_ID',\n",
    "            'TM',\n",
    "            'LAT',\n",
    "            'LON',\n",
    "            'CENTRAL_PRES',\n",
    "            'ENV_PRES',\n",
    "            'MN_RADIUS_GF_SECNE',\n",
    "            'MN_RADIUS_GF_SECSE',\n",
    "            'MN_RADIUS_GF_SECSW',\n",
    "            'MN_RADIUS_GF_SECNW']]\n",
    "\n",
    "sub1 = cyc1.loc[(cyc1['DISTURBANCE_ID']== 'AU199899_09U')]\n",
    "\n",
    "# Remove white space value from all object columns\n",
    "obcols = cyc1.select_dtypes(object).columns\n",
    "cyc1[obcols] = cyc1[obcols].apply(lambda x: x.replace(\" \",\"\"))\n",
    "\n",
    "# Remove negative signs in LON column\n",
    "cyc1['LON'] = cyc1['LON'].apply(lambda x: x.replace(\"-\",\"\"))\n",
    "\n",
    "# Remove trailing white spaces from string values\n",
    "cyc1[['NAME',\n",
    "      'DISTURBANCE_ID',\n",
    "      'TM']] = cyc1[['NAME',\n",
    "                     'DISTURBANCE_ID',\n",
    "                     'TM']].apply(lambda x: x.str.strip())\n",
    "\n",
    "# Convert dtypes from object to numeric\n",
    "cyc1[['LAT',\n",
    "      'LON',\n",
    "      'CENTRAL_PRES',\n",
    "      'ENV_PRES',\n",
    "      'MN_RADIUS_GF_SECNE',\n",
    "      'MN_RADIUS_GF_SECSE',\n",
    "      'MN_RADIUS_GF_SECSW',\n",
    "      'MN_RADIUS_GF_SECNW']] = cyc1[['LAT',\n",
    "                                     'LON',\n",
    "                                     'CENTRAL_PRES',\n",
    "                                     'ENV_PRES',\n",
    "                                     'MN_RADIUS_GF_SECNE',\n",
    "                                     'MN_RADIUS_GF_SECSE',\n",
    "                                     'MN_RADIUS_GF_SECSW',\n",
    "                                     'MN_RADIUS_GF_SECNW']].apply(lambda x:pd.to_numeric(x))\n",
    "\n",
    "# Convert TM to datetime\n",
    "cyc1['TM'] = pd.to_datetime(cyc1['TM'], dayfirst=True)\n",
    "\n",
    "cyc2 = cyc1.copy()\n",
    "\n",
    "# Remove all observations that have missing values in TM\n",
    "cyc2 = cyc2[cyc2['TM'].notnull()]\n",
    "\n",
    "# Resample to 6 hourly interval\n",
    "cyc2 = cyc2.set_index('TM').groupby(['DISTURBANCE_ID', 'NAME']).resample('6H').mean()\n",
    "\n",
    "cyc3= cyc2.reset_index(level=['TM', 'NAME'])\n",
    "\n",
    "# interpolate 'inside' valid values for each group\n",
    "groupunique = cyc3.index.unique().to_list()\n",
    "for i in groupunique:\n",
    "    cyc3.loc[[i],['LAT',\n",
    "                  'LON',\n",
    "                  'CENTRAL_PRES',\n",
    "                  'ENV_PRES',\n",
    "                  'MN_RADIUS_GF_SECNE',\n",
    "                  'MN_RADIUS_GF_SECSE',\n",
    "                  'MN_RADIUS_GF_SECSW',\n",
    "                  'MN_RADIUS_GF_SECNW']] = cyc3.loc[[i],['LAT',\n",
    "                                                         'LON',\n",
    "                                                         'CENTRAL_PRES',\n",
    "                                                         'ENV_PRES',\n",
    "                                                         'MN_RADIUS_GF_SECNE',\n",
    "                                                         'MN_RADIUS_GF_SECSE',\n",
    "                                                         'MN_RADIUS_GF_SECSW',\n",
    "                                                         'MN_RADIUS_GF_SECNW']].interpolate(limit=20,\n",
    "                                                                                            limit_area='inside')\n",
    "\n",
    "cyc3 = cyc3.reset_index()\n",
    "\n",
    "# drop all rows with at least 2 NAs in R34s\n",
    "cyc3 = cyc3.drop(cyc3.loc[sum([(cyc3['MN_RADIUS_GF_SECNE'].isnull()),\n",
    "                               (cyc3['MN_RADIUS_GF_SECSE'].isnull()),\n",
    "                               (cyc3['MN_RADIUS_GF_SECSW'].isnull()),\n",
    "                               (cyc3['MN_RADIUS_GF_SECNW'].isnull())])>=2].index)\n",
    "\n",
    "# drop all rows with at least 2 NAs\n",
    "cyc3 = cyc3.drop(cyc3.loc[sum([(cyc3['MN_RADIUS_GF_SECNE'].isnull()),\n",
    "                               (cyc3['MN_RADIUS_GF_SECSE'].isnull()),\n",
    "                               (cyc3['MN_RADIUS_GF_SECSW'].isnull()),\n",
    "                               (cyc3['MN_RADIUS_GF_SECNW'].isnull()),\n",
    "                               (cyc3['ENV_PRES'].isnull())])>=2].index)\n",
    "\n",
    "# kNN-neighbour imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "cyc3_imp = imputer.fit_transform(cyc3[['ENV_PRES',\n",
    "                                       'MN_RADIUS_GF_SECNE',\n",
    "                                       'MN_RADIUS_GF_SECSE',\n",
    "                                       'MN_RADIUS_GF_SECSW',\n",
    "                                       'MN_RADIUS_GF_SECNW']])\n",
    "\n",
    "cyc3_imp = pd.DataFrame(cyc3_imp, index= cyc3.index,\n",
    "                        columns=['ENV_PRES',\n",
    "                                 'MN_RADIUS_GF_SECNE',\n",
    "                                 'MN_RADIUS_GF_SECSE',\n",
    "                                 'MN_RADIUS_GF_SECSW',\n",
    "                                 'MN_RADIUS_GF_SECNW'])\n",
    "\n",
    "cyc4 = cyc3[['LAT',\n",
    "             'LON',\n",
    "             'CENTRAL_PRES']]\n",
    "\n",
    "cyc4[['ENV_PRES',\n",
    "      'MN_RADIUS_GF_SECNE',\n",
    "      'MN_RADIUS_GF_SECSE',\n",
    "      'MN_RADIUS_GF_SECSW',\n",
    "      'MN_RADIUS_GF_SECNW']] = cyc3_imp\n",
    "\n",
    "cyc4 = cyc3.set_index('TM').groupby(['DISTURBANCE_ID']).resample('6H').interpolate()\n",
    "cyc4 = cyc4.reset_index(level='TM')\n",
    "\n",
    "cyc4[['NAME', 'DISTURBANCE_ID']] = cyc4[['NAME', 'DISTURBANCE_ID']].ffill()\n",
    "\n",
    "cyc4 = cyc4.dropna()\n",
    "cyc4 = cyc4.reset_index(drop=True)\n",
    "\n",
    "# only keep cyclone groups with at least 3 observations\n",
    "s = cyc4.groupby('DISTURBANCE_ID').size() >= 4\n",
    "cyc5 = cyc4.loc[cyc4['DISTURBANCE_ID'].isin(s[s].index)]\n",
    "\n",
    "cyc5.to_csv('cyc5.csv', index=False)\n",
    "\n",
    "################################################################################\n",
    "# Split data based on whether a cyclone ever exists East or West of a\n",
    "#  particular line of Longitude\n",
    "\n",
    "# Only keep cyclones that travel West of a particular Longitude (threshold_LON)\n",
    "threshold_LON = 131 # Approx Longitude of Darwin\n",
    "# Obtain UID of cyclones to keep in West\n",
    "UIDW = cyc5.groupby('DISTURBANCE_ID').filter(lambda x: (x['LON'] < threshold_LON).any())\n",
    "UIDW = UIDW['DISTURBANCE_ID'].unique()\n",
    "# Filter dataframe\n",
    "cyc5W = cyc5[cyc5['DISTURBANCE_ID'].isin(UIDW)]\n",
    "cyc5W.to_csv('cyc5W.csv', index=False)\n",
    "\n",
    "# Only keep cyclones that travel East of a particular Longitude (threshold_LON)\n",
    "# Obtain UID of cyclones to keep in East\n",
    "UIDE = cyc5.groupby('DISTURBANCE_ID').filter(lambda x: (x['LON'] > threshold_LON).any())\n",
    "UIDE = UIDE['DISTURBANCE_ID'].unique()\n",
    "# Filter dataframe\n",
    "cyc5E = cyc5[cyc5['DISTURBANCE_ID'].isin(UIDE)]\n",
    "cyc5E.to_csv('cyc5E.csv', index=False)\n",
    "################################################################################\n",
    "\n",
    "################################################################################\n",
    "# Generate lists of dataframes: Total, East region only and West region only\n",
    "cyc6 = cyc5.groupby('DISTURBANCE_ID')           # For RQ1 and RQ2\n",
    "#cyc6 = cyc5W.groupby('DISTURBANCE_ID')         # For RQ3\n",
    "#cyc6 = cyc5E.groupby('DISTURBANCE_ID')         # For RQ3\n",
    "################################################################################\n",
    "\n",
    "# Write each group as a record of dictionary named d\n",
    "d = dict()\n",
    "for k in list(cyc6.groups.keys()):\n",
    "    dfname = str(k)\n",
    "    d[dfname] = cyc6.get_group(k)\n",
    "\n",
    "# get data frames from dictionary d\n",
    "df_list = []\n",
    "for i in list(cyc6.groups.keys()):\n",
    "    df = 'd[\"' + str(i) + '\"]'\n",
    "    df_list.append(df)\n",
    "\n",
    "################################################################################\n",
    "# Assign dataframes to corresponding names\n",
    "### For RQ1 and RQ2 ###\n",
    "# All\n",
    "(AU199697_14U,AU199899_05U,AU199900_13U,AU200001_06U,AU200203_01U,AU200203_02U,\n",
    " AU200203_03U,AU200203_04U,AU200203_06U,AU200203_07U,AU200304_01U,AU200304_02U,\n",
    " AU200304_03U,AU200304_04U,AU200304_05U,AU200304_07U,AU200304_08U,AU200304_09U,\n",
    " AU200304_11U,AU200405_01U,AU200405_02U,AU200405_03U,AU200405_04U,AU200405_05U,\n",
    " AU200405_07U,AU200405_08U,AU200405_09U,AU200405_10U,AU200506_01U,AU200506_05U,\n",
    " AU200506_06U,AU200506_08U,AU200506_10U,AU200506_14U,AU200506_15U,AU200506_16U,\n",
    " AU200506_17U,AU200506_20U,AU200506_22U,AU200607_05U,AU200607_11U,AU200607_12U,\n",
    " AU200607_13U,AU200607_16U,AU200708_01U,AU200708_03U,AU200708_06U,AU200708_08U,\n",
    " AU200708_11U,AU200708_15U,AU200708_20U,AU200708_22U,AU200708_23U,AU200809_02U,\n",
    " AU200809_03U,AU200809_06U,AU200809_08U,AU200809_10U,AU200809_17U,AU200809_18U,\n",
    " AU200809_23U,AU200910_01U,AU200910_06U,AU200910_07U,AU200910_09U,AU200910_11U,\n",
    " AU200910_12U,AU200910_13U,AU201011_02U,AU201011_09U,AU201011_10U,AU201011_11U,\n",
    " AU201011_12U,AU201011_14U,AU201011_16U,AU201011_17U,AU201011_29U,AU201112_01U,\n",
    " AU201112_04U,AU201112_07U,AU201112_11U,AU201112_12U,AU201112_15U,AU201112_16U,\n",
    " AU201213_03U,AU201213_04U,AU201213_05U,AU201213_10U,AU201213_13U,AU201213_14U,\n",
    " AU201213_17U,AU201213_18U,AU201314_01U,AU201314_03U,AU201314_04U,AU201314_07U,\n",
    " AU201314_09U,AU201314_10U,AU201314_14U,AU201314_15U,AU201314_16U,AU201415_04U,\n",
    " AU201415_13U,AU201415_14U,AU201415_16U,AU201415_17U,AU201415_19U,AU201415_21U,\n",
    " AU201415_24U,AU201516_08U,AU201516_09U,AU201516_10U,AU201617_07U,AU201617_19U,\n",
    " AU201617_20U,AU201617_23U,AU201617_24U,AU201617_26U,AU201617_29U,AU201617_30U,\n",
    " AU201718_02U,AU201718_03U,AU201718_06U,AU201718_17U,AU201718_20U,AU201718_22U,\n",
    " AU201718_24U,AU201819_04U,AU201819_05U,AU201819_07U,AU201819_12U,AU201819_14U,\n",
    " AU201819_17U,AU201819_19U,AU201819_20U,AU201819_21U,AU201819_25U,AU201819_26U,\n",
    " AU201920_03U,AU201920_05U,AU201920_06U,AU201920_07U,AU201920_08U,AU201920_10U,\n",
    " AU201920_12U,AU202021_07U,AU202021_09U,AU202021_10U,AU202021_11U,AU202021_15U,\n",
    " AU202021_17U,AU202021_22U,AU202021_23U,AU202122_02U,AU202122_05U,AU202122_07U,\n",
    " AU202122_08U,AU202122_10U,AU202122_18U,AU202122_22U,AU202122_23U,AU202122_27U,\n",
    " AU202122_28U,AU202122_36U,AU202223_01U,AU202223_05U,AU202223_13U,AU202223_14U,\n",
    " AU202223_21U,AU202223_23U,AU202324_02U,AU202324_04U,AU202324_05U,AU202324_08U,\n",
    " AU202324_09U,AU202324_11U,\n",
    " AU202324_13U) = (d[\"AU199697_14U\"],d[\"AU199899_05U\"],d[\"AU199900_13U\"],d[\"AU200001_06U\"],\n",
    "                  d[\"AU200203_01U\"],d[\"AU200203_02U\"],d[\"AU200203_03U\"],d[\"AU200203_04U\"],\n",
    "                  d[\"AU200203_06U\"],d[\"AU200203_07U\"],d[\"AU200304_01U\"],d[\"AU200304_02U\"],\n",
    "                  d[\"AU200304_03U\"],d[\"AU200304_04U\"],d[\"AU200304_05U\"],d[\"AU200304_07U\"],\n",
    "                  d[\"AU200304_08U\"],d[\"AU200304_09U\"],d[\"AU200304_11U\"],d[\"AU200405_01U\"],\n",
    "                  d[\"AU200405_02U\"],d[\"AU200405_03U\"],d[\"AU200405_04U\"],d[\"AU200405_05U\"],\n",
    "                  d[\"AU200405_07U\"],d[\"AU200405_08U\"],d[\"AU200405_09U\"],d[\"AU200405_10U\"],\n",
    "                  d[\"AU200506_01U\"],d[\"AU200506_05U\"],d[\"AU200506_06U\"],d[\"AU200506_08U\"],\n",
    "                  d[\"AU200506_10U\"],d[\"AU200506_14U\"],d[\"AU200506_15U\"],d[\"AU200506_16U\"],\n",
    "                  d[\"AU200506_17U\"],d[\"AU200506_20U\"],d[\"AU200506_22U\"],d[\"AU200607_05U\"],\n",
    "                  d[\"AU200607_11U\"],d[\"AU200607_12U\"],d[\"AU200607_13U\"],d[\"AU200607_16U\"],\n",
    "                  d[\"AU200708_01U\"],d[\"AU200708_03U\"],d[\"AU200708_06U\"],d[\"AU200708_08U\"],\n",
    "                  d[\"AU200708_11U\"],d[\"AU200708_15U\"],d[\"AU200708_20U\"],d[\"AU200708_22U\"],\n",
    "                  d[\"AU200708_23U\"],d[\"AU200809_02U\"],d[\"AU200809_03U\"],d[\"AU200809_06U\"],\n",
    "                  d[\"AU200809_08U\"],d[\"AU200809_10U\"],d[\"AU200809_17U\"],d[\"AU200809_18U\"],\n",
    "                  d[\"AU200809_23U\"],d[\"AU200910_01U\"],d[\"AU200910_06U\"],d[\"AU200910_07U\"],\n",
    "                  d[\"AU200910_09U\"],d[\"AU200910_11U\"],d[\"AU200910_12U\"],d[\"AU200910_13U\"],\n",
    "                  d[\"AU201011_02U\"],d[\"AU201011_09U\"],d[\"AU201011_10U\"],d[\"AU201011_11U\"],\n",
    "                  d[\"AU201011_12U\"],d[\"AU201011_14U\"],d[\"AU201011_16U\"],d[\"AU201011_17U\"],\n",
    "                  d[\"AU201011_29U\"],d[\"AU201112_01U\"],d[\"AU201112_04U\"],d[\"AU201112_07U\"],\n",
    "                  d[\"AU201112_11U\"],d[\"AU201112_12U\"],d[\"AU201112_15U\"],d[\"AU201112_16U\"],\n",
    "                  d[\"AU201213_03U\"],d[\"AU201213_04U\"],d[\"AU201213_05U\"],d[\"AU201213_10U\"],\n",
    "                  d[\"AU201213_13U\"],d[\"AU201213_14U\"],d[\"AU201213_17U\"],d[\"AU201213_18U\"],\n",
    "                  d[\"AU201314_01U\"],d[\"AU201314_03U\"],d[\"AU201314_04U\"],d[\"AU201314_07U\"],\n",
    "                  d[\"AU201314_09U\"],d[\"AU201314_10U\"],d[\"AU201314_14U\"],d[\"AU201314_15U\"],\n",
    "                  d[\"AU201314_16U\"],d[\"AU201415_04U\"],d[\"AU201415_13U\"],d[\"AU201415_14U\"],\n",
    "                  d[\"AU201415_16U\"],d[\"AU201415_17U\"],d[\"AU201415_19U\"],d[\"AU201415_21U\"],\n",
    "                  d[\"AU201415_24U\"],d[\"AU201516_08U\"],d[\"AU201516_09U\"],d[\"AU201516_10U\"],\n",
    "                  d[\"AU201617_07U\"],d[\"AU201617_19U\"],d[\"AU201617_20U\"],d[\"AU201617_23U\"],\n",
    "                  d[\"AU201617_24U\"],d[\"AU201617_26U\"],d[\"AU201617_29U\"],d[\"AU201617_30U\"],\n",
    "                  d[\"AU201718_02U\"],d[\"AU201718_03U\"],d[\"AU201718_06U\"],d[\"AU201718_17U\"],\n",
    "                  d[\"AU201718_20U\"],d[\"AU201718_22U\"],d[\"AU201718_24U\"],d[\"AU201819_04U\"],\n",
    "                  d[\"AU201819_05U\"],d[\"AU201819_07U\"],d[\"AU201819_12U\"],d[\"AU201819_14U\"],\n",
    "                  d[\"AU201819_17U\"],d[\"AU201819_19U\"],d[\"AU201819_20U\"],d[\"AU201819_21U\"],\n",
    "                  d[\"AU201819_25U\"],d[\"AU201819_26U\"],d[\"AU201920_03U\"],d[\"AU201920_05U\"],\n",
    "                  d[\"AU201920_06U\"],d[\"AU201920_07U\"],d[\"AU201920_08U\"],d[\"AU201920_10U\"],\n",
    "                  d[\"AU201920_12U\"],d[\"AU202021_07U\"],d[\"AU202021_09U\"],d[\"AU202021_10U\"],\n",
    "                  d[\"AU202021_11U\"],d[\"AU202021_15U\"],d[\"AU202021_17U\"],d[\"AU202021_22U\"],\n",
    "                  d[\"AU202021_23U\"],d[\"AU202122_02U\"],d[\"AU202122_05U\"],d[\"AU202122_07U\"],\n",
    "                  d[\"AU202122_08U\"],d[\"AU202122_10U\"],d[\"AU202122_18U\"],d[\"AU202122_22U\"],\n",
    "                  d[\"AU202122_23U\"],d[\"AU202122_27U\"],d[\"AU202122_28U\"],d[\"AU202122_36U\"],\n",
    "                  d[\"AU202223_01U\"],d[\"AU202223_05U\"],d[\"AU202223_13U\"],d[\"AU202223_14U\"],\n",
    "                  d[\"AU202223_21U\"],d[\"AU202223_23U\"],d[\"AU202324_02U\"],d[\"AU202324_04U\"],\n",
    "                  d[\"AU202324_05U\"],d[\"AU202324_08U\"],d[\"AU202324_09U\"],d[\"AU202324_11U\"],\n",
    "                  d[\"AU202324_13U\"])\n",
    "df_all = [AU199697_14U,AU199899_05U,AU199900_13U,AU200001_06U,AU200203_01U,AU200203_02U,\n",
    "          AU200203_03U,AU200203_04U,AU200203_06U,AU200203_07U,AU200304_01U,AU200304_02U,\n",
    "          AU200304_03U,AU200304_04U,AU200304_05U,AU200304_07U,AU200304_08U,AU200304_09U,\n",
    "          AU200304_11U,AU200405_01U,AU200405_02U,AU200405_03U,AU200405_04U,AU200405_05U,\n",
    "          AU200405_07U,AU200405_08U,AU200405_09U,AU200405_10U,AU200506_01U,AU200506_05U,\n",
    "          AU200506_06U,AU200506_08U,AU200506_10U,AU200506_14U,AU200506_15U,AU200506_16U,\n",
    "          AU200506_17U,AU200506_20U,AU200506_22U,AU200607_05U,AU200607_11U,AU200607_12U,\n",
    "          AU200607_13U,AU200607_16U,AU200708_01U,AU200708_03U,AU200708_06U,AU200708_08U,\n",
    "          AU200708_11U,AU200708_15U,AU200708_20U,AU200708_22U,AU200708_23U,AU200809_02U,\n",
    "          AU200809_03U,AU200809_06U,AU200809_08U,AU200809_10U,AU200809_17U,AU200809_18U,\n",
    "          AU200809_23U,AU200910_01U,AU200910_06U,AU200910_07U,AU200910_09U,AU200910_11U,\n",
    "          AU200910_12U,AU200910_13U,AU201011_02U,AU201011_09U,AU201011_10U,AU201011_11U,\n",
    "          AU201011_12U,AU201011_14U,AU201011_16U,AU201011_17U,AU201011_29U,AU201112_01U,\n",
    "          AU201112_04U,AU201112_07U,AU201112_11U,AU201112_12U,AU201112_15U,AU201112_16U,\n",
    "          AU201213_03U,AU201213_04U,AU201213_05U,AU201213_10U,AU201213_13U,AU201213_14U,\n",
    "          AU201213_17U,AU201213_18U,AU201314_01U,AU201314_03U,AU201314_04U,AU201314_07U,\n",
    "          AU201314_09U,AU201314_10U,AU201314_14U,AU201314_15U,AU201314_16U,AU201415_04U,\n",
    "          AU201415_13U,AU201415_14U,AU201415_16U,AU201415_17U,AU201415_19U,AU201415_21U,\n",
    "          AU201415_24U,AU201516_08U,AU201516_09U,AU201516_10U,AU201617_07U,AU201617_19U,\n",
    "          AU201617_20U,AU201617_23U,AU201617_24U,AU201617_26U,AU201617_29U,AU201617_30U,\n",
    "          AU201718_02U,AU201718_03U,AU201718_06U,AU201718_17U,AU201718_20U,AU201718_22U,\n",
    "          AU201718_24U,AU201819_04U,AU201819_05U,AU201819_07U,AU201819_12U,AU201819_14U,\n",
    "          AU201819_17U,AU201819_19U,AU201819_20U,AU201819_21U,AU201819_25U,AU201819_26U,\n",
    "          AU201920_03U,AU201920_05U,AU201920_06U,AU201920_07U,AU201920_08U,AU201920_10U,\n",
    "          AU201920_12U,AU202021_07U,AU202021_09U,AU202021_10U,AU202021_11U,AU202021_15U,\n",
    "          AU202021_17U,AU202021_22U,AU202021_23U,AU202122_02U,AU202122_05U,AU202122_07U,\n",
    "          AU202122_08U,AU202122_10U,AU202122_18U,AU202122_22U,AU202122_23U,AU202122_27U,\n",
    "          AU202122_28U,AU202122_36U,AU202223_01U,AU202223_05U,AU202223_13U,AU202223_14U,\n",
    "          AU202223_21U,AU202223_23U,AU202324_02U,AU202324_04U,AU202324_05U,AU202324_08U,\n",
    "          AU202324_09U,AU202324_11U,AU202324_13U]\n",
    "### For RQ3 ###\n",
    "# West region only\n",
    "(AU199697_14U,AU199899_05U,AU199900_13U,AU200203_02U,AU200203_03U,AU200203_04U,AU200203_06U,\n",
    " AU200203_07U,AU200304_01U,AU200304_03U,AU200304_05U,AU200304_07U,AU200304_08U,AU200304_11U,\n",
    " AU200405_01U,AU200405_02U,AU200405_03U,AU200405_04U,AU200405_07U,AU200405_08U,AU200405_09U,\n",
    " AU200506_01U,AU200506_05U,AU200506_06U,AU200506_14U,AU200506_16U,AU200506_20U,AU200607_11U,\n",
    " AU200607_12U,AU200607_13U,AU200708_01U,AU200708_06U,AU200708_08U,AU200708_11U,AU200708_15U,\n",
    " AU200708_20U,AU200708_22U,AU200708_23U,AU200809_02U,AU200809_03U,AU200809_08U,AU200809_10U,\n",
    " AU200809_18U,AU200910_01U,AU200910_06U,AU200910_12U,AU200910_13U,AU201011_02U,AU201011_09U,\n",
    " AU201011_12U,AU201011_16U,AU201011_17U,AU201011_29U,AU201112_01U,AU201112_07U,AU201112_11U,\n",
    " AU201112_15U,AU201112_16U,AU201213_04U,AU201213_05U,AU201213_10U,AU201213_17U,AU201314_01U,\n",
    " AU201314_03U,AU201314_04U,AU201314_09U,AU201314_14U,AU201314_16U,AU201415_04U,AU201415_16U,\n",
    " AU201415_19U,AU201415_21U,AU201516_08U,AU201516_09U,AU201617_07U,AU201617_20U,AU201617_23U,\n",
    " AU201617_26U,AU201617_29U,AU201617_30U,AU201718_02U,AU201718_03U,AU201718_06U,AU201718_17U,\n",
    " AU201718_20U,AU201819_05U,AU201819_12U,AU201819_17U,AU201819_19U,AU201819_21U,AU201819_25U,\n",
    " AU201920_03U,AU201920_05U,AU201920_08U,AU202021_07U,AU202021_10U,AU202021_15U,AU202021_22U,\n",
    " AU202021_23U,AU202122_02U,AU202122_05U,AU202122_22U,AU202122_23U,AU202122_27U,AU202122_28U,\n",
    " AU202122_36U,AU202223_01U,AU202223_05U,AU202223_13U,AU202223_21U,AU202223_23U,AU202324_04U,\n",
    " AU202324_08U,\n",
    " AU202324_11U) = (d[\"AU199697_14U\"],d[\"AU199899_05U\"],d[\"AU199900_13U\"],d[\"AU200203_02U\"],\n",
    "                  d[\"AU200203_03U\"],d[\"AU200203_04U\"],d[\"AU200203_06U\"],d[\"AU200203_07U\"],\n",
    "                  d[\"AU200304_01U\"],d[\"AU200304_03U\"],d[\"AU200304_05U\"],d[\"AU200304_07U\"],\n",
    "                  d[\"AU200304_08U\"],d[\"AU200304_11U\"],d[\"AU200405_01U\"],d[\"AU200405_02U\"],\n",
    "                  d[\"AU200405_03U\"],d[\"AU200405_04U\"],d[\"AU200405_07U\"],d[\"AU200405_08U\"],\n",
    "                  d[\"AU200405_09U\"],d[\"AU200506_01U\"],d[\"AU200506_05U\"],d[\"AU200506_06U\"],\n",
    "                  d[\"AU200506_14U\"],d[\"AU200506_16U\"],d[\"AU200506_20U\"],d[\"AU200607_11U\"],\n",
    "                  d[\"AU200607_12U\"],d[\"AU200607_13U\"],d[\"AU200708_01U\"],d[\"AU200708_06U\"],\n",
    "                  d[\"AU200708_08U\"],d[\"AU200708_11U\"],d[\"AU200708_15U\"],d[\"AU200708_20U\"],\n",
    "                  d[\"AU200708_22U\"],d[\"AU200708_23U\"],d[\"AU200809_02U\"],d[\"AU200809_03U\"],\n",
    "                  d[\"AU200809_08U\"],d[\"AU200809_10U\"],d[\"AU200809_18U\"],d[\"AU200910_01U\"],\n",
    "                  d[\"AU200910_06U\"],d[\"AU200910_12U\"],d[\"AU200910_13U\"],d[\"AU201011_02U\"],\n",
    "                  d[\"AU201011_09U\"],d[\"AU201011_12U\"],d[\"AU201011_16U\"],d[\"AU201011_17U\"],\n",
    "                  d[\"AU201011_29U\"],d[\"AU201112_01U\"],d[\"AU201112_07U\"],d[\"AU201112_11U\"],\n",
    "                  d[\"AU201112_15U\"],d[\"AU201112_16U\"],d[\"AU201213_04U\"],d[\"AU201213_05U\"],\n",
    "                  d[\"AU201213_10U\"],d[\"AU201213_17U\"],d[\"AU201314_01U\"],d[\"AU201314_03U\"],\n",
    "                  d[\"AU201314_04U\"],d[\"AU201314_09U\"],d[\"AU201314_14U\"],d[\"AU201314_16U\"],\n",
    "                  d[\"AU201415_04U\"],d[\"AU201415_16U\"],d[\"AU201415_19U\"],d[\"AU201415_21U\"],\n",
    "                  d[\"AU201516_08U\"],d[\"AU201516_09U\"],d[\"AU201617_07U\"],d[\"AU201617_20U\"],\n",
    "                  d[\"AU201617_23U\"],d[\"AU201617_26U\"],d[\"AU201617_29U\"],d[\"AU201617_30U\"],\n",
    "                  d[\"AU201718_02U\"],d[\"AU201718_03U\"],d[\"AU201718_06U\"],d[\"AU201718_17U\"],\n",
    "                  d[\"AU201718_20U\"],d[\"AU201819_05U\"],d[\"AU201819_12U\"],d[\"AU201819_17U\"],\n",
    "                  d[\"AU201819_19U\"],d[\"AU201819_21U\"],d[\"AU201819_25U\"],d[\"AU201920_03U\"],\n",
    "                  d[\"AU201920_05U\"],d[\"AU201920_08U\"],d[\"AU202021_07U\"],d[\"AU202021_10U\"],\n",
    "                  d[\"AU202021_15U\"],d[\"AU202021_22U\"],d[\"AU202021_23U\"],d[\"AU202122_02U\"],\n",
    "                  d[\"AU202122_05U\"],d[\"AU202122_22U\"],d[\"AU202122_23U\"],d[\"AU202122_27U\"],\n",
    "                  d[\"AU202122_28U\"],d[\"AU202122_36U\"],d[\"AU202223_01U\"],d[\"AU202223_05U\"],\n",
    "                  d[\"AU202223_13U\"],d[\"AU202223_21U\"],d[\"AU202223_23U\"],d[\"AU202324_04U\"],\n",
    "                  d[\"AU202324_08U\"],d[\"AU202324_11U\"])\n",
    "\n",
    "df_west = [AU199697_14U,AU199899_05U,AU199900_13U,AU200203_02U,AU200203_03U,AU200203_04U,\n",
    "           AU200203_06U,AU200203_07U,AU200304_01U,AU200304_03U,AU200304_05U,AU200304_07U,\n",
    "           AU200304_08U,AU200304_11U,AU200405_01U,AU200405_02U,AU200405_03U,AU200405_04U,\n",
    "           AU200405_07U,AU200405_08U,AU200405_09U,AU200506_01U,AU200506_05U,AU200506_06U,\n",
    "           AU200506_14U,AU200506_16U,AU200506_20U,AU200607_11U,AU200607_12U,AU200607_13U,\n",
    "           AU200708_01U,AU200708_06U,AU200708_08U,AU200708_11U,AU200708_15U,AU200708_20U,\n",
    "           AU200708_22U,AU200708_23U,AU200809_02U,AU200809_03U,AU200809_08U,AU200809_10U,\n",
    "           AU200809_18U,AU200910_01U,AU200910_06U,AU200910_12U,AU200910_13U,AU201011_02U,\n",
    "           AU201011_09U,AU201011_12U,AU201011_16U,AU201011_17U,AU201011_29U,AU201112_01U,\n",
    "           AU201112_07U,AU201112_11U,AU201112_15U,AU201112_16U,AU201213_04U,AU201213_05U,\n",
    "           AU201213_10U,AU201213_17U,AU201314_01U,AU201314_03U,AU201314_04U,AU201314_09U,\n",
    "           AU201314_14U,AU201314_16U,AU201415_04U,AU201415_16U,AU201415_19U,AU201415_21U,\n",
    "           AU201516_08U,AU201516_09U,AU201617_07U,AU201617_20U,AU201617_23U,AU201617_26U,\n",
    "           AU201617_29U,AU201617_30U,AU201718_02U,AU201718_03U,AU201718_06U,AU201718_17U,\n",
    "           AU201718_20U,AU201819_05U,AU201819_12U,AU201819_17U,AU201819_19U,AU201819_21U,\n",
    "           AU201819_25U,AU201920_03U,AU201920_05U,AU201920_08U,AU202021_07U,AU202021_10U,\n",
    "           AU202021_15U,AU202021_22U,AU202021_23U,AU202122_02U,AU202122_05U,AU202122_22U,\n",
    "           AU202122_23U,AU202122_27U,AU202122_28U,AU202122_36U,AU202223_01U,AU202223_05U,\n",
    "           AU202223_13U,AU202223_21U,AU202223_23U,AU202324_04U,AU202324_08U,AU202324_11U]\n",
    "# East region only\n",
    "(AU200001_06U,AU200203_01U,AU200203_06U,AU200304_02U,AU200304_04U,AU200304_09U,\n",
    " AU200405_05U,AU200405_07U,AU200405_10U,AU200506_08U,AU200506_10U,AU200506_15U,\n",
    " AU200506_17U,AU200506_22U,AU200607_05U,AU200607_16U,AU200708_03U,AU200708_08U,\n",
    " AU200809_06U,AU200809_17U,AU200809_23U,AU200910_07U,AU200910_09U,AU200910_11U,\n",
    " AU201011_10U,AU201011_11U,AU201011_14U,AU201011_17U,AU201112_04U,AU201112_12U,\n",
    " AU201213_03U,AU201213_13U,AU201213_14U,AU201213_18U,AU201314_01U,AU201314_07U,\n",
    " AU201314_10U,AU201314_15U,AU201415_13U,AU201415_14U,AU201415_17U,AU201415_24U,\n",
    " AU201516_10U,AU201617_19U,AU201617_24U,AU201718_20U,AU201718_22U,AU201718_24U,\n",
    " AU201819_04U,AU201819_07U,AU201819_14U,AU201819_20U,AU201819_26U,AU201920_06U,\n",
    " AU201920_07U,AU201920_10U,AU201920_12U,AU202021_09U,AU202021_11U,AU202021_17U,\n",
    " AU202122_07U,AU202122_08U,AU202122_10U,AU202122_18U,AU202223_14U,AU202324_02U,\n",
    " AU202324_05U,AU202324_09U,\n",
    " AU202324_13U) = (d[\"AU200001_06U\"],d[\"AU200203_01U\"],d[\"AU200203_06U\"],d[\"AU200304_02U\"],\n",
    "                  d[\"AU200304_04U\"],d[\"AU200304_09U\"],d[\"AU200405_05U\"],d[\"AU200405_07U\"],\n",
    "                  d[\"AU200405_10U\"],d[\"AU200506_08U\"],d[\"AU200506_10U\"],d[\"AU200506_15U\"],\n",
    "                  d[\"AU200506_17U\"],d[\"AU200506_22U\"],d[\"AU200607_05U\"],d[\"AU200607_16U\"],\n",
    "                  d[\"AU200708_03U\"],d[\"AU200708_08U\"],d[\"AU200809_06U\"],d[\"AU200809_17U\"],\n",
    "                  d[\"AU200809_23U\"],d[\"AU200910_07U\"],d[\"AU200910_09U\"],d[\"AU200910_11U\"],\n",
    "                  d[\"AU201011_10U\"],d[\"AU201011_11U\"],d[\"AU201011_14U\"],d[\"AU201011_17U\"],\n",
    "                  d[\"AU201112_04U\"],d[\"AU201112_12U\"],d[\"AU201213_03U\"],d[\"AU201213_13U\"],\n",
    "                  d[\"AU201213_14U\"],d[\"AU201213_18U\"],d[\"AU201314_01U\"],d[\"AU201314_07U\"],\n",
    "                  d[\"AU201314_10U\"],d[\"AU201314_15U\"],d[\"AU201415_13U\"],d[\"AU201415_14U\"],\n",
    "                  d[\"AU201415_17U\"],d[\"AU201415_24U\"],d[\"AU201516_10U\"],d[\"AU201617_19U\"],\n",
    "                  d[\"AU201617_24U\"],d[\"AU201718_20U\"],d[\"AU201718_22U\"],d[\"AU201718_24U\"],\n",
    "                  d[\"AU201819_04U\"],d[\"AU201819_07U\"],d[\"AU201819_14U\"],d[\"AU201819_20U\"],\n",
    "                  d[\"AU201819_26U\"],d[\"AU201920_06U\"],d[\"AU201920_07U\"],d[\"AU201920_10U\"],\n",
    "                  d[\"AU201920_12U\"],d[\"AU202021_09U\"],d[\"AU202021_11U\"],d[\"AU202021_17U\"],\n",
    "                  d[\"AU202122_07U\"],d[\"AU202122_08U\"],d[\"AU202122_10U\"],d[\"AU202122_18U\"],\n",
    "                  d[\"AU202223_14U\"],d[\"AU202324_02U\"],d[\"AU202324_05U\"],d[\"AU202324_09U\"],\n",
    "                  d[\"AU202324_13U\"])\n",
    "df_east = [AU200001_06U,AU200203_01U,AU200203_06U,AU200304_02U,AU200304_04U,AU200304_09U,\n",
    "           AU200405_05U,AU200405_07U,AU200405_10U,AU200506_08U,AU200506_10U,AU200506_15U,\n",
    "           AU200506_17U,AU200506_22U,AU200607_05U,AU200607_16U,AU200708_03U,AU200708_08U,\n",
    "           AU200809_06U,AU200809_17U,AU200809_23U,AU200910_07U,AU200910_09U,AU200910_11U,\n",
    "           AU201011_10U,AU201011_11U,AU201011_14U,AU201011_17U,AU201112_04U,AU201112_12U,\n",
    "           AU201213_03U,AU201213_13U,AU201213_14U,AU201213_18U,AU201314_01U,AU201314_07U,\n",
    "           AU201314_10U,AU201314_15U,AU201415_13U,AU201415_14U,AU201415_17U,AU201415_24U,\n",
    "           AU201516_10U,AU201617_19U,AU201617_24U,AU201718_20U,AU201718_22U,AU201718_24U,\n",
    "           AU201819_04U,AU201819_07U,AU201819_14U,AU201819_20U,AU201819_26U,AU201920_06U,\n",
    "           AU201920_07U,AU201920_10U,AU201920_12U,AU202021_09U,AU202021_11U,AU202021_17U,\n",
    "           AU202122_07U,AU202122_08U,AU202122_10U,AU202122_18U,AU202223_14U,AU202324_02U,\n",
    "           AU202324_05U,AU202324_09U,AU202324_13U]\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4e120",
   "metadata": {},
   "source": [
    "### DATA MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03cbe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "def generate_torch_kwargs():\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # run torch models on CPU, and disable progress bars for all model stages except training.\n",
    "    return {\n",
    "        \"pl_trainer_kwargs\": {\n",
    "            \"accelerator\": \"cpu\",\n",
    "            \"callbacks\": [TFMProgressBar(enable_train_bar_only=True)],\n",
    "        }\n",
    "    }\n",
    "\n",
    "def modelfitting(tscols=['LAT',\n",
    "                         'LON',\n",
    "                         'CENTRAL_PRES',\n",
    "                         'ENV_PRES',\n",
    "                         'MN_RADIUS_GF_SECNE',\n",
    "                         'MN_RADIUS_GF_SECSE',\n",
    "                         'MN_RADIUS_GF_SECSW',\n",
    "                         'MN_RADIUS_GF_SECNW'],\n",
    "                 df_list_of_cyclones=df_all,\n",
    "                 covariates=None):\n",
    "    \"\"\"\n",
    "    N-BEATS model fitting.\n",
    "    Take a name list of target features tscols.\n",
    "    Default tscols = ['LAT',\n",
    "          'LON',\n",
    "          'CENTRAL_PRES',\n",
    "          'ENV_PRES',\n",
    "          'MN_RADIUS_GF_SECNE',\n",
    "          'MN_RADIUS_GF_SECSE',\n",
    "          'MN_RADIUS_GF_SECSW',\n",
    "          'MN_RADIUS_GF_SECNW']\n",
    "    Take a df list of validating cyclones df_list_of_cyclones, default is df_all.\n",
    "    Return a dataframe for model evaluation report.\n",
    "    Return a dataframe for geoplot if 'LON' and 'LAT' are both in target features.\n",
    "    \"\"\"\n",
    "\n",
    "    # for reproducibility\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Storage for evaluation report\n",
    "    bt_act_df_list = []\n",
    "    bt_fc_df_list = []\n",
    "    sc_bt_act_df_list = []\n",
    "    sc_bt_fc_df_list = []\n",
    "    bt_error_list = []\n",
    "\n",
    "    # Flag used to monitor whether previous tracking data has been stored\n",
    "    flag_geo_plot_data = False\n",
    "\n",
    "    # Temporary variable to limit number of cyclones used for validation\n",
    "    max_cyclones = 5\n",
    "    cyclones_so_far = 1\n",
    "\n",
    "    for cyclone in df_list_of_cyclones:\n",
    "        cyclone_name = str(cyclone.iat[0, 1])\n",
    "        print(\"Length = \" + str(len(cyclone)))\n",
    "        print(\"Cyclone number = \" + str(cyclones_so_far))\n",
    "        if cyclones_so_far > max_cyclones:\n",
    "            break\n",
    "        print(\"Cyclone ID: \" + str(cyclone.iat[0, 1]))\n",
    "        # Check length of this cyclone data\n",
    "        if len(cyclone) > 13:\n",
    "            cycdf = d.get(cyclone_name)\n",
    "\n",
    "            # Validation Target series\n",
    "            validation_ts = TimeSeries.from_dataframe(cycdf, \"TM\", tscols)\n",
    "            # Scaling\n",
    "            validation_scaler = Scaler()\n",
    "            # Scale Validation data before splitting\n",
    "            validation_data_scaled = validation_scaler.fit_transform(validation_ts)\n",
    "            # Keep the last 8 obs for validation of prediction\n",
    "            (train_validation_data_scaled,\n",
    "             actual_validation_data_scaled) = (validation_data_scaled[:-8],\n",
    "                                               validation_data_scaled[-8:])\n",
    "\n",
    "            # Create temporary copy of dictionary\n",
    "            dd = {key: value[:] for key, value in d.items()}\n",
    "            # Remove validation cyclone from temp dictionary\n",
    "            dd.pop(cyclone.iat[0, 1])\n",
    "\n",
    "            # Only add cyclones with sufficient number of observations for model to work\n",
    "            cyclones = []\n",
    "            for ea in dd:\n",
    "                this_cyclone = ea\n",
    "                if len(dd.get(this_cyclone)) > 11:\n",
    "                    result = dd.get(this_cyclone)\n",
    "                    cyclones.append(result)\n",
    "\n",
    "            # Handle target series\n",
    "\n",
    "            # Convert all target series in list to Time Series objects\n",
    "            ts_cyclones = []\n",
    "            for ea in cyclones:\n",
    "                result = TimeSeries.from_dataframe(ea, \"TM\", tscols)\n",
    "                ts_cyclones.append(result)\n",
    "\n",
    "            # Scale all series in list\n",
    "            sc_ts_cyclone_list = []\n",
    "            for ea in ts_cyclones:\n",
    "                new_scaler = Scaler()\n",
    "                result = new_scaler.fit_transform(ea)\n",
    "                sc_ts_cyclone_list.append(result)\n",
    "\n",
    "            # Handle covariates\n",
    "            if covariates is not None:\n",
    "\n",
    "                # Validation Covariate series\n",
    "                val_cov_ts = TimeSeries.from_dataframe(cycdf, \"TM\", covariates)\n",
    "                val_cov_scaler = Scaler()\n",
    "                # Scale covariate series\n",
    "                val_cov_scaled = val_cov_scaler.fit_transform(val_cov_ts)\n",
    "\n",
    "                # Training covariate series\n",
    "\n",
    "                # Convert all covariate series in list to Time Series objects\n",
    "                past_cov = []\n",
    "                for ea in cyclones:\n",
    "                    result = TimeSeries.from_dataframe(ea, \"TM\", covariates)\n",
    "                    past_cov.append(result)\n",
    "\n",
    "                # Scale all series in list\n",
    "                sc_past_cov = []\n",
    "                for ea in past_cov:\n",
    "                    new_scaler = Scaler()\n",
    "                    result = new_scaler.fit_transform(ea)\n",
    "                    sc_past_cov.append(result)\n",
    "            else:\n",
    "                sc_past_cov = None\n",
    "                val_cov_scaled = None\n",
    "\n",
    "            # Train model\n",
    "            model_NBEATS = NBEATSModel(\n",
    "                input_chunk_length=6,\n",
    "                output_chunk_length=4,\n",
    "                n_epochs=30,\n",
    "                random_state=0,\n",
    "                **generate_torch_kwargs())\n",
    "\n",
    "            model_NBEATS.fit(sc_ts_cyclone_list,\n",
    "                             past_covariates=sc_past_cov)\n",
    "\n",
    "            # BACKTEST\n",
    "\n",
    "            # HISTORICAL FORECASTS for SCALED DATA UNIVARIATE COMPONENT\n",
    "            sc_bt_fc = model_NBEATS.historical_forecasts(validation_data_scaled,\n",
    "                                                         forecast_horizon=8,\n",
    "                                                         retrain=False,\n",
    "                                                         last_points_only=False,\n",
    "                                                         past_covariates=val_cov_scaled)\n",
    "\n",
    "            # Create list of forecasts\n",
    "            sc_bt_fc_list = [np.zeros(len(tscols))]\n",
    "            for i in range(0, len(sc_bt_fc)):\n",
    "                a = sc_bt_fc[i].values()\n",
    "                sc_bt_fc_list = np.concatenate([sc_bt_fc_list, a])\n",
    "            sc_bt_fc_list = sc_bt_fc_list[1:]\n",
    "\n",
    "            # Create list of run no and cyc id\n",
    "            run_no = [cyclones_so_far] * 8 * len(sc_bt_fc)\n",
    "            cyc_id = [cyclone_name] * 8 * len(sc_bt_fc)\n",
    "\n",
    "            # Create list of corresponding actual values\n",
    "            sc_bt_act_list = [np.zeros(len(tscols))]\n",
    "            for i in range(0, len(sc_bt_fc)):\n",
    "                start = 6 + i\n",
    "                end = 6 + i + 8\n",
    "                a = validation_data_scaled[start:end].values()\n",
    "                sc_bt_act_list = np.concatenate([sc_bt_act_list, a])\n",
    "            sc_bt_act_list = sc_bt_act_list[1:]\n",
    "\n",
    "            # Create list of index of forecasts\n",
    "            sc_obs = np.array(range(1, 9))\n",
    "            for i in range(len(sc_bt_fc) - 1):\n",
    "                b = np.array(range(1, 9))\n",
    "                sc_obs = np.concatenate([sc_obs, b])\n",
    "\n",
    "            # Create df for actual for output\n",
    "            sc_bt_act_df = pd.DataFrame(index=range(len(sc_bt_act_list)),\n",
    "                                        data=sc_bt_act_list,\n",
    "                                        columns=tscols)\n",
    "            sc_bt_act_df['Obs'] = sc_obs\n",
    "            sc_bt_act_df['run_no'] = run_no\n",
    "            sc_bt_act_df['cyc_id'] = cyc_id\n",
    "\n",
    "            # Create df for forecast for output\n",
    "            sc_bt_fc_df = pd.DataFrame(index=range(len(sc_bt_fc_list)),\n",
    "                                       data=sc_bt_fc_list,\n",
    "                                       columns=tscols)\n",
    "            sc_bt_fc_df['Obs'] = sc_obs\n",
    "            sc_bt_fc_df['run_no'] = run_no\n",
    "            sc_bt_fc_df['cyc_id'] = cyc_id\n",
    "\n",
    "            # HISTORICAL FORECASTS for ORIGINAL DATA UNIVARIATE COMPONENT\n",
    "\n",
    "            # Inverse scaling scaled historical forecasts\n",
    "            bt_fc = []\n",
    "            for ts in sc_bt_fc:\n",
    "                inv_sc_bt = validation_scaler.inverse_transform(ts)\n",
    "                bt_fc.append(inv_sc_bt)\n",
    "\n",
    "                # Create list of forecasts\n",
    "            bt_fc_list = [np.zeros(len(tscols))]\n",
    "            for i in range(0, len(bt_fc)):\n",
    "                a = bt_fc[i].values()\n",
    "                bt_fc_list = np.concatenate([bt_fc_list, a])\n",
    "            bt_fc_list = bt_fc_list[1:]\n",
    "\n",
    "            # Create list of corresponding actual values\n",
    "            bt_act_list = [np.zeros(len(tscols))]\n",
    "            for i in range(0, len(bt_fc)):\n",
    "                start = 6 + i\n",
    "                end = 6 + i + 8\n",
    "                a = validation_ts[start:end].values()\n",
    "                bt_act_list = np.concatenate([bt_act_list, a])\n",
    "            bt_act_list = bt_act_list[1:]\n",
    "\n",
    "            # Create list of index of forecasts\n",
    "            obs = np.array(range(1, 9))\n",
    "            for i in range(len(bt_fc) - 1):\n",
    "                b = np.array(range(1, 9))\n",
    "                obs = np.concatenate([obs, b])\n",
    "\n",
    "            # Create df for actual for output\n",
    "            bt_act_df = pd.DataFrame(index=range(len(bt_act_list)),\n",
    "                                     data=bt_act_list,\n",
    "                                     columns=tscols)\n",
    "            bt_act_df['Obs'] = obs\n",
    "            bt_act_df['run_no'] = run_no\n",
    "            bt_act_df['cyc_id'] = cyc_id\n",
    "\n",
    "            # Create df for forecast for output\n",
    "            bt_fc_df = pd.DataFrame(index=range(len(bt_fc_list)),\n",
    "                                    data=bt_fc_list,\n",
    "                                    columns=tscols)\n",
    "            bt_fc_df['Obs'] = obs\n",
    "            bt_fc_df['run_no'] = run_no\n",
    "            bt_fc_df['cyc_id'] = cyc_id\n",
    "\n",
    "            # BACK TEST FOR MODEL GENERAL PERFORMANCE\n",
    "            bt_error = model_NBEATS.backtest(validation_data_scaled,\n",
    "                                             metric=[mae, rmse, mse],\n",
    "                                             forecast_horizon=8,\n",
    "                                             retrain=False,\n",
    "                                             past_covariates=val_cov_scaled)\n",
    "            bt_dict = {'run_no': cyclones_so_far,\n",
    "                       'cyc_id': cyclone_name,\n",
    "                       'MAE': bt_error[0],\n",
    "                       'RMSE': bt_error[1],\n",
    "                       'MSE': bt_error[2]}\n",
    "            bt_error_df = pd.DataFrame(bt_dict, index=[cyclones_so_far - 1])\n",
    "\n",
    "            # Append to storage list\n",
    "            bt_act_df_list.append(bt_act_df)\n",
    "            bt_fc_df_list.append(bt_fc_df)\n",
    "            sc_bt_act_df_list.append(sc_bt_act_df)\n",
    "            sc_bt_fc_df_list.append(sc_bt_fc_df)\n",
    "            bt_error_list.append(bt_error_df)\n",
    "\n",
    "            # Run prediction train_validation_data_scaled for plotting\n",
    "            sc_forecast_NBEATS = model_NBEATS.predict(n=8,\n",
    "                                                      series=train_validation_data_scaled,\n",
    "                                                      past_covariates=val_cov_scaled)\n",
    "\n",
    "            # Reverse scaling (at n=8 for plotting)\n",
    "            forecast_NBEATS = validation_scaler.inverse_transform(sc_forecast_NBEATS)\n",
    "            actual_validation = validation_scaler.inverse_transform(actual_validation_data_scaled)\n",
    "\n",
    "            if 'LON' in tscols and 'LAT' in tscols:\n",
    "                for idx, component in enumerate(tscols):\n",
    "                    if component == 'LAT':\n",
    "                        variable1_pred = forecast_NBEATS.univariate_component(idx)\n",
    "                        variable1_act = actual_validation.univariate_component(idx)\n",
    "                        variable1_past = validation_ts[:-8].univariate_component(idx)\n",
    "\n",
    "                    if component == 'LON':\n",
    "                        variable2_pred = forecast_NBEATS.univariate_component(idx)\n",
    "                        variable2_act = actual_validation.univariate_component(idx)\n",
    "                        variable2_past = validation_ts[:-8].univariate_component(idx)\n",
    "\n",
    "                variable1_pred_values = variable1_pred.values()\n",
    "                variable2_pred_values = variable2_pred.values()\n",
    "                variable1_act_values = variable1_act.values()\n",
    "                variable2_act_values = variable2_act.values()\n",
    "                variable1_past_values = variable1_past.values()\n",
    "                variable2_past_values = variable2_past.values()\n",
    "\n",
    "                # Concatenate values of the past, actual & prediction\n",
    "                #  data to make plots continuous\n",
    "                variable1_existing_values = np.concatenate([variable1_past_values,\n",
    "                                                            variable1_act_values])\n",
    "                variable2_existing_values = np.concatenate([variable2_past_values,\n",
    "                                                            variable2_act_values])\n",
    "                variable1_pred_values = np.concatenate([variable1_past_values,\n",
    "                                                        variable1_pred_values])\n",
    "                variable2_pred_values = np.concatenate([variable2_past_values,\n",
    "                                                        variable2_pred_values])\n",
    "\n",
    "                # Create datetime variables\n",
    "                TM_act = variable1_act.time_index\n",
    "                TM_past = variable1_past.time_index\n",
    "                TM_exist = np.concatenate([TM_past, TM_act])\n",
    "\n",
    "                # Create dataframe for plotting with geopandas\n",
    "                geo_plot_all = pd.DataFrame(index=range(len(variable1_existing_values)))\n",
    "                geo_plot_all['LAT'] = variable1_existing_values\n",
    "                geo_plot_all['LON'] = variable2_existing_values\n",
    "                geo_plot_all['TM'] = TM_exist\n",
    "                geo_plot_all['DISTURBANCE_ID'] = str(cyclone_name + \"-Real\")\n",
    "                geo_plot_pred = pd.DataFrame(index=range(len(variable1_pred_values)))\n",
    "                geo_plot_pred['LAT'] = variable1_pred_values\n",
    "                geo_plot_pred['LON'] = variable2_pred_values\n",
    "                geo_plot_pred['TM'] = TM_exist\n",
    "                geo_plot_pred['DISTURBANCE_ID'] = str(cyclone_name + \"-Predicted\")\n",
    "                if not flag_geo_plot_data:\n",
    "                    geo_plot_data = pd.concat([geo_plot_pred,\n",
    "                                               geo_plot_all],\n",
    "                                              ignore_index=True)\n",
    "                    flag_geo_plot_data = True\n",
    "                else:\n",
    "                    geo_plot_data = pd.concat([geo_plot_pred,\n",
    "                                               geo_plot_all,\n",
    "                                               geo_plot_data],\n",
    "                                              ignore_index=True)\n",
    "\n",
    "            cyclones_so_far += 1\n",
    "    bt_out = [bt_act_df_list,\n",
    "              bt_fc_df_list,\n",
    "              sc_bt_act_df_list,\n",
    "              sc_bt_fc_df_list,\n",
    "              bt_error_list]\n",
    "    for m, n in enumerate(bt_out):\n",
    "        bt_out[m] = pd.concat(n)\n",
    "    if 'LON' in tscols and 'LAT' in tscols:\n",
    "        return [bt_out,\n",
    "                tscols,\n",
    "                len(cyclones),\n",
    "                cyclones_so_far - 1,\n",
    "                covariates,\n",
    "                geo_plot_data]\n",
    "    else:\n",
    "        return [bt_out,\n",
    "                tscols,\n",
    "                len(cyclones),\n",
    "                cyclones_so_far - 1,\n",
    "                covariates]\n",
    "\n",
    "# Self-defined function for Geoplotting\n",
    "def geoplot(geo_plot_data, plotname='trajectory_plot.html'):\n",
    "    \"\"\"\n",
    "    Mapping cyclone path.\n",
    "    Take in a data frame containing data for plotting.\n",
    "    Allow input of plotname, default plotname = 'trajectory_plot.html'.\n",
    "    \"\"\"\n",
    "    # Set CRS to WGS84 coordinate system\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "    # Convert Longitude and Latitude to Points\n",
    "    geometry = [Point(xy) for xy in zip(geo_plot_data['LON'], geo_plot_data['LAT'])]\n",
    "    geo_data = gpd.GeoDataFrame(geo_plot_data, geometry=geometry)\n",
    "\n",
    "    # Create trajectories\n",
    "    geo_data['Time'] = pd.to_datetime(geo_data['TM'],\n",
    "                                      format='%d/%m/%Y %H:%M',\n",
    "                                      errors='coerce')\n",
    "    geo_data = geo_data.set_index('Time')\n",
    "    # Specify minimum length of trajectories\n",
    "    minimum_length = 0\n",
    "\n",
    "    # Create Trajectory Collection\n",
    "    traj_collection = mpd.TrajectoryCollection(geo_data,\n",
    "                                               'DISTURBANCE_ID',\n",
    "                                               min_length=minimum_length)\n",
    "\n",
    "    # Create DataFrame to store trajectory coordinates\n",
    "    trajectory_df = pd.DataFrame(columns=['DISTURBANCE_ID', 'LON', 'LAT'])\n",
    "\n",
    "    # For each trajectory in the TrajectoryCollection\n",
    "    #  Extract coordinates and add to the trajectory DataFrame\n",
    "    for traj in traj_collection:\n",
    "        coords = traj.to_linestring().coords[:]\n",
    "        traj_id = traj.id\n",
    "        for LON, LAT in coords:\n",
    "            trajectory_df = pd.concat([trajectory_df,\n",
    "                                       pd.DataFrame([{'DISTURBANCE_ID': traj_id,\n",
    "                                                      'LON': LON,\n",
    "                                                      'LAT': LAT}])],\n",
    "                                      ignore_index=True)\n",
    "    # Create plot\n",
    "    fig = px.line_mapbox(trajectory_df,\n",
    "                         lat='LAT', lon='LON',\n",
    "                         color='DISTURBANCE_ID',\n",
    "                         hover_name='DISTURBANCE_ID',\n",
    "                         zoom=4)\n",
    "    # Set layout options for map\n",
    "    fig.update_layout(mapbox_style=\"carto-positron\",\n",
    "                      mapbox_zoom=4,\n",
    "                      mapbox_center={\"lat\": trajectory_df['LAT'].mean(),\n",
    "                                     \"lon\": trajectory_df['LON'].mean()})\n",
    "    # Write html file containing interactive plot\n",
    "    fig.write_html(plotname)\n",
    "    print(f\"A geoplot named '{plotname}' was created.\")\n",
    "\n",
    "def unicom_rep(scaled=False):\n",
    "    \"\"\"\n",
    "    Univariate component report.\n",
    "    Default scaled = False to report on original series.\n",
    "    Change scaled = True to report on scaled series.\n",
    "    \"\"\"\n",
    "    tscols = target_col\n",
    "\n",
    "    print('\\n')\n",
    "    print('#' * 50)\n",
    "    print('UNIVARIATE COMPONENT REPORT\\n')\n",
    "    print('#' * 50)\n",
    "    if scaled == True:\n",
    "        bt_act_df = sc_bt_act\n",
    "        bt_fc_df = sc_bt_fc\n",
    "        print('Table of Scaled Error for Each Component.')\n",
    "        print('\\nError Scores: scaled between 0 to 1')\n",
    "    else:\n",
    "        bt_act_df = bt_act\n",
    "        bt_fc_df = bt_fc\n",
    "        print('Table of Error Scores for Each Component.')\n",
    "    print('#' * 50)\n",
    "    print('\\n')\n",
    "    for cycid in bt_fc_df['cyc_id'].unique():\n",
    "        for comp in tscols:\n",
    "            print('\\nCyclone ID: ', cycid)\n",
    "            print('Cyclone Name:', compare_dict[cycid])\n",
    "            print('Component: ', comp)\n",
    "\n",
    "            bt_rmse = []\n",
    "            bt_mse = []\n",
    "            bt_mae = []\n",
    "            bt_obs = []\n",
    "            bt_len = []\n",
    "            for g in bt_fc_df['Obs'].unique():\n",
    "                mask_act = bt_act_df.loc[(bt_act_df['Obs'] == g) &\n",
    "                                         (bt_act_df['cyc_id'] == cycid), comp]\n",
    "                mask_fc = bt_fc_df.loc[(bt_fc_df['Obs'] == g) &\n",
    "                                       (bt_fc_df['cyc_id'] == cycid), comp]\n",
    "\n",
    "                rmse = mean_squared_error(mask_act, mask_fc, squared=False)\n",
    "                mse = mean_squared_error(mask_act, mask_fc, squared=True)\n",
    "                mae = mean_absolute_error(mask_act, mask_fc)\n",
    "\n",
    "                bt_rmse.append(rmse)\n",
    "                bt_mse.append(mse)\n",
    "                bt_mae.append(mae)\n",
    "                bt_obs.append(g)\n",
    "                bt_len.append(len(mask_fc))\n",
    "\n",
    "            bt_rep = pd.DataFrame(list(zip(bt_len, bt_mae, bt_rmse, bt_mse)),\n",
    "                                  columns=['BT_sets', 'MAE', 'RMSE', 'MSE'])\n",
    "\n",
    "            bt_rep = bt_rep.transpose().round(2)\n",
    "            headers = ['6h\\n(n=1)', '12h\\n(n=2)', '18h\\n(n=3)', '24h\\n(n=4)',\n",
    "                       '30h\\n(n=5)', '36h\\n(n=6)', '42h\\n(n=7)', '48h\\n(n=8)']\n",
    "\n",
    "            if scaled == False:\n",
    "                if comp in ['LAT', 'LON']:\n",
    "                    nm = bt_rep.copy()\n",
    "                    nm.loc[['MAE', 'RMSE', 'MSE']] = nm.loc[['MAE', 'RMSE', 'MSE']] * 60\n",
    "                    print('\\nError Units: nautical miles (nm)')\n",
    "                    print(tabulate(nm, headers=headers, tablefmt='psql'))\n",
    "                    print('\\nError Units: decimal degrees')\n",
    "\n",
    "                elif comp in ['MN_RADIUS_GF_SECNE',\n",
    "                              'MN_RADIUS_GF_SECSE',\n",
    "                              'MN_RADIUS_GF_SECSW',\n",
    "                              'MN_RADIUS_GF_SECNW']:\n",
    "                    print('\\nError Units: kilometres')\n",
    "\n",
    "                elif comp in ['ENV_PRES', 'CENTRAL_PRES']:\n",
    "                    print('\\nError Units: hectopascals')\n",
    "\n",
    "            elif scaled == True:\n",
    "                print('\\nError Units: scaled between 0 and 1')\n",
    "\n",
    "            print(tabulate(bt_rep, headers=headers, tablefmt='psql'))\n",
    "            print('\\n')\n",
    "            print('#' * 50)\n",
    "            print('\\n')\n",
    "\n",
    "def eval_rep(scaled=True):\n",
    "    \"\"\"\n",
    "    Generate MODEL EVALUATION REPORT FOR ALL COMPONENTS\n",
    "    \"\"\"\n",
    "    tscols = target_col\n",
    "    traincycount = traincycount_fit\n",
    "    valcycount = valcycount_fit\n",
    "    bt_error = bt_error_fit\n",
    "    covariates = covariates_fit\n",
    "\n",
    "    print('#' * 50)\n",
    "    print('MODEL EVALUATION REPORT FOR ALL COMPONENTS')\n",
    "    print('#' * 50)\n",
    "    print('\\nNumber of fitted target features: ' + str(len(tscols)))\n",
    "    print('List of fitted target features: ')\n",
    "    print([i for i in tscols])\n",
    "    print('List of fitted covariate features:')\n",
    "    if covariates is not None:\n",
    "        print([i for i in covariates])\n",
    "    else:\n",
    "        print('No covariate features.')\n",
    "    print('Number of training cyclones used for each run: ' + str(traincycount))\n",
    "    print('Number of validating cyclones used for each run: 1')\n",
    "    print('Number of validating runs: ' + str(valcycount))\n",
    "    print('Prediction at step n ahead: n= ', [i for i in range(1, 9)])\n",
    "\n",
    "    print('\\n' + '#' * 50)\n",
    "\n",
    "    if scaled == True:\n",
    "        bt_act_df = sc_bt_act\n",
    "        bt_fc_df = sc_bt_fc\n",
    "        print('Table of Scaled Error for Model Performance.')\n",
    "    else:\n",
    "        bt_act_df = bt_act\n",
    "        bt_fc_df = bt_fc\n",
    "        print('Table of Error Scores for Model Performance.')\n",
    "    print('#' * 50)\n",
    "    print('\\n')\n",
    "\n",
    "    for cycid in bt_fc_df['cyc_id'].unique():\n",
    "        print('\\nCyclone ID: ', cycid)\n",
    "        print('Cyclone Name:', compare_dict[cycid])\n",
    "        bt_rmse = []\n",
    "        bt_mse = []\n",
    "        bt_mae = []\n",
    "        bt_obs = []\n",
    "        bt_len = []\n",
    "        for g in bt_fc_df['Obs'].unique():\n",
    "            mask_act = bt_act_df.loc[(bt_act_df['Obs'] == g) &\n",
    "                                     (bt_act_df['cyc_id'] == cycid), tscols]\n",
    "            mask_fc = bt_fc_df.loc[(bt_fc_df['Obs'] == g) &\n",
    "                                   (bt_fc_df['cyc_id'] == cycid), tscols]\n",
    "\n",
    "            rmse = mean_squared_error(mask_act, mask_fc, squared=False)\n",
    "            mse = mean_squared_error(mask_act, mask_fc, squared=True)\n",
    "            mae = mean_absolute_error(mask_act, mask_fc)\n",
    "\n",
    "            bt_rmse.append(rmse)\n",
    "            bt_mse.append(mse)\n",
    "            bt_mae.append(mae)\n",
    "            bt_obs.append(g)\n",
    "            bt_len.append(len(mask_fc))\n",
    "\n",
    "        bt_rep = pd.DataFrame(list(zip(bt_len, bt_mae, bt_rmse, bt_mse)),\n",
    "                              columns=['BT_sets', 'MAE', 'RMSE', 'MSE'])\n",
    "\n",
    "        # Add Backtest error output\n",
    "        mask_bt_error = bt_error.loc[bt_error['cyc_id'] == cycid]\n",
    "        a = {'BT_sets': bt_len[0],\n",
    "             'MAE': mask_bt_error['MAE'],\n",
    "             'RMSE': mask_bt_error['RMSE'],\n",
    "             'MSE': mask_bt_error['MSE']}\n",
    "        bt_rep = pd.concat([bt_rep, pd.DataFrame(a)], ignore_index=True)\n",
    "\n",
    "        bt_rep = bt_rep.round(2).transpose()\n",
    "\n",
    "        bt_rep.columns = ['6h', '12h', '18h', '24h', '30h', '36h', '42h', '48h', 'Overall']\n",
    "        bt_rep = bt_rep[['Overall'] + [x for x in bt_rep.columns if x != 'Overall']]\n",
    "\n",
    "        headers = ['Overall', '6h\\n(n=1)', '12h\\n(n=2)', '18h\\n(n=3)', '24h\\n(n=4)',\n",
    "                   '30h\\n(n=5)', '36h\\n(n=6)', '42h\\n(n=7)', '48h\\n(n=8)']\n",
    "\n",
    "        if scaled == True:\n",
    "            print('Error Units: scaled between 0 to 1')\n",
    "\n",
    "        print(tabulate(bt_rep, headers=headers, tablefmt='psql'))\n",
    "        print('\\n')\n",
    "        print('#' * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ed53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_rep(repname='sumrep.csv'):\n",
    "    \"\"\"\n",
    "    SUMMARY REPORT FOR MODEL EVALUATION\n",
    "    Output csv file mean errors of all validating cyclones\n",
    "    \"\"\"\n",
    "    tscols = target_col\n",
    "    traincycount = traincycount_fit\n",
    "    valcycount = valcycount_fit\n",
    "    bt_error = bt_error_fit\n",
    "    covariates = covariates_fit\n",
    "    bt_act_df = sc_bt_act\n",
    "    bt_fc_df = sc_bt_fc\n",
    "    \n",
    "    print('#'*50)\n",
    "    print('SUMMARY REPORT FOR MODEL EVALUATION')\n",
    "    print('#'*50)\n",
    "    print('\\nNumber of fitted target features: '+ str(len(tscols)))\n",
    "    print('List of fitted target features: ')\n",
    "    print([i for i in tscols])\n",
    "    print('List of fitted covariate features:')\n",
    "    if covariates is not None:\n",
    "        print([i for i in covariates])\n",
    "    else:\n",
    "        print('No covariate features.')\n",
    "    print('\\nValidating Cyclone ID(s): ')\n",
    "    print([cycid for cycid in bt_fc_df['cyc_id'].unique()])\n",
    "    print('Validating Cyclone Name(s):')\n",
    "    print([compare_dict[cycid] for cycid in bt_fc_df['cyc_id'].unique()])\n",
    "    print('\\nNumber of training cyclones used for each run: ' + str(traincycount))\n",
    "    print('Validating method: leave one out')\n",
    "    print('Number of validating runs: ' + str(valcycount))\n",
    "    print('Prediction at step n ahead: n= ', [i for i in range(1,9)])\n",
    "    print('\\n'+'#'*50)\n",
    "    print(f'Summarry Table of Scaled Error after {valcycount} Validating Runs.')\n",
    "    print('#'*50)    \n",
    "    print('\\n')\n",
    "    \n",
    "\n",
    "    bt_rmse = []\n",
    "    bt_mse = []\n",
    "    bt_mae = []\n",
    "    bt_obs = []\n",
    "    bt_len = []\n",
    "    \n",
    "    # Calculate error scores\n",
    "    for g in bt_fc_df['Obs'].unique():\n",
    "\n",
    "        mask_act = bt_act_df.loc[(bt_act_df['Obs']==g),tscols]\n",
    "        mask_fc = bt_fc_df.loc[(bt_fc_df['Obs']==g),tscols]\n",
    "\n",
    "        rmse = mean_squared_error(mask_act, mask_fc, squared=False)\n",
    "        mse = mean_squared_error(mask_act, mask_fc, squared=True)\n",
    "        mae = mean_absolute_error(mask_act, mask_fc)\n",
    "\n",
    "        bt_rmse.append(rmse)\n",
    "        bt_mse.append(mse)\n",
    "        bt_mae.append(mae)\n",
    "        bt_obs.append(g)\n",
    "        bt_len.append(len(mask_fc))\n",
    "\n",
    "\n",
    "    bt_rep = pd.DataFrame(list(zip(bt_len,bt_mae,bt_rmse,bt_mse)),columns=['BT_sets','MAE', 'RMSE','MSE'])\n",
    "    \n",
    "#     # Save to csv before transpose\n",
    "#     bt_rep.to_csv(f'{repname}.csv',index=False)\n",
    "    \n",
    "    bt_rep=bt_rep.round(2).transpose()\n",
    "\n",
    "    bt_rep.columns = ['6h','12h','18h','24h','30h','36h','42h','48h']\n",
    "\n",
    "    headers = ['6h\\n(n=1)', '12h\\n(n=2)', '18h\\n(n=3)', '24h\\n(n=4)', \n",
    "             '30h\\n(n=5)', '36h\\n(n=6)', '42h\\n(n=7)', '48h\\n(n=8)']\n",
    "\n",
    "    print('Error Units: scaled between 0 to 1')   \n",
    "    print(tabulate(bt_rep, headers = headers, tablefmt = 'psql'))\n",
    "    print('\\n')\n",
    "    print('#'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f696e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_set_errors (modelname='fit'):\n",
    "    bt_rmse = []\n",
    "    bt_mse = []\n",
    "    bt_mae = []\n",
    "    bt_set_no = []\n",
    "\n",
    "\n",
    "    tscols = target_col\n",
    "    bt_act_df = sc_bt_act\n",
    "    bt_fc_df = sc_bt_fc\n",
    "\n",
    "    \n",
    "    bt_count = int(len(bt_fc_df)/(bt_fc_df['Obs'].nunique()))\n",
    "    \n",
    "    bt_id = []\n",
    "    for i in range(1,bt_count+1):\n",
    "        a = [i]*bt_fc_df['Obs'].nunique()\n",
    "        bt_id = bt_id + a\n",
    "\n",
    "    bt_fc_df['bt_id'] = bt_id\n",
    "    bt_act_df['bt_id'] = bt_id\n",
    "\n",
    "    # Calculate error scores for each back test set\n",
    "    for g in bt_fc_df['bt_id'].unique():\n",
    "        mask_act = bt_act_df.loc[(bt_act_df['bt_id']==g),tscols]\n",
    "        mask_fc = bt_fc_df.loc[(bt_fc_df['bt_id']==g),tscols]\n",
    "        rmse = mean_squared_error(mask_act, mask_fc, squared=False)\n",
    "        mse = mean_squared_error(mask_act, mask_fc, squared=True)\n",
    "        mae = mean_absolute_error(mask_act, mask_fc)\n",
    "        bt_rmse.append(rmse)\n",
    "        bt_mse.append(mse)\n",
    "        bt_mae.append(mae)\n",
    "        bt_set_no.append(g)\n",
    "\n",
    "    bt_set_error = pd.DataFrame(list(zip(bt_set_no,bt_mae,bt_rmse,bt_mse)),columns=['BT_ID','MAE', 'RMSE','MSE'])\n",
    "    bt_set_error.to_csv(f'bt_set_error_{modelname}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a4ab097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_csv (modelname='fit'):\n",
    "    \"\"\"\n",
    "    save backtest outputs to 4 x csv files.\n",
    "    \"\"\"\n",
    "    bt_act.to_csv(f'bt_act_{modelname}.csv',index=False)\n",
    "    bt_fc.to_csv(f'bt_fc_{modelname}.csv',index=False)\n",
    "    sc_bt_act.to_csv(f'sc_bt_act_{modelname}.csv',index=False)\n",
    "    sc_bt_fc.to_csv(f'sc_bt_fc_{modelname}.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457656e",
   "metadata": {},
   "source": [
    "### Model Fitting and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19217c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name list of target features\n",
    "cycpath = ['LAT',\n",
    "          'LON']\n",
    "\n",
    "# name list of covariate features\n",
    "pres = ['CENTRAL_PRES',\n",
    "          'ENV_PRES']\n",
    "\n",
    "r34s = ['MN_RADIUS_GF_SECNE',\n",
    "        'MN_RADIUS_GF_SECSE',\n",
    "        'MN_RADIUS_GF_SECSW',\n",
    "        'MN_RADIUS_GF_SECNW']\n",
    "\n",
    "# list of comparison cyclones from BOM for validating\n",
    "compare = [AU202021_11U, AU202021_15U, AU202021_17U, AU202021_22U]\n",
    "compare_id = []\n",
    "compare_name = []\n",
    "for cyc in compare:\n",
    "    cycid = cyc['DISTURBANCE_ID'].unique().tolist()\n",
    "    compare_id += cycid\n",
    "    \n",
    "for cyc in compare:\n",
    "    name = cyc['NAME'].unique().tolist()\n",
    "    compare_name += name\n",
    "\n",
    "compare_dict = dict(zip(compare_id,compare_name))\n",
    "# full excel files compare = [AU202021_02U, AU202021_07U, AU202021_09U, AU202021_11U, AU202021_15U, AU202021_17U, AU202021_22U]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f046fdf5",
   "metadata": {},
   "source": [
    "#### 1. Modeling Cyclone Path Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1966710f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length = 14\n",
      "Cyclone number = 1\n",
      "Cyclone ID: AU202021_11U\n",
      "Epoch 29: 100%|██████████████████████████████████████████████████| 131/131 [00:16<00:00,  8.00it/s, train_loss=0.00102]\n",
      "Length = 33\n",
      "Cyclone number = 2\n",
      "Cyclone ID: AU202021_15U\n",
      "Epoch 29: 100%|██████████████████████████████████████████████████| 131/131 [00:16<00:00,  7.78it/s, train_loss=0.00195]\n",
      "Length = 18\n",
      "Cyclone number = 3\n",
      "Cyclone ID: AU202021_17U\n",
      "Epoch 29: 100%|██████████████████████████████████████████████████| 131/131 [00:16<00:00,  7.87it/s, train_loss=0.00107]\n",
      "Length = 30\n",
      "Cyclone number = 4\n",
      "Cyclone ID: AU202021_22U\n",
      "Epoch 29: 100%|██████████████████████████████████████████████████| 131/131 [00:16<00:00,  7.97it/s, train_loss=0.00181]\n"
     ]
    }
   ],
   "source": [
    "# Fitting cycpath\n",
    "path_nocov = modelfitting(tscols=cycpath,df_list_of_cyclones=compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c102299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: only need to change model name in [] in for loop.\n",
    "# Don't change names of variables on the left hand side.\n",
    "\n",
    "for fittedmodel in [path_nocov]:\n",
    "    ### DON'T CHANGE BELOW CODE\n",
    "    \n",
    "    # Assign backtest output of fitted model for reports\n",
    "    bt_act = fittedmodel[0][0]\n",
    "    bt_fc = fittedmodel[0][1]\n",
    "    sc_bt_act = fittedmodel[0][2]\n",
    "    sc_bt_fc = fittedmodel[0][3]\n",
    "    bt_error_fit = fittedmodel[0][4]\n",
    "\n",
    "    # Assign other outputs from fitted model for reports\n",
    "    target_col = fittedmodel[1]\n",
    "    traincycount_fit = fittedmodel[2]\n",
    "    valcycount_fit = fittedmodel[3]\n",
    "    covariates_fit = fittedmodel[4]\n",
    "    \n",
    "    ### DON'T CHANGE ABOVE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd0c7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save back test outputs to csv files\n",
    "bt_csv(modelname='path_nocov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64c97248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate back test set error and save to csv files\n",
    "bt_set_errors(modelname='path_nocov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e715894d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "SUMMARY REPORT FOR MODEL EVALUATION\n",
      "##################################################\n",
      "\n",
      "Number of fitted target features: 2\n",
      "List of fitted target features: \n",
      "['LAT', 'LON']\n",
      "List of fitted covariate features:\n",
      "No covariate features.\n",
      "\n",
      "Validating Cyclone ID(s): \n",
      "['AU202021_11U', 'AU202021_15U', 'AU202021_17U', 'AU202021_22U']\n",
      "Validating Cyclone Name(s):\n",
      "['Lucas', 'Marian', 'Niran', 'Seroja']\n",
      "\n",
      "Number of training cyclones used for each run: 93\n",
      "Validating method: leave one out\n",
      "Number of validating runs: 4\n",
      "Prediction at step n ahead: n=  [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "\n",
      "##################################################\n",
      "Summarry Table of Scaled Error after 4 Validating Runs.\n",
      "##################################################\n",
      "\n",
      "\n",
      "Error Units: scaled between 0 to 1\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|         |      6h |     12h |     18h |     24h |     30h |     36h |     42h |     48h |\n",
      "|         |   (n=1) |   (n=2) |   (n=3) |   (n=4) |   (n=5) |   (n=6) |   (n=7) |   (n=8) |\n",
      "|---------+---------+---------+---------+---------+---------+---------+---------+---------|\n",
      "| BT_sets |   43    |   43    |   43    |   43    |   43    |   43    |   43    |   43    |\n",
      "| MAE     |    0.03 |    0.05 |    0.07 |    0.1  |    0.14 |    0.16 |    0.2  |    0.24 |\n",
      "| RMSE    |    0.04 |    0.06 |    0.09 |    0.13 |    0.18 |    0.21 |    0.24 |    0.29 |\n",
      "| MSE     |    0    |    0    |    0.01 |    0.02 |    0.03 |    0.04 |    0.06 |    0.08 |\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "\n",
      "\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "# summary report\n",
    "summary_rep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b90b5447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A geoplot named 'path_nocov.html' was created.\n"
     ]
    }
   ],
   "source": [
    "# Geoplot for cycpath\n",
    "if len(path_nocov)==6:\n",
    "    geoplot(geo_plot_data=path_nocov[-1],plotname='path_nocov.html')\n",
    "else:\n",
    "    print('No data for geoplot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28148139",
   "metadata": {},
   "source": [
    "#### 2. Modeling Cyclone Path with Covariates 'CENTRAL_PRES',  'ENV_PRES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb7cedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length = 14\n",
      "Cyclone number = 1\n",
      "Cyclone ID: AU202021_11U\n",
      "Epoch 29: 100%|██████████████████████████████████████████████████| 131/131 [00:17<00:00,  7.58it/s, train_loss=0.00047]\n",
      "Length = 33\n",
      "Cyclone number = 2\n",
      "Cyclone ID: AU202021_15U\n",
      "Epoch 29: 100%|█████████████████████████████████████████████████| 131/131 [00:16<00:00,  7.73it/s, train_loss=0.000742]\n",
      "Length = 18\n",
      "Cyclone number = 3\n",
      "Cyclone ID: AU202021_17U\n",
      "Epoch 29: 100%|██████████████████████████████████████████████████| 131/131 [00:16<00:00,  7.82it/s, train_loss=0.00057]\n",
      "Length = 30\n",
      "Cyclone number = 4\n",
      "Cyclone ID: AU202021_22U\n",
      "Epoch 29: 100%|█████████████████████████████████████████████████| 131/131 [00:17<00:00,  7.45it/s, train_loss=0.000938]\n"
     ]
    }
   ],
   "source": [
    "# Fitting cycpath with cov 'CENTRAL_PRES', 'ENV_PRES'\n",
    "path_cov_pres = modelfitting(tscols=cycpath,df_list_of_cyclones=compare, covariates=pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c1cfcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: only need to change model name in [] in 'for loop' for reports\n",
    "# Don't change names of variables on the left hand side.\n",
    "\n",
    "for fittedmodel in [path_cov_pres]:\n",
    "    ### DON'T CHANGE BELOW CODE\n",
    "    \n",
    "    # Assign backtest output of fitted model for reports\n",
    "    bt_act = fittedmodel[0][0]\n",
    "    bt_fc = fittedmodel[0][1]\n",
    "    sc_bt_act = fittedmodel[0][2]\n",
    "    sc_bt_fc = fittedmodel[0][3]\n",
    "    bt_error_fit = fittedmodel[0][4]\n",
    "\n",
    "    # Assign other outputs from fitted model for reports\n",
    "    target_col = fittedmodel[1]\n",
    "    traincycount_fit = fittedmodel[2]\n",
    "    valcycount_fit = fittedmodel[3]\n",
    "    covariates_fit = fittedmodel[4]\n",
    "    \n",
    "    ### DON'T CHANGE ABOVE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8bc6156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save back test outputs to csv files\n",
    "bt_csv(modelname='path_cov_pres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0264a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate back test set error and save to csv files\n",
    "bt_set_errors(modelname='path_cov_pres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02944072",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "SUMMARY REPORT FOR MODEL EVALUATION\n",
      "##################################################\n",
      "\n",
      "Number of fitted target features: 2\n",
      "List of fitted target features: \n",
      "['LAT', 'LON']\n",
      "List of fitted covariate features:\n",
      "['CENTRAL_PRES', 'ENV_PRES']\n",
      "\n",
      "Validating Cyclone ID(s): \n",
      "['AU202021_11U', 'AU202021_15U', 'AU202021_17U', 'AU202021_22U']\n",
      "Validating Cyclone Name(s):\n",
      "['Lucas', 'Marian', 'Niran', 'Seroja']\n",
      "\n",
      "Number of training cyclones used for each run: 93\n",
      "Validating method: leave one out\n",
      "Number of validating runs: 4\n",
      "Prediction at step n ahead: n=  [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "\n",
      "##################################################\n",
      "Summarry Table of Scaled Error after 4 Validating Runs.\n",
      "##################################################\n",
      "\n",
      "\n",
      "Error Units: scaled between 0 to 1\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|         |      6h |     12h |     18h |     24h |     30h |     36h |     42h |     48h |\n",
      "|         |   (n=1) |   (n=2) |   (n=3) |   (n=4) |   (n=5) |   (n=6) |   (n=7) |   (n=8) |\n",
      "|---------+---------+---------+---------+---------+---------+---------+---------+---------|\n",
      "| BT_sets |   43    |   43    |   43    |   43    |   43    |   43    |   43    |   43    |\n",
      "| MAE     |    0.03 |    0.07 |    0.11 |    0.13 |    0.16 |    0.19 |    0.22 |    0.24 |\n",
      "| RMSE    |    0.05 |    0.09 |    0.13 |    0.16 |    0.19 |    0.23 |    0.26 |    0.28 |\n",
      "| MSE     |    0    |    0.01 |    0.02 |    0.03 |    0.04 |    0.06 |    0.07 |    0.08 |\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "\n",
      "\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "# summary report\n",
    "summary_rep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "def0de9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A geoplot named 'path_cov_pres.html' was created.\n"
     ]
    }
   ],
   "source": [
    "# Geoplot\n",
    "if len(path_cov_pres)==6:\n",
    "    geoplot(geo_plot_data=path_cov_pres[-1],plotname='path_cov_pres.html')\n",
    "else:\n",
    "    print('No data for geoplot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ff744",
   "metadata": {},
   "source": [
    "#### 3. Modeling Cyclone Path with Covariates R34s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07608fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length = 14\n",
      "Cyclone number = 1\n",
      "Cyclone ID: AU202021_11U\n",
      "Epoch 29: 100%|█████████████████████████████████████████████████| 131/131 [00:17<00:00,  7.35it/s, train_loss=0.000931]\n",
      "Length = 33\n",
      "Cyclone number = 2\n",
      "Cyclone ID: AU202021_15U\n",
      "Epoch 29: 100%|██████████████████████████████████████████████████| 131/131 [00:18<00:00,  7.23it/s, train_loss=0.00045]\n",
      "Length = 18\n",
      "Cyclone number = 3\n",
      "Cyclone ID: AU202021_17U\n",
      "Epoch 29: 100%|██████████████████████████████████████████████████| 131/131 [00:17<00:00,  7.34it/s, train_loss=0.00189]\n",
      "Length = 30\n",
      "Cyclone number = 4\n",
      "Cyclone ID: AU202021_22U\n",
      "Epoch 29: 100%|██████████████████████████████████████████████████| 131/131 [00:18<00:00,  7.00it/s, train_loss=0.00091]\n"
     ]
    }
   ],
   "source": [
    "# Fitting cycpath with cov R34s\n",
    "path_cov_r34s = modelfitting(tscols=cycpath,df_list_of_cyclones=compare, covariates=r34s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b2693f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: only need to change model name in [] in 'for loop' for reports\n",
    "# Don't change names of variables on the left hand side.\n",
    "\n",
    "for fittedmodel in [path_cov_r34s]:\n",
    "    \n",
    "    ### DON'T CHANGE BELOW CODE\n",
    "    \n",
    "    # Assign backtest output of fitted model for reports\n",
    "    bt_act = fittedmodel[0][0]\n",
    "    bt_fc = fittedmodel[0][1]\n",
    "    sc_bt_act = fittedmodel[0][2]\n",
    "    sc_bt_fc = fittedmodel[0][3]\n",
    "    bt_error_fit = fittedmodel[0][4]\n",
    "\n",
    "    # Assign other outputs from fitted model for reports\n",
    "    target_col = fittedmodel[1]\n",
    "    traincycount_fit = fittedmodel[2]\n",
    "    valcycount_fit = fittedmodel[3]\n",
    "    covariates_fit = fittedmodel[4]\n",
    "    \n",
    "    ### DON'T CHANGE ABOVE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0fa9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save back test outputs to csv files\n",
    "bt_csv(modelname='path_cov_r34s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7e8b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate back test set error and save to csv files\n",
    "bt_set_errors(modelname='path_cov_r34s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4583f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "SUMMARY REPORT FOR MODEL EVALUATION\n",
      "##################################################\n",
      "\n",
      "Number of fitted target features: 2\n",
      "List of fitted target features: \n",
      "['LAT', 'LON']\n",
      "List of fitted covariate features:\n",
      "['MN_RADIUS_GF_SECNE', 'MN_RADIUS_GF_SECSE', 'MN_RADIUS_GF_SECSW', 'MN_RADIUS_GF_SECNW']\n",
      "\n",
      "Validating Cyclone ID(s): \n",
      "['AU202021_11U', 'AU202021_15U', 'AU202021_17U', 'AU202021_22U']\n",
      "Validating Cyclone Name(s):\n",
      "['Lucas', 'Marian', 'Niran', 'Seroja']\n",
      "\n",
      "Number of training cyclones used for each run: 93\n",
      "Validating method: leave one out\n",
      "Number of validating runs: 4\n",
      "Prediction at step n ahead: n=  [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "\n",
      "##################################################\n",
      "Summarry Table of Scaled Error after 4 Validating Runs.\n",
      "##################################################\n",
      "\n",
      "\n",
      "Error Units: scaled between 0 to 1\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|         |      6h |     12h |     18h |     24h |     30h |     36h |     42h |     48h |\n",
      "|         |   (n=1) |   (n=2) |   (n=3) |   (n=4) |   (n=5) |   (n=6) |   (n=7) |   (n=8) |\n",
      "|---------+---------+---------+---------+---------+---------+---------+---------+---------|\n",
      "| BT_sets |   43    |   43    |   43    |   43    |   43    |   43    |   43    |   43    |\n",
      "| MAE     |    0.05 |    0.08 |    0.11 |    0.13 |    0.16 |    0.18 |    0.2  |    0.2  |\n",
      "| RMSE    |    0.06 |    0.09 |    0.13 |    0.15 |    0.19 |    0.22 |    0.24 |    0.25 |\n",
      "| MSE     |    0    |    0.01 |    0.02 |    0.02 |    0.04 |    0.05 |    0.06 |    0.07 |\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "\n",
      "\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "# summary report\n",
    "summary_rep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8893b6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A geoplot named 'path_cov_r34s.html' was created.\n"
     ]
    }
   ],
   "source": [
    "# Geoplot\n",
    "if len(path_cov_r34s)==6:\n",
    "    geoplot(geo_plot_data=path_cov_r34s[-1],plotname='path_cov_r34s.html')\n",
    "else:\n",
    "    print('No data for geoplot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d655c44",
   "metadata": {},
   "source": [
    "#### 4. Modeling Cyclone Path with Covariates R34s and 'CENTRAL_PRES', 'ENV_PRES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a4c0ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length = 14\n",
      "Cyclone number = 1\n",
      "Cyclone ID: AU202021_11U\n",
      "Epoch 29: 100%|█████████████████████████████████████████████████| 131/131 [00:18<00:00,  7.08it/s, train_loss=0.000521]\n",
      "Length = 33\n",
      "Cyclone number = 2\n",
      "Cyclone ID: AU202021_15U\n",
      "Epoch 29: 100%|█████████████████████████████████████████████████| 131/131 [00:17<00:00,  7.31it/s, train_loss=0.000452]\n",
      "Length = 18\n",
      "Cyclone number = 3\n",
      "Cyclone ID: AU202021_17U\n",
      "Epoch 29: 100%|██████████████████████████████████████████████████| 131/131 [00:17<00:00,  7.29it/s, train_loss=0.00111]\n",
      "Length = 30\n",
      "Cyclone number = 4\n",
      "Cyclone ID: AU202021_22U\n",
      "Epoch 29: 100%|██████████████████████████████████████████████████| 131/131 [00:18<00:00,  7.20it/s, train_loss=0.00124]\n"
     ]
    }
   ],
   "source": [
    "# Fitting cycpath with all covs\n",
    "path_cov_all = modelfitting(tscols=cycpath,df_list_of_cyclones=compare, covariates=r34s+pres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e9a7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: only need to change model name in [] in  'for loop' for reports\n",
    "# Don't change names of variables on the left hand side.\n",
    "\n",
    "for fittedmodel in [path_cov_all]:\n",
    "   ### DON'T CHANGE BELOW CODE\n",
    "    \n",
    "    # Assign backtest output of fitted model for reports\n",
    "    bt_act = fittedmodel[0][0]\n",
    "    bt_fc = fittedmodel[0][1]\n",
    "    sc_bt_act = fittedmodel[0][2]\n",
    "    sc_bt_fc = fittedmodel[0][3]\n",
    "    bt_error_fit = fittedmodel[0][4]\n",
    "\n",
    "    # Assign other outputs from fitted model for reports\n",
    "    target_col = fittedmodel[1]\n",
    "    traincycount_fit = fittedmodel[2]\n",
    "    valcycount_fit = fittedmodel[3]\n",
    "    covariates_fit = fittedmodel[4]\n",
    "    \n",
    "    ### DON'T CHANGE ABOVE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a3a6111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save back test outputs to csv files\n",
    "bt_csv(modelname='path_cov_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8fd44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate back test set error and save to csv files\n",
    "bt_set_errors(modelname='path_cov_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "990762f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "SUMMARY REPORT FOR MODEL EVALUATION\n",
      "##################################################\n",
      "\n",
      "Number of fitted target features: 2\n",
      "List of fitted target features: \n",
      "['LAT', 'LON']\n",
      "List of fitted covariate features:\n",
      "['MN_RADIUS_GF_SECNE', 'MN_RADIUS_GF_SECSE', 'MN_RADIUS_GF_SECSW', 'MN_RADIUS_GF_SECNW', 'CENTRAL_PRES', 'ENV_PRES']\n",
      "\n",
      "Validating Cyclone ID(s): \n",
      "['AU202021_11U', 'AU202021_15U', 'AU202021_17U', 'AU202021_22U']\n",
      "Validating Cyclone Name(s):\n",
      "['Lucas', 'Marian', 'Niran', 'Seroja']\n",
      "\n",
      "Number of training cyclones used for each run: 93\n",
      "Validating method: leave one out\n",
      "Number of validating runs: 4\n",
      "Prediction at step n ahead: n=  [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "\n",
      "##################################################\n",
      "Summarry Table of Scaled Error after 4 Validating Runs.\n",
      "##################################################\n",
      "\n",
      "\n",
      "Error Units: scaled between 0 to 1\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "|         |      6h |     12h |     18h |     24h |     30h |     36h |     42h |     48h |\n",
      "|         |   (n=1) |   (n=2) |   (n=3) |   (n=4) |   (n=5) |   (n=6) |   (n=7) |   (n=8) |\n",
      "|---------+---------+---------+---------+---------+---------+---------+---------+---------|\n",
      "| BT_sets |   43    |   43    |   43    |   43    |   43    |   43    |   43    |   43    |\n",
      "| MAE     |    0.05 |    0.07 |    0.09 |    0.12 |    0.14 |    0.16 |    0.17 |    0.19 |\n",
      "| RMSE    |    0.06 |    0.09 |    0.11 |    0.15 |    0.17 |    0.19 |    0.21 |    0.23 |\n",
      "| MSE     |    0    |    0.01 |    0.01 |    0.02 |    0.03 |    0.04 |    0.05 |    0.06 |\n",
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n",
      "\n",
      "\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "# summary report\n",
    "summary_rep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21d92168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A geoplot named 'path_cov_all.html' was created.\n"
     ]
    }
   ],
   "source": [
    "# Geoplot\n",
    "if len(path_cov_all)==6:\n",
    "    geoplot(geo_plot_data=path_cov_all[-1],plotname='path_cov_all.html')\n",
    "else:\n",
    "    print('No data for geoplot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfa9476",
   "metadata": {},
   "source": [
    "### Model comparison with Paired t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3b6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in evaluation report files generated from previous sessions\n",
    "\n",
    "fcycpath = pd.read_csv('bt_set_error_path_nocov.csv')\n",
    "fcycpath_cov_pres = pd.read_csv('bt_set_error_path_cov_pres.csv')\n",
    "fcycpath_cov_r34s = pd.read_csv('bt_set_error_path_cov_r34s.csv')\n",
    "fcycpath_cov_all = pd.read_csv('bt_set_error_path_cov_all.csv')\n",
    "\n",
    "# Get MAE, RMSE, MSE columns only\n",
    "no_cov, cov_pres, cov_r34s, cov_all = fcycpath.iloc[:,-3:], fcycpath_cov_pres.iloc[:,-3:], fcycpath_cov_r34s.iloc[:,-3:], fcycpath_cov_all.iloc[:,-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b5b76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Table of Mean of Error Scores Accross 8 Forecast Points\n",
      "##################################################\n",
      "+----+---------------+----------------------------------------------------------------------+--------+--------+--------+\n",
      "|    | Model         | Covariates                                                           |    MAE |   RMSE |    MSE |\n",
      "|----+---------------+----------------------------------------------------------------------+--------+--------+--------|\n",
      "|  0 | path_nocov    | none                                                                 | 0.1216 | 0.1462 | 0.0309 |\n",
      "|  1 | path_cov_pres | ['CENTRAL_PRES', 'ENV_PRES']                                         | 0.1448 | 0.1674 | 0.0375 |\n",
      "|  2 | path_cov_r34s | ['r34_ne', 'r34_se', 'r34_sw', 'r34_nw']                             | 0.1382 | 0.1561 | 0.0334 |\n",
      "|  3 | path_cov_all  | ['r34_ne', 'r34_se', 'r34_sw', 'r34_nw', 'CENTRAL_PRES', 'ENV_PRES'] | 0.1246 | 0.1423 | 0.0282 |\n",
      "+----+---------------+----------------------------------------------------------------------+--------+--------+--------+\n",
      "##################################################\n",
      "\n",
      "PAIRED T-TEST OF MAE SCORES AT 95% CONFIDENCE LEVEL\n",
      "+----+------------+---------------+-----------+--------------------------+\n",
      "|    | Model_1    | Model_2       |   p_value | Significant alpha=0.05   |\n",
      "|----+------------+---------------+-----------+--------------------------|\n",
      "|  0 | path_nocov | path_cov_pres |     0.044 | yes                      |\n",
      "|  1 | path_nocov | path_cov_r34s |     0.056 | no                       |\n",
      "|  2 | path_nocov | path_cov_all  |     0.776 | no                       |\n",
      "+----+------------+---------------+-----------+--------------------------+\n",
      "##################################################\n",
      "\n",
      "##################################################\n",
      "\n",
      "PAIRED T-TEST OF RMSE SCORES AT 95% CONFIDENCE LEVEL\n",
      "+----+------------+---------------+-----------+--------------------------+\n",
      "|    | Model_1    | Model_2       |   p_value | Significant alpha=0.05   |\n",
      "|----+------------+---------------+-----------+--------------------------|\n",
      "|  0 | path_nocov | path_cov_pres |     0.102 | no                       |\n",
      "|  1 | path_nocov | path_cov_r34s |     0.313 | no                       |\n",
      "|  2 | path_nocov | path_cov_all  |     0.733 | no                       |\n",
      "+----+------------+---------------+-----------+--------------------------+\n",
      "##################################################\n",
      "\n",
      "##################################################\n",
      "\n",
      "PAIRED T-TEST OF MSE SCORES AT 95% CONFIDENCE LEVEL\n",
      "+----+------------+---------------+-----------+--------------------------+\n",
      "|    | Model_1    | Model_2       |   p_value | Significant alpha=0.05   |\n",
      "|----+------------+---------------+-----------+--------------------------|\n",
      "|  0 | path_nocov | path_cov_pres |     0.169 | no                       |\n",
      "|  1 | path_nocov | path_cov_r34s |     0.509 | no                       |\n",
      "|  2 | path_nocov | path_cov_all  |     0.518 | no                       |\n",
      "+----+------------+---------------+-----------+--------------------------+\n",
      "##################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate paired t-test report\n",
    "from scipy import stats\n",
    "from tabulate import tabulate\n",
    "\n",
    "r34s_short = ['r34_ne','r34_se','r34_sw','r34_nw']\n",
    "errordf = [no_cov, cov_pres, cov_r34s, cov_all]\n",
    "modelname = ['path_nocov','path_cov_pres', 'path_cov_r34s', 'path_cov_all']\n",
    "covset = ['none',pres,r34s_short,r34s_short+pres]\n",
    "\n",
    "# Generate table of mean errors across all models\n",
    "mean_list=[]\n",
    "for df in errordf:\n",
    "    m = df.mean().values\n",
    "    mean_list.append(m)\n",
    "mean_df = pd.DataFrame(mean_list, columns= ['MAE','RMSE','MSE'])\n",
    "mean_df['Model'] = modelname\n",
    "mean_df['Covariates'] = covset\n",
    "mean_df = mean_df[['Model','Covariates','MAE','RMSE','MSE']]\n",
    "print('#'*50)\n",
    "print('Table of Mean of Error Scores Accross 8 Forecast Points')\n",
    "print('#'*50)\n",
    "print(tabulate(mean_df.round(4), headers = 'keys', tablefmt = 'psql'))\n",
    "\n",
    "# Generate table of Paired t-test\n",
    "for metrics in ['MAE','RMSE','MSE']:\n",
    "    p_val = []\n",
    "    for df in errordf[1:]:\n",
    "        p = stats.ttest_rel(no_cov[metrics], df[metrics]).pvalue.round(3)\n",
    "        p_val.append(p)\n",
    "\n",
    "    pair_ttest = pd.DataFrame([np.repeat(modelname[:1],len(modelname)-1),modelname[1:],p_val],\n",
    "                             index=['Model_1','Model_2', 'p_value'])\n",
    "    pair_ttest = pair_ttest.transpose()\n",
    "    pair_ttest['Significant alpha=0.05'] = np.where(pair_ttest['p_value']<0.05,'yes', 'no')\n",
    "    print('#'*50)\n",
    "    print(f\"\\nPAIRED T-TEST OF {metrics} SCORES AT 95% CONFIDENCE LEVEL\")\n",
    "    print(tabulate(pair_ttest, headers = 'keys', tablefmt = 'psql'))\n",
    "    print('#'*50+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36832c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
